[
  {
    "content": {
      "TLDR": "In this paper we present Virtuoso Strings, a dataset for timing analysis and automatic music transcription (AMT) tasks requiring note onset annotations. This dataset takes advantage of real-world recordings in multitrack format and is curated as part of the Augmented Reality Music Ensemble (ARME) project which investigates musician synchronisation and multimodal music analysis. The dataset is comprised of repeated recordings of quartet, trio, duet and solo ensemble performances with different temporal expressions and leadership role assignments, providing new possibilities for developing and evaluating AMT models with respect to diverse musical performance styles. To reduce the cost of the labour-intensive manual annotation, a semi-automatic method was utilised for both annotation and quality control. The presented dataset consists of 746 tracks with a total of 68,728 onsets. Every track includes onset annotations for a single string instrument, enabling the creation of audio files with different combinations of instruments to be used in the AMT evaluation process.",
      "abstract": "In this paper we present Virtuoso Strings, a dataset for timing analysis and automatic music transcription (AMT) tasks requiring note onset annotations. This dataset takes advantage of real-world recordings in multitrack format and is curated as part of the Augmented Reality Music Ensemble (ARME) project which investigates musician synchronisation and multimodal music analysis. The dataset is comprised of repeated recordings of quartet, trio, duet and solo ensemble performances with different temporal expressions and leadership role assignments, providing new possibilities for developing and evaluating AMT models with respect to diverse musical performance styles. To reduce the cost of the labour-intensive manual annotation, a semi-automatic method was utilised for both annotation and quality control. The presented dataset consists of 746 tracks with a total of 68,728 onsets. Every track includes onset annotations for a single string instrument, enabling the creation of audio files with different combinations of instruments to be used in the AMT evaluation process.",
      "authors": [
        "Tomczak, Maciej*",
        " Li, Min Susan",
        " Di Luca, Massimiliano"
      ],
      "bilibili_id": "",
      "channel_name": "lp-1-tomczak",
      "channel_url": "https://slack.com/app_redirect?channel=C064C28K0V7",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1FrjpqyGu4Wl3Oj5WsxA6_1fkmVqRjrEu/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "001.png",
      "title": "Virtuoso Strings: A Dataset of String Ensemble Recordings and Onset Annotations for Timing Analysis",
      "youtube_id": ""
    },
    "forum": "312",
    "id": "312",
    "position": "1"
  },
  {
    "content": {
      "TLDR": "We present Music Scope Pad, an application for efficiently discovering what one wants to view among many unknown music videos (MVs). Current video players only enable the user to view one MV at a time, so to explore what the user wants to view among many unknown MVs, they need to play each MV individually, which requires a large amount of manipulation. Our app features artificial-intelligence processing of video acquired with the device's front camera to detect the natural movements of the user's head and hands while listening to music, enabling the user to explore MVs. Ten MVs are played simultaneously on the iPad screen and through the spatial acoustics of the device. The user can then explore the MVs they want to view by moving their head left or right. The volume of each MV through the spatial acoustics is automatically adjusted so that the MVs closer to the center of the screen are louder. If the user then cups their hands around their ears, as if they were listening carefully, they can hear the MVs directly in front of them on the screen through the spatial acoustics with more emphasis. If the user keeps their focus on one MV for more than three seconds, that MV will be selected and only that video will be played from the beginning. ",
      "abstract": "We present Music Scope Pad, an application for efficiently discovering what one wants to view among many unknown music videos (MVs). Current video players only enable the user to view one MV at a time, so to explore what the user wants to view among many unknown MVs, they need to play each MV individually, which requires a large amount of manipulation. Our app features artificial-intelligence processing of video acquired with the device's front camera to detect the natural movements of the user's head and hands while listening to music, enabling the user to explore MVs. Ten MVs are played simultaneously on the iPad screen and through the spatial acoustics of the device. The user can then explore the MVs they want to view by moving their head left or right. The volume of each MV through the spatial acoustics is automatically adjusted so that the MVs closer to the center of the screen are louder. If the user then cups their hands around their ears, as if they were listening carefully, they can hear the MVs directly in front of them on the screen through the spatial acoustics with more emphasis. If the user keeps their focus on one MV for more than three seconds, that MV will be selected and only that video will be played from the beginning. ",
      "authors": [
        "Hamanaka, Masatoshi*"
      ],
      "bilibili_id": "",
      "channel_name": "lp-2-hamanaka",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZAMPRNF",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1m8FshdzWL7Enl8ASaVDxms__LAlWbypR/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "002.png",
      "title": "Music Scope Pad",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1UzC_0f_qtntgvnpkkpf-wkvVGVqrwM9q"
    },
    "forum": "315",
    "id": "315",
    "position": "2"
  },
  {
    "content": {
      "TLDR": "This paper presents a transformer machine-learning model trained with a large dataset of chord sequences. The dataset includes several styles, such as jazz, rock, pop, blues, or music for cinema, among others. We investigated three modeling strategies: 1) We started the tokenization method by treating all different chords as unique elements, which resulted in a vocabulary of 5202 independent chords as tokens. 2) We expressed the chords as a tuple describing root, nature (e.g., major, minor, diminished, major seventh), and extensions (e.g., additions, alterations), which produces a vocabulary of 59 tokens. 3) We extended the second model by complementing the transformer model with chord information containing eight MIDI notes added to the positional embeddings. We analyze sequences generated by comparing them with the training dataset using trigram analysis, which reveals common chord progressions and source duplications. Secondly, we compared the generated sequences from a musical perspective, rating their plausibility concerning the training data. The third strategy reported lower validation loss and more musical consistency in the suggested progressions.",
      "abstract": "This paper presents a transformer machine-learning model trained with a large dataset of chord sequences. The dataset includes several styles, such as jazz, rock, pop, blues, or music for cinema, among others. We investigated three modeling strategies: 1) We started the tokenization method by treating all different chords as unique elements, which resulted in a vocabulary of 5202 independent chords as tokens. 2) We expressed the chords as a tuple describing root, nature (e.g., major, minor, diminished, major seventh), and extensions (e.g., additions, alterations), which produces a vocabulary of 59 tokens. 3) We extended the second model by complementing the transformer model with chord information containing eight MIDI notes added to the positional embeddings. We analyze sequences generated by comparing them with the training dataset using trigram analysis, which reveals common chord progressions and source duplications. Secondly, we compared the generated sequences from a musical perspective, rating their plausibility concerning the training data. The third strategy reported lower validation loss and more musical consistency in the suggested progressions.",
      "authors": [
        "Cabrera Dalmazzo, David*",
        " D\u00e9guernel, Ken",
        " Sturm, Bob L. T. "
      ],
      "bilibili_id": "",
      "channel_name": "lp-3-dalmazzo",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDGD49G",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/14xEoj_bGwcHCBlzX0zhZpmWIz9y2v816/preview",
      "poster_link": "https://drive.google.com/file/d/1dhgnb5AIjfxHmZsAYQotRyOZ_6p8UORH/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "003.png",
      "title": "The Chordinator: Chord Progression Modeling and Generation using Transformers",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1k0C-uFmAnG-ma2nDWvc9DgdrfRIiGlAz"
    },
    "forum": "316",
    "id": "316",
    "position": "3"
  },
  {
    "content": {
      "TLDR": "Cornelissen et al. (2021) propose representing melodic contour using a linear combination of few cosine functions for a variety of tasks such as visualization, classification and summarization. The authors apply this representation to studying melodies from four traditions (Gregorian chant, German, Chinese and Sioux folk songs) and claim that it provides a common ground for comparing contours across traditions due to its being data independent, i.e., culturally neutral. We study this claim in the domain of Irish Traditional Dance Music and find evidence that challenges its veracity. Specifically, we find that the cosine representation of the melodic contours of this music is not nearly as efficient as it is for the other traditions. We contrast this representation with a principal component analysis of the melodic contours. Our work contributes to a more nuanced understanding of cosine-based representations of melodic contour.",
      "abstract": "Cornelissen et al. (2021) propose representing melodic contour using a linear combination of few cosine functions for a variety of tasks such as visualization, classification and summarization. The authors apply this representation to studying melodies from four traditions (Gregorian chant, German, Chinese and Sioux folk songs) and claim that it provides a common ground for comparing contours across traditions due to its being data independent, i.e., culturally neutral. We study this claim in the domain of Irish Traditional Dance Music and find evidence that challenges its veracity. Specifically, we find that the cosine representation of the melodic contours of this music is not nearly as efficient as it is for the other traditions. We contrast this representation with a principal component analysis of the melodic contours. Our work contributes to a more nuanced understanding of cosine-based representations of melodic contour.",
      "authors": [
        "Cros Vila, Laura*",
        " Sturm, Bob L. T. "
      ],
      "bilibili_id": "",
      "channel_name": "lp-4-vila",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZAN3PA7",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1PBHtPvmTiGQjxySUe0ZDa-4IoWvNANLI/preview",
      "poster_link": "https://drive.google.com/file/d/1U0Vp26MUYUH5pyMqRLFYSqGZUNe-jWfr/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "004.png",
      "title": "Cosine Contours: A Case Study with Melodies from Irish Traditional Dance Music",
      "youtube_id": ""
    },
    "forum": "318",
    "id": "318",
    "position": "4"
  },
  {
    "content": {
      "TLDR": "Rhythm-based video games challenge players to match their actions with musical cues, turning songs into interactive experiences. The design of the game charts, which dictate the timing and placement of on-screen notes, are manually crafted by players and developers. With AutoOsu, we introduce a CRNN-based model for generating rhythm game charts for a given audio track, conditioned on an intended difficulty level. In previous studies, this task is often divided into two: onset detection, which determines timing points for notes; and action generation, where notes are distributed among a set of available keys. These two sub-tasks are typically handled with two separately trained models, and audio information is only given to the onset detection model. We instead jointly train the two recurrent layers who both receive audio information, which streamlines the training process and helps better utilize musical features.",
      "abstract": "Rhythm-based video games challenge players to match their actions with musical cues, turning songs into interactive experiences. The design of the game charts, which dictate the timing and placement of on-screen notes, are manually crafted by players and developers. With AutoOsu, we introduce a CRNN-based model for generating rhythm game charts for a given audio track, conditioned on an intended difficulty level. In previous studies, this task is often divided into two: onset detection, which determines timing points for notes; and action generation, where notes are distributed among a set of available keys. These two sub-tasks are typically handled with two separately trained models, and audio information is only given to the onset detection model. We instead jointly train the two recurrent layers who both receive audio information, which streamlines the training process and helps better utilize musical features.",
      "authors": [
        "Lee, Sihun*",
        " Jeong, Dasaem"
      ],
      "bilibili_id": "",
      "channel_name": "lp-5-lee",
      "channel_url": "https://slack.com/app_redirect?channel=C063WFRT50D",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1zdgVPPRPBEueRfVDUl5yBUWrh_eWAJ3O/preview",
      "poster_link": "https://drive.google.com/file/d/1slXIcNaKQqbtS0SAECSHMToK37qzi37-/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "005.png",
      "title": "AutoOsu: Audio-Aware Action Generation for Rhythm Games",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1t4NXtS80VY-8kQboqeV0789Ie8buPBF1"
    },
    "forum": "319",
    "id": "319",
    "position": "5"
  },
  {
    "content": {
      "TLDR": "We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre-conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.",
      "abstract": "We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre-conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.",
      "authors": [
        "Kroher, Nadine",
        " Cuesta, Helena*",
        " Pikrakis, Aggelos"
      ],
      "bilibili_id": "",
      "channel_name": "lp-6-cuesta",
      "channel_url": "https://slack.com/app_redirect?channel=C063WFS2021",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/16LUDN42VsaUa1Nhp106Gbagpxk4CduC9/preview",
      "poster_link": "https://drive.google.com/file/d/1kk2_IVl3-7H6QKDnZyB2u4o-lHNpoQuL/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "006.png",
      "title": "Can MusicGen Create Training Data for MIR Tasks?",
      "youtube_id": ""
    },
    "forum": "320",
    "id": "320",
    "position": "6"
  },
  {
    "content": {
      "TLDR": "A method and apparatus is described for generating and recording acoustic piano performances automatically and at scale.  This technique has generated nearly 500 hours of studio piano recordings, including a complete re-recording of the MAESTRO solo piano dataset. A software utility has been implemented to simultaneously manage MIDI playback on a Yamaha Diskalvier and a synchronized audio capture session. As part of this late-breaking demo, we release the re-recorded studio audio for the MAESTRO Studio dataset and the Python software package piano-capture used for automating data collection. As well as adding to the pool of existing training data for MIR tasks such as transcription and audio-MIDI alignment, this work facilitates the creation of further datasets, for example in other musical styles.",
      "abstract": "A method and apparatus is described for generating and recording acoustic piano performances automatically and at scale.  This technique has generated nearly 500 hours of studio piano recordings, including a complete re-recording of the MAESTRO solo piano dataset. A software utility has been implemented to simultaneously manage MIDI playback on a Yamaha Diskalvier and a synchronized audio capture session. As part of this late-breaking demo, we release the re-recorded studio audio for the MAESTRO Studio dataset and the Python software package piano-capture used for automating data collection. As well as adding to the pool of existing training data for MIR tasks such as transcription and audio-MIDI alignment, this work facilitates the creation of further datasets, for example in other musical styles.",
      "authors": [
        "Edwards, Andrew C*",
        " Dixon, Simon",
        " Maezawa, Akira",
        " Kusaka, Yuta"
      ],
      "bilibili_id": "",
      "channel_name": "lp-7-edwards",
      "channel_url": "https://slack.com/app_redirect?channel=C064C2UQ38R",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/15_Fnl0FibQ6Jp8XvHCJc8fTbZYl4Btf9/preview",
      "poster_link": "https://drive.google.com/file/d/1SCuNRk58yaW1BAnvrd_wjsBI9Cs845fw/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "007.png",
      "title": "AUTOMATIC PRODUCTION OF ACOUSTIC PIANO TRANSCRIPTION DATA",
      "youtube_id": ""
    },
    "forum": "321",
    "id": "321",
    "position": "7"
  },
  {
    "content": {
      "TLDR": "Music performance analysis has traditionally been a peripheral topic in the music information retrieval community. However, it is sometimes included in the goals of music information retrieval studies\u2014from low- and high-level parameter detection and synchronization methods to feature selection and measure transfer. In this contribution, we introduce the MemoVision software, which utilizes retrieval and feature selection methods for comparative music analysis. It is built with JavaScript and Python languages, combining the accessibility of a web-based interface and state-of-the-art feature extraction possibilities. Its features can be used separately or as a visualization tool for music performance case studies.",
      "abstract": "Music performance analysis has traditionally been a peripheral topic in the music information retrieval community. However, it is sometimes included in the goals of music information retrieval studies\u2014from low- and high-level parameter detection and synchronization methods to feature selection and measure transfer. In this contribution, we introduce the MemoVision software, which utilizes retrieval and feature selection methods for comparative music analysis. It is built with JavaScript and Python languages, combining the accessibility of a web-based interface and state-of-the-art feature extraction possibilities. Its features can be used separately or as a visualization tool for music performance case studies.",
      "authors": [
        "Istvanek, Matej*",
        " Miklanek, Stepan"
      ],
      "bilibili_id": "",
      "channel_name": "lp-8-istvanek",
      "channel_url": "https://slack.com/app_redirect?channel=C064C2UUDHP",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/10a1fbxBS_fTCGeGQYXX2az_1UQT1pTuf/preview",
      "poster_link": "https://drive.google.com/file/d/12ltmy5vpCqnniP-hnWMos9sVF_vne7UC/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "008.png",
      "title": "MemoVision: a tool for feature selection and visualization of performance data",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1W93WTRBKcsOtVSdOPk4aPJovrGVjkmqu"
    },
    "forum": "322",
    "id": "322",
    "position": "8"
  },
  {
    "content": {
      "TLDR": "The present here the recent development of an online platform for musicians, researchers and an open community of  enthusiasts of audio and music with a view to build a public database of music recordings from a wide variety of styles and different cultures. The data generated and collected will primarily be audio data, coming from various sources, including field recordings, existing datasets, and users\u2019 collaboration. The platform aims at gathering a distributed music crowdsourcing database collection where each music piece is built from asynchronous recordings of different tracks at remote sites. The complete tool and databases generated will be openly distributed for research purpose.",
      "abstract": "The present here the recent development of an online platform for musicians, researchers and an open community of  enthusiasts of audio and music with a view to build a public database of music recordings from a wide variety of styles and different cultures. The data generated and collected will primarily be audio data, coming from various sources, including field recordings, existing datasets, and users\u2019 collaboration. The platform aims at gathering a distributed music crowdsourcing database collection where each music piece is built from asynchronous recordings of different tracks at remote sites. The complete tool and databases generated will be openly distributed for research purpose.",
      "authors": [
        "Richard, Ga\u00ebl*",
        " Gil Panal, Jose Manuel",
        " David, Aur\u00e9lien"
      ],
      "bilibili_id": "",
      "channel_name": "lp-9-richard",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDH90N6",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1GgMsbMs1AbakAg4SP47OsszU09hsPQv0/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "009.png",
      "title": "The Hi-Audio Online Platform for distributed music crowdsourcing database collection",
      "youtube_id": ""
    },
    "forum": "323",
    "id": "323",
    "position": "9"
  },
  {
    "content": {
      "TLDR": "This paper investigates training methods for musical instrument recognition (IR). Many studies have tackled IR and improved performance based on limited primary datasets (e.g., the OpenMIC dataset). As such, IR on other datasets, especially small ones, is yet to be studied. We propose to utilize a large synthesized dataset for IR on small datasets with real-world samples. Specifically, we first pre-train a transformer-based model with the Slakh2100 dataset to initialize its weights. We then fine-tune the model by training using the target datasets. We compare our approach with the adaptor approach, widely known as effective in fine-tuning large language models. We also investigate how the IR performance changes when we can access only a limited number of samples from the target dataset. Our experiment shows that the weight initialization performs consistently better than training from scratch and the adaptor approach.",
      "abstract": "This paper investigates training methods for musical instrument recognition (IR). Many studies have tackled IR and improved performance based on limited primary datasets (e.g., the OpenMIC dataset). As such, IR on other datasets, especially small ones, is yet to be studied. We propose to utilize a large synthesized dataset for IR on small datasets with real-world samples. Specifically, we first pre-train a transformer-based model with the Slakh2100 dataset to initialize its weights. We then fine-tune the model by training using the target datasets. We compare our approach with the adaptor approach, widely known as effective in fine-tuning large language models. We also investigate how the IR performance changes when we can access only a limited number of samples from the target dataset. Our experiment shows that the weight initialization performs consistently better than training from scratch and the adaptor approach.",
      "authors": [
        "Tanaka, Keitaro*",
        " Luo, Yin-Jyun",
        " Cheuk, Kin Wai",
        " Yoshii, Kazuyoshi",
        " Morishima, Shigeo",
        " Dixon, Simon"
      ],
      "bilibili_id": "",
      "channel_name": "lp-10-tanaka",
      "channel_url": "https://slack.com/app_redirect?channel=C0641SRQ7MJ",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1MBtS7S8wMRgL7aLE_0ySwCaSvaBVO_cF/preview",
      "poster_link": "https://drive.google.com/file/d/10Drq5VqaRIa86jw17kTNeSyYB528GxNs/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "010.png",
      "title": "On the Use of Synthesized Datasets and Transformer Adaptors for Musical Instrument Recognition",
      "youtube_id": ""
    },
    "forum": "324",
    "id": "324",
    "position": "10"
  },
  {
    "content": {
      "TLDR": "Automated beat tracking is a central task in music information retrieval (MIR) and aims to find time positions that humans would tap a long when listening to music. In a previous contribution, we built upon an existing beat tracking system and evaluated the results on annotated datasets of Western classical music. In our experiments, we obtained poor evaluation results which we first attributed to the complexity of the music. We later discovered that many of the poor results can be traced back to technical issues in the processing pipeline and inaccurate beat annotations. In this contribution, we report on some of these issues and make suggestions regarding visualization and sonification to better understand the data and results.",
      "abstract": "Automated beat tracking is a central task in music information retrieval (MIR) and aims to find time positions that humans would tap a long when listening to music. In a previous contribution, we built upon an existing beat tracking system and evaluated the results on annotated datasets of Western classical music. In our experiments, we obtained poor evaluation results which we first attributed to the complexity of the music. We later discovered that many of the poor results can be traced back to technical issues in the processing pipeline and inaccurate beat annotations. In this contribution, we report on some of these issues and make suggestions regarding visualization and sonification to better understand the data and results.",
      "authors": [
        "Chiu, Ching-Yu*",
        " M\u00fcller, Meinard"
      ],
      "bilibili_id": "",
      "channel_name": "lp-11-chiu",
      "channel_url": "https://slack.com/app_redirect?channel=C063JT7ML07",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/16H89T7zTrc8HdLRG1SbHlZ7o6_Tx_IqN/preview",
      "poster_link": "https://drive.google.com/file/d/1s4uaz5_ikg9aM21hfl91pwzy9_KPDoMh/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "011.png",
      "title": "What can go wrong when conducting beat tracking experiments",
      "youtube_id": ""
    },
    "forum": "325",
    "id": "325",
    "position": "11"
  },
  {
    "content": {
      "TLDR": "The improvement of automatic transcription algorithms has led to the accessibility of high-quality symbolic music data aligned with audio, especially for piano music. However, the transcribed MIDI files need extra annotation, such as beat, chord, and structure, to utilize the dataset in other MIR tasks, such as automatic music composition, melody harmonization, and harmonic analysis. Music annotations, especially chord annotations, need high-level music domain knowledge. Although there are a handful of algorithms for chord annotation, the prediction accuracy is limited. Therefore, it needs humans to correct the annotation eventually. We suggest a web-based semi-automated chord annotation tool for piano music to facilitate this validation process conveniently. We integrated the previous piano music transcription, alignment, quantization, and chord annotation research into one system. Given piano audio input, users can modify the predicted chord annotation of converted MIDI. The demo page is available in our website.",
      "abstract": "The improvement of automatic transcription algorithms has led to the accessibility of high-quality symbolic music data aligned with audio, especially for piano music. However, the transcribed MIDI files need extra annotation, such as beat, chord, and structure, to utilize the dataset in other MIR tasks, such as automatic music composition, melody harmonization, and harmonic analysis. Music annotations, especially chord annotations, need high-level music domain knowledge. Although there are a handful of algorithms for chord annotation, the prediction accuracy is limited. Therefore, it needs humans to correct the annotation eventually. We suggest a web-based semi-automated chord annotation tool for piano music to facilitate this validation process conveniently. We integrated the previous piano music transcription, alignment, quantization, and chord annotation research into one system. Given piano audio input, users can modify the predicted chord annotation of converted MIDI. The demo page is available in our website.",
      "authors": [
        "Lee, Seolhee*",
        " Choi, Eunjin",
        " Bae, Joonhyung",
        " Kim, Hyerin",
        " Nakamura, Eita",
        " Jeong, Dasaem",
        " Nam, Juhan"
      ],
      "bilibili_id": "",
      "channel_name": "lp-12-lee",
      "channel_url": "https://slack.com/app_redirect?channel=C063WFSSCR3",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1mGMsClAjyqaM0eExnE0rfVan2vfhG3kK/preview",
      "poster_link": "https://drive.google.com/file/d/1VaTDENC9RRgtBdKqfi9nZ607gnieN_Cg/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "012.png",
      "title": "Bridging Audio and Symbolic Piano Data through a Web-Based Annotation Interface",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1H42Kd_17CefAuktH_lcHtyaQGGM6L-lI"
    },
    "forum": "326",
    "id": "326",
    "position": "12"
  },
  {
    "content": {
      "TLDR": "We explore the concept of combining physical modeling of the piano with deep learning using methods from differentiable digital signal processing. The core of our proposed approach is a  modal synthesis model for the piano string, which is combined with a linear filter to approximate the acoustic properties of a grand piano. In a preliminary experiment, we train a neural network to estimate an excitation signal for a string in an autoencoder setting and show that the system can match the spectral content of a given target note. Our differentiable piano model could be utilized in a multitude of music processing tasks, including sound matching, signal enhancement, or source separation.",
      "abstract": "We explore the concept of combining physical modeling of the piano with deep learning using methods from differentiable digital signal processing. The core of our proposed approach is a  modal synthesis model for the piano string, which is combined with a linear filter to approximate the acoustic properties of a grand piano. In a preliminary experiment, we train a neural network to estimate an excitation signal for a string in an autoencoder setting and show that the system can match the spectral content of a given target note. Our differentiable piano model could be utilized in a multitude of music processing tasks, including sound matching, signal enhancement, or source separation.",
      "authors": [
        "Berendes, Hans-Ulrich",
        " Schw\u00e4r, Simon J*",
        " Sch\u00e4fer, Maximilian",
        " M\u00fcller, Meinard"
      ],
      "bilibili_id": "",
      "channel_name": "lp-13-schw\u00e4r",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZAPBC3V",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1YVe1LUEwdjoSbZ3RUQBXCQEUu1mPxeND/preview",
      "poster_link": "https://drive.google.com/file/d/14s0unpCZlwjthBNBY-8rjVF0StsUjTvT/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "013.png",
      "title": "Towards Differentiable Piano Synthesis Based On Physical Modeling",
      "youtube_id": ""
    },
    "forum": "327",
    "id": "327",
    "position": "13"
  },
  {
    "content": {
      "TLDR": "There have been many attempts to build automatic DJ systems in both industry and academia. However, these often focus solely on track selection and mixing, neglecting the importance of music structure and mix point selection\u2014key elements for determining transition points to the next track. We introduce a web demonstration, DJ StructFreak, which prioritizes mix point selection and music structures. The system identifies the best matching segment for user-selected mix points using music structure embeddings. As a result, it offers a continuous music listening experience and a new level of freedom for users in mix point selection. DJ StructFreak will be accessible via a link provided upon acceptance.",
      "abstract": "There have been many attempts to build automatic DJ systems in both industry and academia. However, these often focus solely on track selection and mixing, neglecting the importance of music structure and mix point selection\u2014key elements for determining transition points to the next track. We introduce a web demonstration, DJ StructFreak, which prioritizes mix point selection and music structures. The system identifies the best matching segment for user-selected mix points using music structure embeddings. As a result, it offers a continuous music listening experience and a new level of freedom for users in mix point selection. DJ StructFreak will be accessible via a link provided upon acceptance.",
      "authors": [
        "Kim, Taejun",
        " Nam, Juhan*"
      ],
      "bilibili_id": "",
      "channel_name": "lp-14-nam",
      "channel_url": "https://slack.com/app_redirect?channel=C063JT8564X",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1fbwMeFUp2c8lTTsEmsJxhSkp3o5otijk/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "014.png",
      "title": "DJ StructFreak: Automatic DJ system Built with Music Structure Embeddings",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1aD8P_h7GWDvTsIRHnD8qNFeO7hCTEAGM"
    },
    "forum": "328",
    "id": "328",
    "position": "14"
  },
  {
    "content": {
      "TLDR": "We present ``abcMLM,'' a masked language model for generating folk-like music that permits a wider range of possibilities compared to autoregressive approaches.  It has similar performance to autoregressive approaches overall,  albeit with a few peculiarities arising from a different training objective.  We demonstrate abcMLM and its different interaction modalities, and discuss its trade-offs in relation to autoregressive modeling.",
      "abstract": "We present ``abcMLM,'' a masked language model for generating folk-like music that permits a wider range of possibilities compared to autoregressive approaches.  It has similar performance to autoregressive approaches overall,  albeit with a few peculiarities arising from a different training objective.  We demonstrate abcMLM and its different interaction modalities, and discuss its trade-offs in relation to autoregressive modeling.",
      "authors": [
        "Casini, Luca*",
        " Jonason, Nicolas",
        " Sturm, Bob L. T. "
      ],
      "bilibili_id": "",
      "channel_name": "lp-15-casini",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDJ4T3L",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/12pm_LN6s5t85cWbD_iCZTtqk0R3C1NoN/preview",
      "poster_link": "https://drive.google.com/file/d/1Sjn6Ry-gVC8qFH4oS2Thuvm4dJnwsw6m/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "015.png",
      "title": "Generating folk-like music in abc-notation with Masked Language Models",
      "youtube_id": ""
    },
    "forum": "329",
    "id": "329",
    "position": "15"
  },
  {
    "content": {
      "TLDR": "Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs.",
      "abstract": "Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs.",
      "authors": [
        "Vinay, Ashvala*",
        " Lerch, Alexander"
      ],
      "bilibili_id": "",
      "channel_name": "lp-16-vinay",
      "channel_url": "https://slack.com/app_redirect?channel=C063SQ2JDJA",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1ArXSEip-EwYcaKsvZkm_MYaJAtmTU_Ox/preview",
      "poster_link": "https://drive.google.com/file/d/1pqP3O2pRJXLIJjIQxaRUR08XmnQi3fFn/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "016.png",
      "title": "AQUATK: An Audio Quality Assessment Toolkit",
      "youtube_id": ""
    },
    "forum": "330",
    "id": "330",
    "position": "16"
  },
  {
    "content": {
      "TLDR": "This paper presents a new method of encoding performance data in MEI using the recently added <extData> element. Performance data was extracted using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT) and encoded as a JSON object within an <extData> element linked to a specific musical note. A set of pop music vocals has been encoded to demonstrate both the range of descriptors that can be encoded in <extData> and how this method can be used for transcriptions in the absence of a fully specified musical score. ",
      "abstract": "This paper presents a new method of encoding performance data in MEI using the recently added <extData> element. Performance data was extracted using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT) and encoded as a JSON object within an <extData> element linked to a specific musical note. A set of pop music vocals has been encoded to demonstrate both the range of descriptors that can be encoded in <extData> and how this method can be used for transcriptions in the absence of a fully specified musical score. ",
      "authors": [
        "Devaney, Johanna*",
        " Beauchamp, Cecilia"
      ],
      "bilibili_id": "",
      "channel_name": "lp-17-devaney",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZAPV1SP",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1Rqn9ueBnKlplJVahEh2y_5zz6RtVxBD_/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "017.png",
      "title": "Encoding performance data in MEI with the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)",
      "youtube_id": ""
    },
    "forum": "331",
    "id": "331",
    "position": "17"
  },
  {
    "content": {
      "TLDR": "We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant  examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.",
      "abstract": "We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant  examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.",
      "authors": [
        "Jonason, Nicolas*",
        " Casini, Luca",
        " Sturm, Bob L. T. "
      ],
      "bilibili_id": "",
      "channel_name": "lp-18-jonason",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDJKDA6",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1Ch28jptrc8pnYnB4nipgMzsxjPtkpZRz/preview",
      "poster_link": "https://drive.google.com/file/d/12uZ8sAvl2jlx7SyqrR3l8wwqz0eD7xsV/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "018.png",
      "title": "Retrieval Augmented Generation of Symbolic Music with LLMs",
      "youtube_id": ""
    },
    "forum": "332",
    "id": "332",
    "position": "18"
  },
  {
    "content": {
      "TLDR": "The equitable distribution of academic data is crucial for ensuring equal research opportunities, and ultimately further progress. Yet, due to the complexity of using the API for audio data that corresponds to the Million Song Dataset along with its misreporting (before 2016) and the discontinuation of this API (after 2016), access to this data has become restricted to those within certain affiliations that are connected peer-to-peer. In this paper, we delve into this issue, drawing insights from the experiences of 22 individuals who either attempted to access the data or played a role in its creation. With this, we hope to initiate more critical dialogue and more thoughtful consideration with regard to access privilege in the MIR community.",
      "abstract": "The equitable distribution of academic data is crucial for ensuring equal research opportunities, and ultimately further progress. Yet, due to the complexity of using the API for audio data that corresponds to the Million Song Dataset along with its misreporting (before 2016) and the discontinuation of this API (after 2016), access to this data has become restricted to those within certain affiliations that are connected peer-to-peer. In this paper, we delve into this issue, drawing insights from the experiences of 22 individuals who either attempted to access the data or played a role in its creation. With this, we hope to initiate more critical dialogue and more thoughtful consideration with regard to access privilege in the MIR community.",
      "authors": [
        "Kim, Haven*",
        " Choi, Keunwoo",
        " Modrzejewski, Mateusz",
        " Liem, Cynthia C. S."
      ],
      "bilibili_id": "",
      "channel_name": "lp-19-kim",
      "channel_url": "https://slack.com/app_redirect?channel=C063SQ34R8W",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1sR1AkQMj2P8E551qpZWPlZhkukPCZbhX/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "019.png",
      "title": "The Biased Journey of MSD_AUDIO.ZIP",
      "youtube_id": ""
    },
    "forum": "334",
    "id": "334",
    "position": "19"
  },
  {
    "content": {
      "TLDR": "Despite the importance of musical key detection to computational music understanding, programmatically identifying the tonal center of a musical composition remains a challenging MIR task for modern deep learning systems, resulting in shortcomings in playlist generation and DJ systems. One bottleneck is the lack of availability of reliable tonal center ground truth. Furthermore, deep learning systems that are trained on classical and pop songs exhibit limitations generalizing to other genres. In this paper, we present a new expert-labeled dataset for the evaluation of key detection containing 260 hours (5489 songs) of song-level key and mode annotations, spread across 17 genres. Code for the dataset is made freely available for public use under a Creative Commons license. The dataset\u2019s reusability is enhanced by a bonus script we attach which enables researchers to recreate the dataset as attuned to their individual MIR tasks.",
      "abstract": "Despite the importance of musical key detection to computational music understanding, programmatically identifying the tonal center of a musical composition remains a challenging MIR task for modern deep learning systems, resulting in shortcomings in playlist generation and DJ systems. One bottleneck is the lack of availability of reliable tonal center ground truth. Furthermore, deep learning systems that are trained on classical and pop songs exhibit limitations generalizing to other genres. In this paper, we present a new expert-labeled dataset for the evaluation of key detection containing 260 hours (5489 songs) of song-level key and mode annotations, spread across 17 genres. Code for the dataset is made freely available for public use under a Creative Commons license. The dataset\u2019s reusability is enhanced by a bonus script we attach which enables researchers to recreate the dataset as attuned to their individual MIR tasks.",
      "authors": [
        "Wong, Stella*",
        " Hernandez, Gandalf"
      ],
      "bilibili_id": "",
      "channel_name": "lv-20-wong",
      "channel_url": "https://slack.com/app_redirect?channel=C064ND9QML0",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1JGpo5YN93KOsxebPBrD_z4weScslRpE6/preview",
      "poster_link": "https://drive.google.com/file/d/1Gw-z0KM59rldYqv5ef1bdTCjxbKOpWwq/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "020.png",
      "title": "FMAK: A DATASET OF KEY AND MODE ANNOTATIONS FOR THE FREE MUSIC ARCHIVE \u2013 EXTENDED ABSTRACT",
      "youtube_id": ""
    },
    "forum": "335",
    "id": "335",
    "position": "20"
  },
  {
    "content": {
      "TLDR": "We have investigated the classification of different textural elements in orchestral symbolic music data. A simple convolutional neural network (CNN) is utilized to perform the classification task in a track-wise and bar-wise manner. Preliminary results are reported, and different training parameters, including the use of contextual data and the combination of tracks, are also discussed. Code and data are available at: https://github.com/YaHsuanChu/orchestraTextureClassification ",
      "abstract": "We have investigated the classification of different textural elements in orchestral symbolic music data. A simple convolutional neural network (CNN) is utilized to perform the classification task in a track-wise and bar-wise manner. Preliminary results are reported, and different training parameters, including the use of contextual data and the combination of tracks, are also discussed. Code and data are available at: https://github.com/YaHsuanChu/orchestraTextureClassification ",
      "authors": [
        "CHU, YA-HSUAN",
        " Su, Li*"
      ],
      "bilibili_id": "",
      "channel_name": "lp-21-chu",
      "channel_url": "https://slack.com/app_redirect?channel=C0641STGVV2",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1J2XFSBWqJubpOiZkdSy-qgt8zmcszu1p/preview",
      "poster_link": "https://drive.google.com/file/d/1639jhcBmcsPFEEbbQFFxkZvPxvuPbmlx/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "021.png",
      "title": "Orchestral Texture Classification with Convolution",
      "youtube_id": ""
    },
    "forum": "336",
    "id": "336",
    "position": "21"
  },
  {
    "content": {
      "TLDR": "In the heart of \"rhythm games\" - games where players must perform actions in sync with a piece of music - are \"charts\", the directives to be given to players. We newly formulate chart generation as a sequence generation problem to train a Transformer using a large dataset. We also introduce tempo-informed preprocessing and training procedures, some of which are suggested to be integral for a successful training. Our model is found to outperform the baselines on a large dataset, and is also found to benefit from pretraining and finetuning.",
      "abstract": "In the heart of \"rhythm games\" - games where players must perform actions in sync with a piece of music - are \"charts\", the directives to be given to players. We newly formulate chart generation as a sequence generation problem to train a Transformer using a large dataset. We also introduce tempo-informed preprocessing and training procedures, some of which are suggested to be integral for a successful training. Our model is found to outperform the baselines on a large dataset, and is also found to benefit from pretraining and finetuning.",
      "authors": [
        "Yi, Jayeon J.*",
        " Lee, Sungho",
        " Lee, Kyogu"
      ],
      "bilibili_id": "",
      "channel_name": "lv-22-yi",
      "channel_url": "https://slack.com/app_redirect?channel=C063SQ3L17G",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1wBsPrqQRMOPATngJPbBGyNFGtG0UeWMP/preview",
      "poster_link": "https://drive.google.com/file/d/1rFYi-3zlubNcJpJtffKvyI_-YwMXKG3t/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "022.png",
      "title": "Beat-Aligned Spectrogram-to-Sequence Generation of Rhythm-Game Charts",
      "youtube_id": "https://drive.google.com/uc?export=view&id=133ZaWGSnPCH4cU0mBsHQp3Zqnzg1XKMM"
    },
    "forum": "337",
    "id": "337",
    "position": "22"
  },
  {
    "content": {
      "TLDR": "Pitch variability in rap vocals is overlooked in favor of the genre\u2019s uniquely dynamic rhythmic properties. We present an analysis of fundamental frequency (F0) variation in rap vocals over the past 14 years, focusing on song examples that represent the state of modern rap music. Our analy- sis aims at identifying meaningful trends over time, and is in turn a continuation of the 2023 analysis by Georgieva, Ripoll\u00e9s & McFee. They found rap to be an outlier with larger F0 variation compared to other genres, but with a declining trend since the genre\u2019s inception. However, they only analyzed data through 2010. Our analysis looks be- yond 2010. We once again observe rap\u2019s large F0 variation, but with a decelerated decline in recent years.",
      "abstract": "Pitch variability in rap vocals is overlooked in favor of the genre\u2019s uniquely dynamic rhythmic properties. We present an analysis of fundamental frequency (F0) variation in rap vocals over the past 14 years, focusing on song examples that represent the state of modern rap music. Our analy- sis aims at identifying meaningful trends over time, and is in turn a continuation of the 2023 analysis by Georgieva, Ripoll\u00e9s & McFee. They found rap to be an outlier with larger F0 variation compared to other genres, but with a declining trend since the genre\u2019s inception. However, they only analyzed data through 2010. Our analysis looks be- yond 2010. We once again observe rap\u2019s large F0 variation, but with a decelerated decline in recent years.",
      "authors": [
        "Walls, Kelvin L*",
        " Roman, Iran R",
        " Steers, Bea",
        " Georgieva, Elena"
      ],
      "bilibili_id": "",
      "channel_name": "lp-23-walls",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDKFCCS",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1IpGW0WOOE-Hz_32j2zDXku4-e2ipavoT/preview",
      "poster_link": "https://drive.google.com/file/d/11LrQ9FOijWZ_GOi_pgFJ2hJH2E5xUVq2/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "023.png",
      "title": "TOTAL VARIATION IN POPULAR RAP VOCALS FROM 2009-2023: EXTENSION OF THE ANALYSIS BY GEORGIEVA, RIPOLL\u00c9S & MCFEE",
      "youtube_id": ""
    },
    "forum": "338",
    "id": "338",
    "position": "23"
  },
  {
    "content": {
      "TLDR": "This work describes a validated procedure for collecting reference data to estimate personal values from song lyrics. The undertaken work describes our method for sampling songs for lyric annotation, the specific wording and response format of the annotation tool, and estimation of necessary number of annotations per song. We describe two waves of data collection with over 500 respondents each, the first as a method to estimate the necessary number of ratings using 20 songs, and the second to examine the structure of annotations on 360 songs. We show promising similarity in the structure of annotations when compared with the theoretical structure of personal values.",
      "abstract": "This work describes a validated procedure for collecting reference data to estimate personal values from song lyrics. The undertaken work describes our method for sampling songs for lyric annotation, the specific wording and response format of the annotation tool, and estimation of necessary number of annotations per song. We describe two waves of data collection with over 500 respondents each, the first as a method to estimate the necessary number of ratings using 20 songs, and the second to examine the structure of annotations on 360 songs. We show promising similarity in the structure of annotations when compared with the theoretical structure of personal values.",
      "authors": [
        "Demetriou, Andrew M.*",
        " Kim, Jaehun",
        " Manolios, Sandy",
        " Liem, Cynthia C. S."
      ],
      "bilibili_id": "",
      "channel_name": "lp-24-demetriou",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDKL3SN",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1xCIVdJ8bu3WBWNU0rL3NuCGG-JJ7yjvA/preview",
      "poster_link": "https://drive.google.com/file/d/1W3xup0DN-EJ9zJ7NQNy0FaJZi8DN5O58/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "024.png",
      "title": "TOWARDS AUTOMATED ESTIMATION OF VALUES FROM SONG LYRICS: A DATA COLLECTION PROTOCOL",
      "youtube_id": ""
    },
    "forum": "339",
    "id": "339",
    "position": "24"
  },
  {
    "content": {
      "TLDR": "In the composition process, selecting appropriate single-instrumental music sequences and assigning their track-role is an indispensable task. However, manually determining the track-role for a myriad of music samples can be time-consuming and labor-intensive. This study introduces a deep learning model designed to automatically predict the track-role of single-instrumental music sequences. Our evaluations show a prediction accuracy of 87% in the symbolic domain and 84% in the audio domain. The proposed track-role prediction methods hold promise for future applications in AI music generation and analysis.",
      "abstract": "In the composition process, selecting appropriate single-instrumental music sequences and assigning their track-role is an indispensable task. However, manually determining the track-role for a myriad of music samples can be time-consuming and labor-intensive. This study introduces a deep learning model designed to automatically predict the track-role of single-instrumental music sequences. Our evaluations show a prediction accuracy of 87% in the symbolic domain and 84% in the audio domain. The proposed track-role prediction methods hold promise for future applications in AI music generation and analysis.",
      "authors": [
        "Han, ChangHeon*",
        " Lee, Suhyun",
        " Ko, Minsam"
      ],
      "bilibili_id": "",
      "channel_name": "lp-25-han",
      "channel_url": "https://slack.com/app_redirect?channel=C064NDAK2LQ",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1EjIDSv-dVRUIOTAq_56eMFo75VDhHKiK/preview",
      "poster_link": "https://drive.google.com/file/d/16IIOWV6HG34rzIDgJOeNfqz6BXoW8HpH/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "025.png",
      "title": "Track Role Prediction of Single-Instrumental Sequences",
      "youtube_id": ""
    },
    "forum": "341",
    "id": "341",
    "position": "25"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "Silva, Nishal S*",
        " Turchet, Luca"
      ],
      "bilibili_id": "",
      "channel_name": "lp-26-silva",
      "channel_url": "https://slack.com/app_redirect?channel=C064C2XPBJ5",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1Zth4D96_1gIV4NmR6V9nqHq1UygqDulZ/preview",
      "poster_link": "https://drive.google.com/file/d/1lEqzPLCw1ddbFTQQUFoXNRXty6QSc5WV/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "026.png",
      "title": "Demo of a smart musical instrument-based real time pattern detection system",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1OGRp2iXM6TMereu_QmqH6Hx_MvoGjjEI"
    },
    "forum": "342",
    "id": "342",
    "position": "26"
  },
  {
    "content": {
      "TLDR": "In this paper, we discuss the evaluation for the automatic lyric transcription (ALT) task. We argue that existing lyric transcription benchmarks, primarily focusing on word content, may overlook the complex nuances of written lyrics. This leads to potential misalignment between the creative process of musicians and songwriters as well as listeners' experiences. For example, the absence of line breaks can strip the lyrics of their original rhythm, emotional emphasis, and rhyme scheme. To address this issue, we introduce JamALT, a new lyrics transcription benchmark based on the JamendoLyrics dataset. This includes an enhanced version of the data, geared specifically towards ALT evaluation by implementing music industry's transcription and formatting guidelines for lyrics, particularly in terms of punctuation, line breaks, letter case, and non-word vocal sounds. We also propose a suite of evaluation metrics beyond traditional word error rate, which are designed to capture the aforementioned issues. We hope that the proposed benchmark contributes to the ALT task, enabling more precise and reliable assessments of transcription systems and enhancing the user experience in lyrics applications such as subtitle renderings for live captioning or karaoke.",
      "abstract": "In this paper, we discuss the evaluation for the automatic lyric transcription (ALT) task. We argue that existing lyric transcription benchmarks, primarily focusing on word content, may overlook the complex nuances of written lyrics. This leads to potential misalignment between the creative process of musicians and songwriters as well as listeners' experiences. For example, the absence of line breaks can strip the lyrics of their original rhythm, emotional emphasis, and rhyme scheme. To address this issue, we introduce JamALT, a new lyrics transcription benchmark based on the JamendoLyrics dataset. This includes an enhanced version of the data, geared specifically towards ALT evaluation by implementing music industry's transcription and formatting guidelines for lyrics, particularly in terms of punctuation, line breaks, letter case, and non-word vocal sounds. We also propose a suite of evaluation metrics beyond traditional word error rate, which are designed to capture the aforementioned issues. We hope that the proposed benchmark contributes to the ALT task, enabling more precise and reliable assessments of transcription systems and enhancing the user experience in lyrics applications such as subtitle renderings for live captioning or karaoke.",
      "authors": [
        "C\u00edfka, Ond\u0159ej*",
        " Dimitriou, Constantinos",
        " Wang, Cheng-i",
        " Schreiber, Hendrik",
        " Miner, Luke",
        " St\u00f6ter, Fabian-Robert"
      ],
      "bilibili_id": "",
      "channel_name": "lp-27-c\u00edfka",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDL4RNE",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1Bc7fG0MrPsnpGiGxW5nXNZVqOVMYwgsq/preview",
      "poster_link": "https://drive.google.com/file/d/1YXnhLoQY75xDaLl3IVOPzclsKJLzdJjP/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "027.png",
      "title": "Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark",
      "youtube_id": ""
    },
    "forum": "343",
    "id": "343",
    "position": "27"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "KONG, Yuexuan*",
        " Hennequin, Romain",
        " Tran, Viet-Anh"
      ],
      "bilibili_id": "",
      "channel_name": "lp-28-kong",
      "channel_url": "https://slack.com/app_redirect?channel=C064NDB3HQ8",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1IGbmPeJlfENKWa9giJemxpmk4eU6r_MH/preview",
      "poster_link": "https://drive.google.com/file/d/1p-PaeAEoEhdPXgtlH7WDv7YdOCNcAoMo/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "028.png",
      "title": "STraDa: A Singer Traits Dataset",
      "youtube_id": ""
    },
    "forum": "344",
    "id": "344",
    "position": "28"
  },
  {
    "content": {
      "TLDR": "Style transfer, in the context of music generation, has been primarily enabled through content-style disentanglement. However, a notable limitation of existing disentanglement models is their confinement to short musical clips, typically spanning only a few bars. In this late-breaking demo, we propose and formalize a novel idea of modular style prior modelling to bridge this research gap. Our focus centres on the specific task of accompaniment arrangement, beginning with AccoMontage, a piano arranger that leverages chord-texture disentanglement and a primitive, rule-based style planner to maintain a long-term texture structure. Subsequently, we introduce Q&A-XL, a multi-track orchestrator with a more generic latent style prior model, which characterizes the global structure of orchestration style. From end to end, the complete system is named AccoMontage-3, which is capable of generating full-band accompaniment for whole pieces of music, with cohesive multi-track arrangement and coherent long-term structure.",
      "abstract": "Style transfer, in the context of music generation, has been primarily enabled through content-style disentanglement. However, a notable limitation of existing disentanglement models is their confinement to short musical clips, typically spanning only a few bars. In this late-breaking demo, we propose and formalize a novel idea of modular style prior modelling to bridge this research gap. Our focus centres on the specific task of accompaniment arrangement, beginning with AccoMontage, a piano arranger that leverages chord-texture disentanglement and a primitive, rule-based style planner to maintain a long-term texture structure. Subsequently, we introduce Q&A-XL, a multi-track orchestrator with a more generic latent style prior model, which characterizes the global structure of orchestration style. From end to end, the complete system is named AccoMontage-3, which is capable of generating full-band accompaniment for whole pieces of music, with cohesive multi-track arrangement and coherent long-term structure.",
      "authors": [
        "Zhao, Jingwei*",
        " Xia, Gus",
        " Wang, Ye"
      ],
      "bilibili_id": "",
      "channel_name": "lp-29-zhao",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDLEXEW",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1AsH1nokCxHdh455NdNBC6XPg5AeuKxhj/preview",
      "poster_link": "https://drive.google.com/file/d/1MePltXU54NrbJgDxz_kMCJGEpB4KymXU/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "029.png",
      "title": "Interpretable Modular Representation Learning for Full-Band Accompaniment Arrangement",
      "youtube_id": ""
    },
    "forum": "345",
    "id": "345",
    "position": "29"
  },
  {
    "content": {
      "TLDR": "Learning the harmonic structure of music is crucial for various music information retrieval (MIR) tasks. Word2vec skip-gram, a well-established technique in natural language processing, has been found to effectively learn harmonic concepts in music. It represents music slices in a vector space, preserving meaningful geometric relationships. These embeddings hold great promise as inputs for MIR tasks, particularly automatic chord recognition (ACR). However, ACR research predominantly focuses on audio data due to the limited availability of well-annotated symbolic music datasets. In this work, we propose an innovative approach utilizing the Harmony Transformer (HT) architecture by Chen and Su. Instead of incorporating input embedding within the network, we leverage skip-gram as an unsupervised embedding technique. Our experiments show that this unsupervised method produces embeddings that adeptly capture harmonic concepts. We also introduce a novel visualization method to assess the fidelity of these embeddings in representing harmonic musical concepts. We perform our experiments on the Lakh MIDI and the BPS-FH dataset.",
      "abstract": "Learning the harmonic structure of music is crucial for various music information retrieval (MIR) tasks. Word2vec skip-gram, a well-established technique in natural language processing, has been found to effectively learn harmonic concepts in music. It represents music slices in a vector space, preserving meaningful geometric relationships. These embeddings hold great promise as inputs for MIR tasks, particularly automatic chord recognition (ACR). However, ACR research predominantly focuses on audio data due to the limited availability of well-annotated symbolic music datasets. In this work, we propose an innovative approach utilizing the Harmony Transformer (HT) architecture by Chen and Su. Instead of incorporating input embedding within the network, we leverage skip-gram as an unsupervised embedding technique. Our experiments show that this unsupervised method produces embeddings that adeptly capture harmonic concepts. We also introduce a novel visualization method to assess the fidelity of these embeddings in representing harmonic musical concepts. We perform our experiments on the Lakh MIDI and the BPS-FH dataset.",
      "authors": [
        "Ebrahimzadeh, Maral*",
        " Krug, Valerie",
        " Stober, Sebastian"
      ],
      "bilibili_id": "",
      "channel_name": "lp-30-ebrahimzadeh",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDLKHUJ",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1-dAeViSpdzK3TqFylh4EflOL3ep524HA/preview",
      "poster_link": "https://drive.google.com/file/d/16YODMNDeGUUH_OmkQ3K7h2XOhnNC1xT-/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "030.png",
      "title": "Improving Embeddings in Harmony Transformer",
      "youtube_id": ""
    },
    "forum": "346",
    "id": "346",
    "position": "30"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "Ou, Longshen*",
        " Ma, Xichu",
        " Wang, Ye"
      ],
      "bilibili_id": "",
      "channel_name": "lp-31-ou",
      "channel_url": "https://slack.com/app_redirect?channel=C063WFVV353",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1dGJ8XTzTxFIPuJDkxJwJ0jdmko5SjSqG/preview",
      "poster_link": "https://drive.google.com/file/d/1roV-FGa10hqYba-9F2Zx6uCy2OieQ1m1/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "031.png",
      "title": "Singable and Controllable Neural Lyric Translation: a Late-Breaking Showcase",
      "youtube_id": ""
    },
    "forum": "347",
    "id": "347",
    "position": "31"
  },
  {
    "content": {
      "TLDR": "",
      "abstract": "",
      "authors": [
        "Akesbi, Kamil*",
        " Martin, Benjamin",
        " Desblancs, Dorian"
      ],
      "bilibili_id": "",
      "channel_name": "lp-32-akesbi",
      "channel_url": "https://slack.com/app_redirect?channel=C064NDBP8CQ",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1uSa1o0pdPGS9IVLqG8C1GbmWl0uugC2e/preview",
      "poster_link": "https://drive.google.com/file/d/1u9Scgo8FULDmiHBUSduAtZqUYWA_pziV/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "032.png",
      "title": "A Noise Augmentation Pipeline for Realistic Query-By-Example Simulation",
      "youtube_id": ""
    },
    "forum": "348",
    "id": "348",
    "position": "32"
  },
  {
    "content": {
      "TLDR": "LARS is an open-source cross-platform VST3 plugin written in C++ and designed for separating a stereo drums mixture into five individual stems. At the core of LARS lies a novel bank of parallel U-Nets pretrained using a collection of over 1224 hours of isolated drum clips, making LARS the first neural-network-based plugin available for drum source separation. Thanks to a competitive real-time ratio and user-friendly interface, LARS emerges as a versatile tool for music producers and engineers alike. These features makes LARS suitable for tasks such as drum replacement, drum loops decomposition, audio restoration, remixing, and remastering. Furthermore, LARS may find its way into contemporary music production workflows by complementing existing music demixing models.",
      "abstract": "LARS is an open-source cross-platform VST3 plugin written in C++ and designed for separating a stereo drums mixture into five individual stems. At the core of LARS lies a novel bank of parallel U-Nets pretrained using a collection of over 1224 hours of isolated drum clips, making LARS the first neural-network-based plugin available for drum source separation. Thanks to a competitive real-time ratio and user-friendly interface, LARS emerges as a versatile tool for music producers and engineers alike. These features makes LARS suitable for tasks such as drum replacement, drum loops decomposition, audio restoration, remixing, and remastering. Furthermore, LARS may find its way into contemporary music production workflows by complementing existing music demixing models.",
      "authors": [
        "Mezza, Alessandro I*",
        " di Palma, Riccardo",
        " Morena, Edoardo",
        " Orsatti, Alessandro",
        " Giampiccolo, Riccardo",
        " Bernardini, Alberto",
        " Sarti, Augusto"
      ],
      "bilibili_id": "",
      "channel_name": "lp-33-mezza",
      "channel_url": "https://slack.com/app_redirect?channel=C0641SVF892",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1mvutl-trgN06__8uloCCeq8nVmdgCJl-/preview",
      "poster_link": "https://drive.google.com/file/d/1Srp7TxtVq0xfou3gJ46WznjkhC4nvB-X/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "033.png",
      "title": "LARS: An Open-Source VST3 Plug-in for Deep Drums Demixing With Pretrained Models",
      "youtube_id": ""
    },
    "forum": "349",
    "id": "349",
    "position": "33"
  },
  {
    "content": {
      "TLDR": "The purpose of this study is to debilitate the enjoyment of music for both hearing-impaired and normal-hearing individuals by visually representing music. In order to effectively and distinctively portray the musical rhythm, we focus on Chironomie, a conducting technique used in Gregorian chant. Generally, Chironomie is drawn by a curve that corresponds to the musical score, and this curve is determined by whether a short segment of the score represents one of two classes: Arsis or Thesis. In pursuit of our objective, our endeavors encompass two essential facets: the adaptation of Chironomie for Western tonal music to express intuitively perceivable musical features like tension and relaxation, and the evaluation whether Chironomie can effectively convey music visually. We report an automated method for estimating Arsis and Thesis within composite beats to generate Chironomie. Additionally, it presents evaluation experiments involving normal-hearing to assess the effectiveness of Chironomie.",
      "abstract": "The purpose of this study is to debilitate the enjoyment of music for both hearing-impaired and normal-hearing individuals by visually representing music. In order to effectively and distinctively portray the musical rhythm, we focus on Chironomie, a conducting technique used in Gregorian chant. Generally, Chironomie is drawn by a curve that corresponds to the musical score, and this curve is determined by whether a short segment of the score represents one of two classes: Arsis or Thesis. In pursuit of our objective, our endeavors encompass two essential facets: the adaptation of Chironomie for Western tonal music to express intuitively perceivable musical features like tension and relaxation, and the evaluation whether Chironomie can effectively convey music visually. We report an automated method for estimating Arsis and Thesis within composite beats to generate Chironomie. Additionally, it presents evaluation experiments involving normal-hearing to assess the effectiveness of Chironomie.",
      "authors": [
        "Tatsumi, Kana*",
        " Sako, Shinji",
        " Ramirez, Rafael"
      ],
      "bilibili_id": "",
      "channel_name": "lp-34-tatsumi",
      "channel_url": "https://slack.com/app_redirect?channel=C063SQ5JAF8",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1d81e1FNuA4KQ101tmfRZgv_uzcntvnu7/preview",
      "poster_link": "https://drive.google.com/file/d/1VR64-Uona5b3PKRx16B8jxfrCj-sLGWG/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "034.png",
      "title": "Music Visualization using Chironomie",
      "youtube_id": ""
    },
    "forum": "350",
    "id": "350",
    "position": "34"
  },
  {
    "content": {
      "TLDR": "In the rapidly expanding field of music information retrieval (MIR), automatic transcription remains one of the most sought-after capabilities, especially for songs that employ multiple instruments. Musscribe emerges as a state-of-the-art transcription tool that addresses this challenge by integrating three distinct methodologies: demixing, harmonic dilated convolution, and joint beat tracking. Demixing is employed to isolate individual instruments within a song by separating overlapping audio sources, thus ensuring each instrument is transcribed distinctly. Beat tracking is then run as a parallel process to extract the joint beat and downbeat estimations. These processes results in an output midi file, which is then quantized using information derived from the beat tracking. As such, this method paves the way for more accurate and sophisticated analyses, bridging the gap between human and machine understanding of music. Together, these methodologies allow us to produce transcriptions that are not only accurate but also highly representative of the original compositions. Preliminary tests and evaluations showcase the potential in transcribing complex musical pieces with high fidelity, outperforming many contemporary tools in the market. This innovative approach not only has implications for music transcription but also for broader applications in audio analysis, remixing, and digital music production. The model has been instrumental in accelerating the composition process for several Norwegian television shows. Moreover, its efficacy can be observed in the Netflix series \"A Storm for Christmas.\" Renowned composer Peter Baden harnessed this tool to enhance his workflow, proving the demand for innovative tools like this in the professional music industry. ",
      "abstract": "In the rapidly expanding field of music information retrieval (MIR), automatic transcription remains one of the most sought-after capabilities, especially for songs that employ multiple instruments. Musscribe emerges as a state-of-the-art transcription tool that addresses this challenge by integrating three distinct methodologies: demixing, harmonic dilated convolution, and joint beat tracking. Demixing is employed to isolate individual instruments within a song by separating overlapping audio sources, thus ensuring each instrument is transcribed distinctly. Beat tracking is then run as a parallel process to extract the joint beat and downbeat estimations. These processes results in an output midi file, which is then quantized using information derived from the beat tracking. As such, this method paves the way for more accurate and sophisticated analyses, bridging the gap between human and machine understanding of music. Together, these methodologies allow us to produce transcriptions that are not only accurate but also highly representative of the original compositions. Preliminary tests and evaluations showcase the potential in transcribing complex musical pieces with high fidelity, outperforming many contemporary tools in the market. This innovative approach not only has implications for music transcription but also for broader applications in audio analysis, remixing, and digital music production. The model has been instrumental in accelerating the composition process for several Norwegian television shows. Moreover, its efficacy can be observed in the Netflix series \"A Storm for Christmas.\" Renowned composer Peter Baden harnessed this tool to enhance his workflow, proving the demand for innovative tools like this in the professional music industry. ",
      "authors": [
        "Monstad, Lars L*",
        " Lartillot, Olivier"
      ],
      "bilibili_id": "",
      "channel_name": "lp-35-monstad",
      "channel_url": "https://slack.com/app_redirect?channel=C063WFWJVNH",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1BtsicT43bP7wFyHIyYIXGV1iK0wbNG0l/preview",
      "poster_link": "https://drive.google.com/file/d/169OykNG250k2d73dJd9XalFYeKEQTu8h/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "035.png",
      "title": "AUTOMATIC TRANSCRIPTION OF MULTI-INSTRUMENTAL SONGS: INTEGRATING DEMIXING, HARMONIC DILATED CONVOLUTION, AND JOINT BEAT TRACKING",
      "youtube_id": ""
    },
    "forum": "351",
    "id": "351",
    "position": "35"
  },
  {
    "content": {
      "TLDR": "Recently, multi-band spectrogram-based approaches such as Band-Split RNN (BSRNN) have demonstrated promising results for music source separation. In our recent work, we introduce the BS-RoFormer model which inherits the idea of band-split scheme in BSRNN at the front-end, and then uses the hierarchical Transformer with Rotary Position Embedding (RoPE) to model the inner-band and inter-band sequences for multi-band mask estimation. This model has achieved state-of-the-art performance, but the band-split scheme is defined empirically, without analytic supports from the literature. In this paper, we propose Mel-RoFormer, which adopts the Mel-band scheme that maps the frequency bins into overlapped subbands according to the mel scale. In contract, the band-split mapping in BSRNN and BS-RoFormer is non-overlapping and designed based on heuristics. Using the MUSDB18HQ dataset for experiments, we demonstrate that Mel-RoFormer outperforms BS-RoFormer in the separation tasks of vocals, drums, and other stems.",
      "abstract": "Recently, multi-band spectrogram-based approaches such as Band-Split RNN (BSRNN) have demonstrated promising results for music source separation. In our recent work, we introduce the BS-RoFormer model which inherits the idea of band-split scheme in BSRNN at the front-end, and then uses the hierarchical Transformer with Rotary Position Embedding (RoPE) to model the inner-band and inter-band sequences for multi-band mask estimation. This model has achieved state-of-the-art performance, but the band-split scheme is defined empirically, without analytic supports from the literature. In this paper, we propose Mel-RoFormer, which adopts the Mel-band scheme that maps the frequency bins into overlapped subbands according to the mel scale. In contract, the band-split mapping in BSRNN and BS-RoFormer is non-overlapping and designed based on heuristics. Using the MUSDB18HQ dataset for experiments, we demonstrate that Mel-RoFormer outperforms BS-RoFormer in the separation tasks of vocals, drums, and other stems.",
      "authors": [
        "Wang, Ju-Chiang*",
        " Lu, Wei-Tsung",
        " Won, Minz"
      ],
      "bilibili_id": "",
      "channel_name": "lp-36-wang",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZAT4HUK",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1SeS_P8YnbwbwLM3uWFeSHLMwNcZJ_avA/preview",
      "poster_link": "https://drive.google.com/file/d/1HyNJON4mmVE_umH2jp06F2Wi896itCY8/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "036.png",
      "title": "Mel-Band RoFormer for Music Source Separation",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1sB9v9J3KYJXpWY9YSSLvqJt0KqP4nkJr"
    },
    "forum": "353",
    "id": "353",
    "position": "36"
  },
  {
    "content": {
      "TLDR": "The VocalNotes dataset is a collection of audio and annotations for excerpts of vocal performances from five musical traditions - Japanese Minyo, Chinese Hebei Bangzi opera, Russian traditional singing, Alpine yodel and Jewish Romaniote chant. For each tradition the dataset contains: about 10 minutes of audio; documentation for the songs from which annotated fragments originate; f0, independent onset, offset and note pitch annotations created by two or three experts; The dataset was created as part of the VocalNotes project [1]. It is released under CC-BY-NC-SA license and can be accessed by filling out a request form.",
      "abstract": "The VocalNotes dataset is a collection of audio and annotations for excerpts of vocal performances from five musical traditions - Japanese Minyo, Chinese Hebei Bangzi opera, Russian traditional singing, Alpine yodel and Jewish Romaniote chant. For each tradition the dataset contains: about 10 minutes of audio; documentation for the songs from which annotated fragments originate; f0, independent onset, offset and note pitch annotations created by two or three experts; The dataset was created as part of the VocalNotes project [1]. It is released under CC-BY-NC-SA license and can be accessed by filling out a request form.",
      "authors": [
        "Proutskova, Polina*",
        " McBride, John M",
        " Ozaki, Yuto",
        " Chiba, Gakuto",
        " Li, Yukun",
        " Zhaoxin, Yu",
        " Yue, Wei",
        " Crowdus, Miranda",
        " Zuckerberg, Gabriel",
        " Velichkina, Olga",
        " Nikolaenko, Yulia",
        " Wey, Yannick",
        " Shuster, Lawrence",
        " Savage, Patrick E.",
        " Phillips, Elizabeth",
        " Killick, Andrew"
      ],
      "bilibili_id": "",
      "channel_name": "lp-37-proutskova",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDMPG2E",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1VIcWtjUY0HM94ssTCaZLeAeUCjkH7zUO/preview",
      "poster_link": "https://drive.google.com/file/d/1SrktmoICd97VQJcXrZCziqCLVTJ8NE3C/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "037.png",
      "title": "THE VocalNotes DATASET",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1xUOJ51PsMMsAFAa3TI-bXgJuarcIwxZi"
    },
    "forum": "354",
    "id": "354",
    "position": "37"
  },
  {
    "content": {
      "TLDR": "A common, and arguably innate, human response when listening to music is to tap one's foot to mark the regular pulse of the beat. A more complex form of interactive synchronization occurs when listeners tap out rhythmic patterns using their fingers, hands, or even some form of improvised drumsticks.  In this late-breaking demo, we explore this interaction by leveraging the style transfer capabilities of a neural audio synthesis model by training it on a drum dataset and feeding it tapped rhythm recordings at inference time. We also provide a concise and high-level overview of the results, which, in our assessment, not only justify further research but also establish an intriguing baseline for future investigations. Finally, we point out several future research directions.",
      "abstract": "A common, and arguably innate, human response when listening to music is to tap one's foot to mark the regular pulse of the beat. A more complex form of interactive synchronization occurs when listeners tap out rhythmic patterns using their fingers, hands, or even some form of improvised drumsticks.  In this late-breaking demo, we explore this interaction by leveraging the style transfer capabilities of a neural audio synthesis model by training it on a drum dataset and feeding it tapped rhythm recordings at inference time. We also provide a concise and high-level overview of the results, which, in our assessment, not only justify further research but also establish an intriguing baseline for future investigations. Finally, we point out several future research directions.",
      "authors": [
        "Santos, Andr\u00e9 C.*",
        " Cardoso, F. Amilcar"
      ],
      "bilibili_id": "",
      "channel_name": "lp-38-santos",
      "channel_url": "https://slack.com/app_redirect?channel=C063ZDMUCLA",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1hzKEZD6DP-WUyWbY6J2PV7_SIc3kZQGb/preview",
      "poster_link": "https://drive.google.com/file/d/1jyF43nHZDdgiFjmJrE6evWFvkDZ7-3LQ/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "038.png",
      "title": "From Taps to Drums: Audio-to-audio Percussion Style Transfer",
      "youtube_id": ""
    },
    "forum": "355",
    "id": "355",
    "position": "38"
  },
  {
    "content": {
      "TLDR": "We introduce a system that allows users of Ableton Live to create MIDI-clips by naming them with musical descriptions. Users can compose by typing the desired musical content directly in Ableton's clip view, which is then inserted by our integrated system. This allows users to stay in the flow of their creative process while quickly generating musical ideas. The system works by prompting ChatGPT to reply using one of several text-based musical formats, such as ABC notation, chord symbols, or drum tablature. This is an important step in integrating generative AI tools into pre-existing musical workflows, and could be valuable for content makers who prefer to express their creative vision through descriptive language.",
      "abstract": "We introduce a system that allows users of Ableton Live to create MIDI-clips by naming them with musical descriptions. Users can compose by typing the desired musical content directly in Ableton's clip view, which is then inserted by our integrated system. This allows users to stay in the flow of their creative process while quickly generating musical ideas. The system works by prompting ChatGPT to reply using one of several text-based musical formats, such as ABC notation, chord symbols, or drum tablature. This is an important step in integrating generative AI tools into pre-existing musical workflows, and could be valuable for content makers who prefer to express their creative vision through descriptive language.",
      "authors": [
        "Hollowell, Sven*",
        " Namgyal, Tashi",
        " Marshall, Paul"
      ],
      "bilibili_id": "",
      "channel_name": "lp-39-hollowell",
      "channel_url": "https://slack.com/app_redirect?channel=C063JTC8PV5",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/14xCaG-5JkcBn4a-WUt3eOkjIZwOKRUoZ/preview",
      "poster_link": "https://drive.google.com/file/d/1NSNE5zNzG9AmTH5BMQx6eG8-R-qsFxau/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "039.png",
      "title": "JAMMIN-GPT: Text-Based Improvisation Using LLMs in Ableton Live",
      "youtube_id": ""
    },
    "forum": "356",
    "id": "356",
    "position": "39"
  },
  {
    "content": {
      "TLDR": "We designed a visual interface for comparing different guitar tablature (tab) versions of the same piece. By automatically aligning the bars of these versions and visually encoding different metrics, our interface helps determine similarity, difficulty, and correctness. During our design, we collected and integrated feedback from musicians and finally conducted a qualitative evaluation with five guitarists. Results confirm that our interface effectively supports comparison and helps musicians choose a version appropriate for their personal skills and tastes.",
      "abstract": "We designed a visual interface for comparing different guitar tablature (tab) versions of the same piece. By automatically aligning the bars of these versions and visually encoding different metrics, our interface helps determine similarity, difficulty, and correctness. During our design, we collected and integrated feedback from musicians and finally conducted a qualitative evaluation with five guitarists. Results confirm that our interface effectively supports comparison and helps musicians choose a version appropriate for their personal skills and tastes.",
      "authors": [
        "Heyen, Frank*",
        " Diaz Mendoza, Alejandro Gabino",
        " Ngo, Quynh Quang",
        " Sedlmair, Michael"
      ],
      "bilibili_id": "",
      "channel_name": "lp-40-heyen",
      "channel_url": "https://slack.com/app_redirect?channel=C063SQ6JA6A",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1KvFlXGEmMjySR_h3lmAz0CDK3thHS3PP/preview",
      "poster_link": "https://drive.google.com/file/d/11Jjpuw0R40KlD7cmLeOMnypYGLFY0got/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "040.png",
      "title": "Visual Guitar Tab Comparison",
      "youtube_id": ""
    },
    "forum": "357",
    "id": "357",
    "position": "40"
  },
  {
    "content": {
      "TLDR": "In this study, we examined different facets of the mridangam stroke transcription pipeline. We employed datasets comprising of mridangam strokes, some artificially generated through simulations using a browser application in which the strokes can be generated by typing the syllables on a text window, and others recorded authentically. We investigate the effectiveness of clustering techniques on the task of stroke classification. Additionally, pre-trained residual neural networks (ResNet) were also examined by fine-tuning them on the task. Our initial findings, supported by existing literature, underscore the necessity of introducing additional steps to address composite strokes (strokes created by combining two basic strokes) and accommodate full-length recordings of the mridangam. We also delved into the task of audio segmentation using onset detection techniques as part of our broader exploration within this context. Our aim was to segment mridangam recordings into individual strokes. Our preliminary finding suggests that utilizing spectral flux for onset detection, coupled with a post-processing step involving a local average computation step, produced promising results. ",
      "abstract": "In this study, we examined different facets of the mridangam stroke transcription pipeline. We employed datasets comprising of mridangam strokes, some artificially generated through simulations using a browser application in which the strokes can be generated by typing the syllables on a text window, and others recorded authentically. We investigate the effectiveness of clustering techniques on the task of stroke classification. Additionally, pre-trained residual neural networks (ResNet) were also examined by fine-tuning them on the task. Our initial findings, supported by existing literature, underscore the necessity of introducing additional steps to address composite strokes (strokes created by combining two basic strokes) and accommodate full-length recordings of the mridangam. We also delved into the task of audio segmentation using onset detection techniques as part of our broader exploration within this context. Our aim was to segment mridangam recordings into individual strokes. Our preliminary finding suggests that utilizing spectral flux for onset detection, coupled with a post-processing step involving a local average computation step, produced promising results. ",
      "authors": [
        "Krishnan, Gopika*",
        " Ganguli, Kaustuv",
        " Guedes, Carlos"
      ],
      "bilibili_id": "",
      "channel_name": "lv-41-krishnan",
      "channel_url": "https://slack.com/app_redirect?channel=C064NDD89EU",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1DhlhRTFYW7AARv3GxmO1J0JZudv3sVa-/preview",
      "poster_link": "https://drive.google.com/file/d/1-7w_w2S_7NQ6nzAGcD4xz1La6eGiQLNV/preview",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "041.png",
      "title": "Optimizing the Mridangam Stroke Transcription Pipeline: Addressing Key Challenges",
      "youtube_id": ""
    },
    "forum": "358",
    "id": "358",
    "position": "41"
  },
  {
    "content": {
      "TLDR": "In this preliminary study, we investigate the ability of disentangled sequential autoencoders, an unsupervised deep-generative model, to learn representations of the voice timbre for the task of speaker or singer verification. Despite its simplicity, the framework yields promising performance, and we plan to conduct a comprehensive benchmark with state-of-the-art self-supervised learning models.",
      "abstract": "In this preliminary study, we investigate the ability of disentangled sequential autoencoders, an unsupervised deep-generative model, to learn representations of the voice timbre for the task of speaker or singer verification. Despite its simplicity, the framework yields promising performance, and we plan to conduct a comprehensive benchmark with state-of-the-art self-supervised learning models.",
      "authors": [
        "Luo, Yin-Jyun*",
        " Dixon, Simon"
      ],
      "bilibili_id": "",
      "channel_name": "lv-42-luo",
      "channel_url": "https://slack.com/app_redirect?channel=C064HKGJ06Q",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1VMrQVBTxKzKi6FZKAq82nQ8xTAgh4nY0/preview",
      "poster_link": "https://drive.google.com/file/d/1f37xw4vKwM0tWAD0FX6fNkNHRw0tLPLX/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "042.png",
      "title": "SPEAKER AND SINGER VERIFICATION USING A DISENTANGLED VOCAL TIMBRE REPRESENTATION",
      "youtube_id": ""
    },
    "forum": "359",
    "id": "359",
    "position": "42"
  },
  {
    "content": {
      "TLDR": "We present two inverted dictation ear training games, now included in the upgraded Troubadour platform for music theory learning. The Troubadour platform offers e-learning games for ear training in rhythm, intervals and harmonies in an open-sourced and user-centred environment. With now added inverted dictation games, the platform offers a new modality of ear training, which enables the students to practice their voice or instruments individually without the necessity of the teacher's presence while practicing.",
      "abstract": "We present two inverted dictation ear training games, now included in the upgraded Troubadour platform for music theory learning. The Troubadour platform offers e-learning games for ear training in rhythm, intervals and harmonies in an open-sourced and user-centred environment. With now added inverted dictation games, the platform offers a new modality of ear training, which enables the students to practice their voice or instruments individually without the necessity of the teacher's presence while practicing.",
      "authors": [
        "Pesek, Matev\u017e*",
        " Podbreznik, Matija",
        " \u0160avli, Peter",
        " Marolt, Matija"
      ],
      "bilibili_id": "",
      "channel_name": "lv-43-pesek",
      "channel_url": "https://slack.com/app_redirect?channel=C0640L6KRST",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1OloK4iEPeuPnZc_Y_3dKzqH6P3JWq20l/preview",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "043.png",
      "title": "Inverted dictation games for home and remote ear training",
      "youtube_id": ""
    },
    "forum": "360",
    "id": "360",
    "position": "43"
  },
  {
    "content": {
      "TLDR": "We propose \\textit{Coco-Mulla}, a plug-and-play module that adds direct content-based controls (chords, drums, and piano-roll) to existing text-to-music large language models, e.g.  MusicGen. Coco-Mulla employs the Parameter-Efficient Fine-Tuning (PEFT) method, which only requires fine-tuning with fewer than 4\\% of the original model's parameters on a dataset containing fewer than 300 songs. Experiments show that our approach achieves effective content-based controls while keeping the quality of generated music. Furthermore, by combining content-based controls and text descriptions, our system achieves flexible music variation generation and style transfer. \\footnote{Our source codes and demos are available at \\url{https://kikyo-16.github.io/coco-mulla}.}",
      "abstract": "We propose \\textit{Coco-Mulla}, a plug-and-play module that adds direct content-based controls (chords, drums, and piano-roll) to existing text-to-music large language models, e.g.  MusicGen. Coco-Mulla employs the Parameter-Efficient Fine-Tuning (PEFT) method, which only requires fine-tuning with fewer than 4\\% of the original model's parameters on a dataset containing fewer than 300 songs. Experiments show that our approach achieves effective content-based controls while keeping the quality of generated music. Furthermore, by combining content-based controls and text descriptions, our system achieves flexible music variation generation and style transfer. \\footnote{Our source codes and demos are available at \\url{https://kikyo-16.github.io/coco-mulla}.}",
      "authors": [
        "Lin, Liwei*",
        " Xia, Guangyu",
        " Jiang, Junyan",
        " Zhang, Yixiao"
      ],
      "bilibili_id": "",
      "channel_name": "lv-44-lin",
      "channel_url": "https://slack.com/app_redirect?channel=C064F6S0NUA",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/19S9qbuJeSHtJ25NkuW1sht8Z4VlNx-Hp/preview",
      "poster_link": "https://drive.google.com/file/d/1-s1d4e7GdfJ9oQEcI6k2Nf_EPr_jVQdS/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "044.png",
      "title": "Equipping MusicGen with Chord and Rhythm Controls",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1zyhZaJKKud7llIO0J9eiPTEpwSg7phR1"
    },
    "forum": "361",
    "id": "361",
    "position": "44"
  },
  {
    "content": {
      "TLDR": "The proliferation of Large Language Models (LLMs) has significantly revolutionized Natural Language Processing. At Utopia, we developed \u201cEnhance Lyrics\u201d, a lyrics-based mood/theme detection and keyword generation tool, which combines predictions from LLMs with  smaller transformer models. Specifically, instead of fine tuning the LLMs for our downstream tasks, we use them in a zero-shot fashion, followed by a much smaller  transformer finetuned with our downstream task datasets. Our research suggests a hybrid approach can enhance performance compared to leveraging only LLMs. ",
      "abstract": "The proliferation of Large Language Models (LLMs) has significantly revolutionized Natural Language Processing. At Utopia, we developed \u201cEnhance Lyrics\u201d, a lyrics-based mood/theme detection and keyword generation tool, which combines predictions from LLMs with  smaller transformer models. Specifically, instead of fine tuning the LLMs for our downstream tasks, we use them in a zero-shot fashion, followed by a much smaller  transformer finetuned with our downstream task datasets. Our research suggests a hybrid approach can enhance performance compared to leveraging only LLMs. ",
      "authors": [
        "Sarig\u00f6l, Emre*",
        " Reetz, Steffen",
        " Hu, Rui",
        " Lidy, Thomas"
      ],
      "bilibili_id": "",
      "channel_name": "lv-45-sarig\u00f6l",
      "channel_url": "https://slack.com/app_redirect?channel=C0648HHC89L",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1n1x4JlGuAnl0yV7zNs2uTvYxKzgm-mWo/preview",
      "poster_link": "https://drive.google.com/file/d/17ANS9VsWujCRgHNGy96DBLBK_PlH-Jya/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "045.png",
      "title": "ENHANCE LYRICS: Blending Large and Small Transformers for in-depth Song Analysis",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1ibmG5BDCSX2IQfEDuGzi0KIHTwN7_9HW"
    },
    "forum": "362",
    "id": "362",
    "position": "45"
  },
  {
    "content": {
      "TLDR": "It is standard practice to use spectrograms as input features for discriminating MIR tasks. However, recent research showed using representations produced by Jukebox (a music language model) led to better model performance. This was tested on music tagging, genre classification, key detection, emotion recognition, and music transcription. In this paper, we test it on beat and downbeat tracking. Specifically, we compare compressed Jukebox embeddings with spectrograms as input to a model that jointly predicts beat, downbeat, and tempo. Experiments show that the two inputs bring comparable results for beat tracking, while using Jukebox embeddings leads to significant improvements for downbeat tracking.",
      "abstract": "It is standard practice to use spectrograms as input features for discriminating MIR tasks. However, recent research showed using representations produced by Jukebox (a music language model) led to better model performance. This was tested on music tagging, genre classification, key detection, emotion recognition, and music transcription. In this paper, we test it on beat and downbeat tracking. Specifically, we compare compressed Jukebox embeddings with spectrograms as input to a model that jointly predicts beat, downbeat, and tempo. Experiments show that the two inputs bring comparable results for beat tracking, while using Jukebox embeddings leads to significant improvements for downbeat tracking.",
      "authors": [
        "Tian, Haokun*",
        " Liu, Kun",
        " Fuentes, Magdalena"
      ],
      "bilibili_id": "",
      "channel_name": "lv-46-tian",
      "channel_url": "https://slack.com/app_redirect?channel=C0640LXA1D5",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/12iEm6Tk5tYJkgQXX2nRUyUKy-wtd68HE/preview",
      "poster_link": "https://drive.google.com/file/d/1rtZ0PNWeph_gBMNKjw7trLvTELT9Gkd4/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "046.png",
      "title": "BEAT AND DOWNBEAT TRACKING WITH GENERATIVE EMBEDDINGS",
      "youtube_id": ""
    },
    "forum": "363",
    "id": "363",
    "position": "46"
  },
  {
    "content": {
      "TLDR": "In recent years, the application of deep learning methods to guitar effect modelling has garnered considerable attention. Motivated by recent advancements in neural vocoders, we experimented with two different discriminator architectures in replacement of the discriminator employed in an existing GAN-based guitar amplifier modelling framework, for better audio quality. We report the preliminary result of evaluating these methods on the audio samples of 3 amplifiers from the EGDB dataset.",
      "abstract": "In recent years, the application of deep learning methods to guitar effect modelling has garnered considerable attention. Motivated by recent advancements in neural vocoders, we experimented with two different discriminator architectures in replacement of the discriminator employed in an existing GAN-based guitar amplifier modelling framework, for better audio quality. We report the preliminary result of evaluating these methods on the audio samples of 3 amplifiers from the EGDB dataset.",
      "authors": [
        "Chen, Yu-Hua*",
        " Choi, Woosung",
        " Liao, WeiHsiang",
        " Martinez Ramirez, Marco A",
        " Cheuk, Kin Wai",
        " Yang, Yi-Hsuan",
        " Mitsufuji, Yuki"
      ],
      "bilibili_id": "",
      "channel_name": "lv-47-chen",
      "channel_url": "https://slack.com/app_redirect?channel=C0640M3HKM5",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1L5jhq9c5xon-EtnXlABh1lj2htN_vOKv/preview",
      "poster_link": "https://drive.google.com/file/d/1OIzrZP8HpyLxYuMON2bvlRE0Ty7iye28/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "047.png",
      "title": "Neural Amplifier Modelling with several GAN variants",
      "youtube_id": "https://drive.google.com/uc?export=view&id=1iRgMDVgoP9aF8JEiqPPyp96wTvBYBA7s"
    },
    "forum": "364",
    "id": "364",
    "position": "47"
  },
  {
    "content": {
      "TLDR": "We present an automatic accompaniment method for lead sheets, allowing a user to play along to a favorite piece of music in a semi-improvised manner. We realize this by combining a score follower with a downbeat phase tracker, using a score follower to track at a harmony level, while the exact position inside the measure is narrowed down by a downbeat tracker. The two information sources are fused by model averaging. To prevent the downbeat tracker from tracking tempi at octaves of the correct tempo, the downbeat tracker is conditioned on the target bpm of the performance. We will present a live demo.",
      "abstract": "We present an automatic accompaniment method for lead sheets, allowing a user to play along to a favorite piece of music in a semi-improvised manner. We realize this by combining a score follower with a downbeat phase tracker, using a score follower to track at a harmony level, while the exact position inside the measure is narrowed down by a downbeat tracker. The two information sources are fused by model averaging. To prevent the downbeat tracker from tracking tempi at octaves of the correct tempo, the downbeat tracker is conditioned on the target bpm of the performance. We will present a live demo.",
      "authors": [
        "Maezawa, Akira*"
      ],
      "bilibili_id": "",
      "channel_name": "lv-48-maezawa",
      "channel_url": "https://slack.com/app_redirect?channel=C0640M96615",
      "day": 4,
      "paper_link": "https://drive.google.com/file/d/1t7IGJaipOjjTMIwGrDAtv7VRNRMWc1vv/preview",
      "poster_link": "https://drive.google.com/file/d/1tVAEx2Z1PBWWQx0u1gbfh4aBXVbHAVx2/preview",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "048.png",
      "title": "AUTOMATIC ACCOMPANIMENT FROM A LEAD-SHEET BY JOINTLY TRACKING AT CHORD AND DOWNBEAT LEVEL",
      "youtube_id": ""
    },
    "forum": "365",
    "id": "365",
    "position": "48"
  }
]
