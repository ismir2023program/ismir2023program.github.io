[
  {
    "content": {
      "TLDR": "Musicology research suggests a correspondence between manual gesture and melodic contour in raga performance. Computational tools such as pose estimation from video and time series pattern matching potentially facilitate larger-scale studies of gesture and audio correspondence. We present a dataset of audiovisual recordings of Hindustani vocal music comprising 9 ragas sung by 11 expert performers. With the automatic segmentation of the audiovisual time series based on analyses of the extracted F0 contour, we study whether melodic similarity implies gesture similarity. Our results indicate that specific representations of gesture kinematics can predict high-level melodic features such as held notes and raga-characteristic motifs significantly better than chance.\n",
      "abstract": "Musicology research suggests a correspondence between manual gesture and melodic contour in raga performance. Computational tools such as pose estimation from video and time series pattern matching potentially facilitate larger-scale studies of gesture and audio correspondence. We present a dataset of audiovisual recordings of Hindustani vocal music comprising 9 ragas sung by 11 expert performers. With the automatic segmentation of the audiovisual time series based on analyses of the extracted F0 contour, we study whether melodic similarity implies gesture similarity. Our results indicate that specific representations of gesture kinematics can predict high-level melodic features such as held notes and raga-characteristic motifs significantly better than chance.\n<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1DvOBLNNh3C8wfID8ClLVXxXqj2suDBzO)</b>",
      "authors": [
        "Shreyas M Nadkarni (Indian Institute of Technology Bombay)",
        " Sujoy Roychowdhury (Indian Institute of Technology Bombay)",
        " Preeti Rao (Indian Institute of Technology  Bombay)*",
        " Martin Clayton (Durham University)"
      ],
      "authors_and_affil": [
        "Shreyas M Nadkarni (Indian Institute of Technology Bombay)",
        " Sujoy Roychowdhury (Indian Institute of Technology Bombay)",
        " Preeti Rao (Indian Institute of Technology  Bombay)*",
        " Martin Clayton (Durham University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B57QBB3",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        "Knowledge-driven approaches to MIR -> computational ethnomusicology"
      ],
      "long_presentation": "True",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000001.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1SJX_IvazSTMMezLbKk_2kT-4g3_pSFev/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-01-rao",
      "title": "Exploring the Correspondence of Melodic Contour With Gesture in Raga Alap Singing",
      "video": "https://drive.google.com/uc?export=view&id=1DvOBLNNh3C8wfID8ClLVXxXqj2suDBzO"
    },
    "forum": "64",
    "id": "64",
    "pic_id": "https://drive.google.com/file/d/1GlniwEHuaE5n0Zei7oXLyVn_dHGy0bmm/view?usp=sharing",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Thanks to advancements in deep learning (DL), automatic music transcription (AMT) systems recently outperformed previous ones fully based on manual feature design. Many of these highly capable DL models, however, are computationally expensive. Researchers are moving towards smaller models capable of maintaining state-of-the-art (SOTA) results by embedding musical knowledge in the network architecture. Existing approaches employ convolutional blocks specifically designed to capture the harmonic structure. These approaches, however, require either large kernels or multiple kernels, with each kernel aiming to capture a different harmonic. We present TriAD, a convolutional block that achieves an unequally distanced dilation over the frequency axis. This allows our method to capture multiple harmonics with a single yet small kernel. We compare TriAD with other methods of capturing harmonics, and we observe that our approach maintains SOTA results while reducing the number of parameters required. We also conduct an ablation study showing that our proposed method effectively relies on harmonic information.",
      "abstract": "Thanks to advancements in deep learning (DL), automatic music transcription (AMT) systems recently outperformed previous ones fully based on manual feature design. Many of these highly capable DL models, however, are computationally expensive. Researchers are moving towards smaller models capable of maintaining state-of-the-art (SOTA) results by embedding musical knowledge in the network architecture. Existing approaches employ convolutional blocks specifically designed to capture the harmonic structure. These approaches, however, require either large kernels or multiple kernels, with each kernel aiming to capture a different harmonic. We present TriAD, a convolutional block that achieves an unequally distanced dilation over the frequency axis. This allows our method to capture multiple harmonics with a single yet small kernel. We compare TriAD with other methods of capturing harmonics, and we observe that our approach maintains SOTA results while reducing the number of parameters required. We also conduct an ablation study showing that our proposed method effectively relies on harmonic information.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1yFCCW_X06c_V-Q_xlfOnCbM8JoNzIVtu)</b>",
      "authors": [
        "Miguel Perez Fernandez (Universitat Pompeu Fabra",
        " Huawei)*",
        " Holger Kirchhoff (Huawei)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "authors_and_affil": [
        "Miguel Perez Fernandez (Universitat Pompeu Fabra",
        " Huawei)*",
        " Holger Kirchhoff (Huawei)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J0VR14P",
      "day": "2",
      "keywords": [
        " Knowledge-driven approaches to MIR -> representations of music",
        " Musical features and properties -> representations of music",
        " MIR tasks -> pattern matching and detection",
        "MIR tasks -> music transcription and annotation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000002.pdf",
      "poster_pdf": "https://drive.google.com/file/d/14jgsYz5Nb8m3OGaiW4RxQe26yi35tyhL/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-02-fernandez",
      "title": "TriAD: Capturing Harmonics With 3D Convolutions",
      "video": "https://drive.google.com/uc?export=view&id=1yFCCW_X06c_V-Q_xlfOnCbM8JoNzIVtu"
    },
    "forum": "11",
    "id": "11",
    "pic_id": "https://drive.google.com/file/d/1juo5XqKdMdu8902Kq6GWZ2BOAE1wCFw4/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The practices of data collection in training sets for Automatic Music Generation (AMG) tasks are opaque and overlooked. In this paper, we aimed to identify these practices and surface the values they embed. We systematically identified all datasets used to train AMG models presented at the last ten editions of ISMIR. For each dataset, we checked how it was populated and the extent to which musicians wittingly contributed to its creation.\\ Almost half of the datasets (42.6%) were indiscriminately populated by accumulating music data available online without seeking any sort of permission. We discuss the ideologies that underlie this practice and propose a number of suggestions AMG dataset creators might follow. Overall, this paper contributes to the emerging  self-critical corpus of work of the ISMIR community, reflecting on the ethical considerations and the social responsibility of our work.",
      "abstract": "The practices of data collection in training sets for Automatic Music Generation (AMG) tasks are opaque and overlooked. In this paper, we aimed to identify these practices and surface the values they embed. We systematically identified all datasets used to train AMG models presented at the last ten editions of ISMIR. For each dataset, we checked how it was populated and the extent to which musicians wittingly contributed to its creation.\\ Almost half of the datasets (42.6%) were indiscriminately populated by accumulating music data available online without seeking any sort of permission. We discuss the ideologies that underlie this practice and propose a number of suggestions AMG dataset creators might follow. Overall, this paper contributes to the emerging  self-critical corpus of work of the ISMIR community, reflecting on the ethical considerations and the social responsibility of our work.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1-QENG5zYQSGwLoG9VowxWCQ2p9GqEKf5)</b>",
      "authors": [
        "Fabio Morreale (University of Auckland)*",
        " Megha Sharma (University of Tokyo)",
        " I-Chieh Wei (University of Auckland)"
      ],
      "authors_and_affil": [
        "Fabio Morreale (University of Auckland)*",
        " Megha Sharma (University of Tokyo)",
        " I-Chieh Wei (University of Auckland)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B5Y847K",
      "day": "2",
      "keywords": [
        "Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",
        " Philosophical and ethical discussions -> legal and societal aspects of MIR",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000003.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1UqLBTjYlhPcuEpl676ybWk9Nq0kGb9H8/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-03-morreale",
      "title": "Data Collection in Music Generation Training Sets: A Critical Analysis",
      "video": "https://drive.google.com/uc?export=view&id=1-QENG5zYQSGwLoG9VowxWCQ2p9GqEKf5"
    },
    "forum": "15",
    "id": "15",
    "pic_id": "https://drive.google.com/file/d/1LO7bnd58OVp9yfX6fId8yV5tJF-rIk68/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Validity is the truth of an inference made from evidence and is a central concern in scientific work. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Puzzling MIR phenomena like adversarial attacks, horses, and performance glass ceilings become less mysterious through the lens of validity. In this paper, we review the subject of validity as presented in a key reference of causal inference: Shadish et al., Experimental and Quasi-experimental Designs for Generalised Causal Inference [1]. We discuss the four types of validity and threats to each one. We consider them in relationship to MIR experiments grounded with a practical demonstration using a typical MIR experiment.",
      "abstract": "Validity is the truth of an inference made from evidence and is a central concern in scientific work. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Puzzling MIR phenomena like adversarial attacks, horses, and performance glass ceilings become less mysterious through the lens of validity. In this paper, we review the subject of validity as presented in a key reference of causal inference: Shadish et al., Experimental and Quasi-experimental Designs for Generalised Causal Inference [1]. We discuss the four types of validity and threats to each one. We consider them in relationship to MIR experiments grounded with a practical demonstration using a typical MIR experiment.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1gr803BCvb98rdFiAzIpGivrGkgEtNkt-)</b>",
      "authors": [
        "Bob L. T.  Sturm (KTH Royal Institute of Technology)",
        " Arthur Flexer (Johannes Kepler University Linz)*"
      ],
      "authors_and_affil": [
        "Bob L. T.  Sturm (KTH Royal Institute of Technology)",
        " Arthur Flexer (Johannes Kepler University Linz)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VJZQLER",
      "day": "2",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> evaluation methodology",
        "Philosophical and ethical discussions",
        " Philosophical and ethical discussions -> philosophical and methodological foundations",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000004.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1J9FH9FKTPCGRiRyCMbB_23NdfKI-7JW8/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-04-flexer",
      "title": "A Review of Validity and Its Relationship to Music Information Research",
      "video": "https://drive.google.com/uc?export=view&id=1gr803BCvb98rdFiAzIpGivrGkgEtNkt-"
    },
    "forum": "18",
    "id": "18",
    "pic_id": "https://drive.google.com/file/d/1LV47ziqlkzs6AtO4SW_7iPcVeUW8BRO6/view",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "In Carnatic music concerts, taniavartanam is a solo percussion segment that showcases intricate and elaborate extempore rhythmic evolution through a series of homogeneous sections with shared rhythmic characteristics. While taniavartanam segments have been segmented from concerts earlier, no effort has been made to analyze these percussion segments. This paper attempts to further segment the taniavartanam portion into musically meaningful segments. A taniavartanam segment consists of an abhipraya, where artists show their prowess at extempore enunciation of percussion stroke segments, followed by an optional korapu, where each artist challenges the other, and concluding with mohra and korvai, each with its own nuances. This work helps obtain a comprehensive musical description of the taniavartanam in Carnatic concerts. However, analysis is complicated owing to a plethora of tala and nade. The segmentation of a taniavartanam section can be used for further analysis, such as stroke sequence recognition, and help find relations between different learning schools. The study uses 12 hours of taniavartanam segments consisting of four tala-s and five nade-s for analysis and achieves 0.85 F1-score in the segmentation task.",
      "abstract": "In Carnatic music concerts, taniavartanam is a solo percussion segment that showcases intricate and elaborate extempore rhythmic evolution through a series of homogeneous sections with shared rhythmic characteristics. While taniavartanam segments have been segmented from concerts earlier, no effort has been made to analyze these percussion segments. This paper attempts to further segment the taniavartanam portion into musically meaningful segments. A taniavartanam segment consists of an abhipraya, where artists show their prowess at extempore enunciation of percussion stroke segments, followed by an optional korapu, where each artist challenges the other, and concluding with mohra and korvai, each with its own nuances. This work helps obtain a comprehensive musical description of the taniavartanam in Carnatic concerts. However, analysis is complicated owing to a plethora of tala and nade. The segmentation of a taniavartanam section can be used for further analysis, such as stroke sequence recognition, and help find relations between different learning schools. The study uses 12 hours of taniavartanam segments consisting of four tala-s and five nade-s for analysis and achieves 0.85 F1-score in the segmentation task.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1u1UzWleSWCC83KIgrl-JyiHyVkfqVDd6)</b>",
      "authors": [
        "Gowriprasad R (IIT Madras)*",
        " Srikrishnan Sridharan (Carnatic Percussionist)",
        " R Aravind (Indian Institute of Technology Madras)",
        " Hema A Murthy (IIT Madras)"
      ],
      "authors_and_affil": [
        "Gowriprasad R (IIT Madras)*",
        " Srikrishnan Sridharan (Carnatic Percussionist)",
        " R Aravind (Indian Institute of Technology Madras)",
        " Hema A Murthy (IIT Madras)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YDWBJ67",
      "day": "2",
      "keywords": [
        "Musical features and properties -> structure, segmentation, and form",
        "Applications -> music retrieval systems"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000005.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1_GZqiSYYzUFfLsvr2L8qPjzZYFQF9Iei/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-05-r",
      "title": "Segmentation and Analysis of Taniavartanam in Carnatic Music Concerts",
      "video": "https://drive.google.com/uc?export=view&id=1u1UzWleSWCC83KIgrl-JyiHyVkfqVDd6"
    },
    "forum": "19",
    "id": "19",
    "pic_id": "https://drive.google.com/file/d/1HvjtFZBzPh8_T1ICDvb7yBk3BK6mfBv6/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.",
      "abstract": "Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1QsT16gA_H-9jCpNLm_lV2flwRr7H_NY8)</b>",
      "authors": [
        "Changhong Wang (Telecom Paris, Institut polytechnique de Paris)*",
        " Ga\u00ebl Richard (Telecom Paris, Institut polytechnique de Paris)",
        " Brian McFee (New York University)"
      ],
      "authors_and_affil": [
        "Changhong Wang (Telecom Paris, Institut polytechnique de Paris)*",
        " Ga\u00ebl Richard (Telecom Paris, Institut polytechnique de Paris)",
        " Brian McFee (New York University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YGS69S6",
      "day": "2",
      "keywords": [
        "Musical features and properties -> timbre, instrumentation, and singing voice",
        " Knowledge-driven approaches to MIR -> representations of music",
        " MIR tasks -> automatic classification",
        " Philosophical and ethical discussions -> philosophical and methodological foundations",
        "Applications -> music retrieval systems",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000006.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1xSh8IkEE4LD6SYbg-TwJfMFSNYPX4s1G/view?usp=sharing",
      "session": [
        "1"
      ],
      "slack_channel": "p1-06-wang",
      "title": "Transfer Learning and Bias Correction With Pre-Trained Audio Embeddings",
      "video": "https://drive.google.com/uc?export=view&id=1QsT16gA_H-9jCpNLm_lV2flwRr7H_NY8"
    },
    "forum": "280",
    "id": "280",
    "pic_id": "https://drive.google.com/file/d/1CX9JLGWqc1qrqyLtjHC5hA85qiBjqvYG/view?usp=share_link",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010\u20132019 Billboard \u201cHot 100\u201d year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men\u2019s voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women\u2019s voices. ",
      "abstract": "The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010\u20132019 Billboard \u201cHot 100\u201d year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, panning, and gender of the performers), and relevant metadata. CoSoD complements other popular music datasets by focusing exclusively on musical collaborations between independent acts. In addition to facilitating the study of song form and vocal production, CoSoD allows for the in-depth study of gender as it relates to various timbral, pitch, and formal parameters in musical collaborations. In this paper, we detail the contents of the dataset and outline the annotation process. We also present an experiment using CoSoD that examines how the use of reverberation, layering, and panning are related to the gender of the artist. In this experiment, we find that men\u2019s voices are on average treated with less reverberation and occupy a more narrow position in the stereo mix than women\u2019s voices. <br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1GFvSckyxGyvPrMzXF7m5K-aq-4ov5sAn)</b>",
      "authors": [
        "Mich\u00e8le Duguay (Harvard University)*",
        " Kate Mancey (Harvard University)",
        " Johanna Devaney (Brooklyn College)"
      ],
      "authors_and_affil": [
        "Mich\u00e8le Duguay (Harvard University)*",
        " Kate Mancey (Harvard University)",
        " Johanna Devaney (Brooklyn College)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B645ETB",
      "day": "2",
      "keywords": [
        "Musical features and properties -> timbre, instrumentation, and singing voice",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000007.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1OGexuCofkntBg7wsXCBzB1KLrT30cDXs/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-07-duguay",
      "title": "Collaborative Song Dataset (CoSoD): An Annotated Dataset of Multi-Artist Collaborations in Popular Music",
      "video": "https://drive.google.com/uc?export=view&id=1GFvSckyxGyvPrMzXF7m5K-aq-4ov5sAn"
    },
    "forum": "37",
    "id": "37",
    "pic_id": "https://drive.google.com/file/d/15hqeiDppOtjRo2idW-bo4NYiSJMlMXLK/view",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Recently, there has been a surge in Artificial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-based art creation tools - tend to be polarizing, with opinions ranging from enthusiasm about their potential to fear about how these tools will impact the livelihood and creativity of human creators. However, a more desirable future path is most likely somewhere in between these two polar opposites where productive and ethical human-AI collaboration could happen through the use of these tools. To explore this possibility, we first need to improve our understanding of how music creators perceive and utilize these types of tools in their creative process. We conducted case studies of a range of music creators to better understand their perception and usage of AI-based music creation tools. Through a thematic analysis of these cases, we identify the opportunities and challenges related to the use of AI for music creation from the perspective of the musicians and discuss the design implications for AI music tools.",
      "abstract": "Recently, there has been a surge in Artificial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-based art creation tools - tend to be polarizing, with opinions ranging from enthusiasm about their potential to fear about how these tools will impact the livelihood and creativity of human creators. However, a more desirable future path is most likely somewhere in between these two polar opposites where productive and ethical human-AI collaboration could happen through the use of these tools. To explore this possibility, we first need to improve our understanding of how music creators perceive and utilize these types of tools in their creative process. We conducted case studies of a range of music creators to better understand their perception and usage of AI-based music creation tools. Through a thematic analysis of these cases, we identify the opportunities and challenges related to the use of AI for music creation from the perspective of the musicians and discuss the design implications for AI music tools.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=12ZEKkCsiIM7gYj_Qkrc5ol0FGDtKb9EX)</b>",
      "authors": [
        "Michele Newman (University of Washington)*",
        " Lidia J Morris (University of Washington)",
        " Jin Ha Lee (University of Washington)"
      ],
      "authors_and_affil": [
        "Michele Newman (University of Washington)*",
        " Lidia J Morris (University of Washington)",
        " Jin Ha Lee (University of Washington)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGHGZCG",
      "day": "2",
      "keywords": [
        " Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",
        "Applications -> music composition, performance, and production",
        "Human-centered MIR -> human-computer interaction",
        " MIR tasks -> music generation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000008.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1lTcgeRKggJmoOSX44-Sg1GrrZ2lurfl6/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-08-newman",
      "title": "Human-AI Music Creation: Understanding the Perceptions and Experiences of Music Creators for Ethical and Productive Collaboration",
      "video": "https://drive.google.com/uc?export=view&id=12ZEKkCsiIM7gYj_Qkrc5ol0FGDtKb9EX"
    },
    "forum": "58",
    "id": "58",
    "pic_id": "https://drive.google.com/file/d/1sOy7sZgpwlBoD_QHqYP9APDTRvUqi9cw/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways, and recent research has focused on developing more efficient methods. However, the key differences between these methods are often unclear, and few studies have compared them. In this work, we analyze the current common tokenization methods and experiment with time and note duration representations. We compare the performance of these two impactful criteria on several tasks, including composer classification, emotion classification, music generation, and sequence representation. We demonstrate that explicit information leads to better results depending on the task.",
      "abstract": "Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways, and recent research has focused on developing more efficient methods. However, the key differences between these methods are often unclear, and few studies have compared them. In this work, we analyze the current common tokenization methods and experiment with time and note duration representations. We compare the performance of these two impactful criteria on several tasks, including composer classification, emotion classification, music generation, and sequence representation. We demonstrate that explicit information leads to better results depending on the task.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=19mvoQ2eSKYbbUn2HbRVjW-8Vimnb6nBh)</b>",
      "authors": [
        "Nathan Fradet (LIP6 - Sorbonne University)*",
        " Nicolas Gutowski (University of Angers)",
        " Fabien Chhel (Groupe ESEO)",
        " Jean-Pierre Briot (CNRS)"
      ],
      "authors_and_affil": [
        "Nathan Fradet (LIP6 - Sorbonne University)*",
        " Nicolas Gutowski (University of Angers)",
        " Fabien Chhel (Groupe ESEO)",
        " Jean-Pierre Briot (CNRS)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTCAHQE",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production",
        " MIR tasks -> music generation",
        " Musical features and properties -> representations of music",
        " Applications -> music retrieval systems",
        "MIR fundamentals and methodology -> symbolic music processing",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000009.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1t-Odohpt7ZGzGBqbGGJVmXYJtgHfDpY8/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-09-fradet",
      "title": "Impact of Time and Note Duration Tokenizations on Deep Learning Symbolic Music Modeling",
      "video": "https://drive.google.com/uc?export=view&id=19mvoQ2eSKYbbUn2HbRVjW-8Vimnb6nBh"
    },
    "forum": "45",
    "id": "45",
    "pic_id": "https://drive.google.com/file/d/1AaS2ikOX0wJ7eMYB15eQnRr8whwSmgA2/view",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding language. We use a probabilistic approach to control the exact timing according to patterns discovered in new analyses of existing micro-timing data (jembe drumming and Viennese waltz). This implementation also required the introduction of musical metre into Sonic Pi. The new metre and micro-timing systems are inherently flexible, and thus open to a wide range of creative possibilities including (but not limited to): creating new micro-timing profiles for additional styles; expanded definitions of metre; and the free mixing of one micro-timing style with the musical content of another. The code is freely available as a Sonic Pi plug-in and released open source at https://github.com/MaxTheComputerer/sonicpi-metre.",
      "abstract": "Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding language. We use a probabilistic approach to control the exact timing according to patterns discovered in new analyses of existing micro-timing data (jembe drumming and Viennese waltz). This implementation also required the introduction of musical metre into Sonic Pi. The new metre and micro-timing systems are inherently flexible, and thus open to a wide range of creative possibilities including (but not limited to): creating new micro-timing profiles for additional styles; expanded definitions of metre; and the free mixing of one micro-timing style with the musical content of another. The code is freely available as a Sonic Pi plug-in and released open source at https://github.com/MaxTheComputerer/sonicpi-metre.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=13IupDzqB5wZ13ihRCwvW2X4eY_0VKSww)</b>",
      "authors": [
        "Max Johnson (University of Cambridge)",
        " Mark R H Gotham (Durham)*"
      ],
      "authors_and_affil": [
        "Max Johnson (University of Cambridge)",
        " Mark R H Gotham (Durham)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YDZKV5Z",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production",
        " Musical features and properties -> rhythm, beat, tempo",
        " Human-centered MIR -> human-computer interaction",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Computational musicology -> mathematical music theory"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000010.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Ce-lVzeVmR8ZewQV4t9vv0KOzCr-VUDA/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-10-gotham",
      "title": "Musical Micro-Timing for Live Coding",
      "video": "https://drive.google.com/uc?export=view&id=13IupDzqB5wZ13ihRCwvW2X4eY_0VKSww"
    },
    "forum": "93",
    "id": "93",
    "pic_id": "https://drive.google.com/file/d/1_wE0HU7LTpsi57jFw47FQTcQQjYCxXJb/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Optical Music Recognition (OMR) is a well-established research field focused on the task of reading musical notation from images of music scores. In the standard OMR workflow, layout analysis is a critical component for identifying relevant parts of the image, such as staff lines, text, or notes. State-of-the-art approaches to this task are based on machine learning, which entails having to label a training corpus, an error-prone, laborious, and expensive task that must be performed by experts. In this paper, we propose a novel few-shot strategy for building robust models by utilizing only partial annotations, therefore requiring minimal human effort. Specifically, we introduce a masking layer and an oversampling technique to train models using a small set of annotated patches from the training images. Our proposal enables achieving high performance even with scarce training data, as demonstrated by experiments on four benchmark datasets. The results indicate that this approach achieves performance values comparable to models trained with a fully annotated corpus, but, in this case, requiring the annotation of only between 20% and 39% of this data.",
      "abstract": "Optical Music Recognition (OMR) is a well-established research field focused on the task of reading musical notation from images of music scores. In the standard OMR workflow, layout analysis is a critical component for identifying relevant parts of the image, such as staff lines, text, or notes. State-of-the-art approaches to this task are based on machine learning, which entails having to label a training corpus, an error-prone, laborious, and expensive task that must be performed by experts. In this paper, we propose a novel few-shot strategy for building robust models by utilizing only partial annotations, therefore requiring minimal human effort. Specifically, we introduce a masking layer and an oversampling technique to train models using a small set of annotated patches from the training images. Our proposal enables achieving high performance even with scarce training data, as demonstrated by experiments on four benchmark datasets. The results indicate that this approach achieves performance values comparable to models trained with a fully annotated corpus, but, in this case, requiring the annotation of only between 20% and 39% of this data.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Yk4OZSK1v7reHk99gfC_SD_i-MkAk9DZ)</b>",
      "authors": [
        "Francisco J. Castellanos (University of Alicante)*",
        " Antonio Javier Gallego (Universidad de Alicante)",
        " Ichiro Fujinaga (McGill University)"
      ],
      "authors_and_affil": [
        "Francisco J. Castellanos (University of Alicante)*",
        " Antonio Javier Gallego (Universidad de Alicante)",
        " Ichiro Fujinaga (McGill University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGK281E",
      "day": "2",
      "keywords": [
        "MIR tasks -> optical music recognition",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000011.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1_bVna_LTgc60q8uVuiVBzoM65a1qtWdg/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-11-castellanos",
      "title": "A Few-Shot Neural Approach for Layout Analysis of Music Score Images",
      "video": "https://drive.google.com/uc?export=view&id=1Yk4OZSK1v7reHk99gfC_SD_i-MkAk9DZ"
    },
    "forum": "47",
    "id": "47",
    "pic_id": "https://drive.google.com/file/d/1n06Hd9Os7bmpYO9lGfmABVz7nz7LEFnK/view",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Drummers spend extensive time practicing rudiments to develop technique, speed, coordination, and phrasing. These rudiments are often practiced on \"silent\" practice pads using only the hands. Additionally, many percussive instruments across cultures are played exclusively with the hands. Building on these concepts and inspired by Einstein's probably apocryphal quote, \"Make everything as simple as possible, but not simpler,\" we hypothesize that a dual-voice reduction could serve as a natural and meaningful compressed representation of multi-voiced drum patterns. This representation would retain more information than its corresponding monotonic representation while maintaining relative simplicity for tasks such as rhythm analysis and generation. To validate this potential representation, we investigate whether experienced drummers can consistently represent and reproduce the rhythmic essence of a given drum pattern using only their two hands. We present TapTamDrum: a novel dataset of repeated dualizations from four experienced drummers, along with preliminary analysis and tools for further exploration of the data. ",
      "abstract": "Drummers spend extensive time practicing rudiments to develop technique, speed, coordination, and phrasing. These rudiments are often practiced on \"silent\" practice pads using only the hands. Additionally, many percussive instruments across cultures are played exclusively with the hands. Building on these concepts and inspired by Einstein's probably apocryphal quote, \"Make everything as simple as possible, but not simpler,\" we hypothesize that a dual-voice reduction could serve as a natural and meaningful compressed representation of multi-voiced drum patterns. This representation would retain more information than its corresponding monotonic representation while maintaining relative simplicity for tasks such as rhythm analysis and generation. To validate this potential representation, we investigate whether experienced drummers can consistently represent and reproduce the rhythmic essence of a given drum pattern using only their two hands. We present TapTamDrum: a novel dataset of repeated dualizations from four experienced drummers, along with preliminary analysis and tools for further exploration of the data. <br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1XuSgxSZWStQ-ojpQ88Zs_ZJ1ZqQbtAf7)</b>",
      "authors": [
        "Behzad Haki (Universitat Pompeu Fabra)*",
        " B_a_ej Kotowski (MTG)",
        " Cheuk Lun Isaac Lee (Universitat Pompeu Fabra )",
        " Sergi Jord\u00e0 (Universitat Pompeu Fabra)"
      ],
      "authors_and_affil": [
        "Behzad Haki (Universitat Pompeu Fabra)*",
        " B_a_ej Kotowski (MTG)",
        " Cheuk Lun Isaac Lee (Universitat Pompeu Fabra )",
        " Sergi Jord\u00e0 (Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTCLBE2",
      "day": "2",
      "keywords": [
        " Musical features and properties -> rhythm, beat, tempo",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Musical features and properties -> representations of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000012.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1riEMHjWzUeSl6UDSchFFDwMUa5vLn55O/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-12-haki",
      "title": "TapTamDrum: A Dataset for Dualized Drum Patterns",
      "video": "https://drive.google.com/uc?export=view&id=1XuSgxSZWStQ-ojpQ88Zs_ZJ1ZqQbtAf7"
    },
    "forum": "33",
    "id": "33",
    "pic_id": "https://drive.google.com/file/d/1HrcaIZ5sVn1gMcPy2ZLP1QRF0wD1a9A2/view",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distributions corresponding to different taxonomic classes. Results indicate that the networks are strong classifiers especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs as evidenced by increased KL-Divergence across distributions. We argue that the VAE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine. Further design challenges around generalisation to different datasets have been identified.",
      "abstract": "Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distributions corresponding to different taxonomic classes. Results indicate that the networks are strong classifiers especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs as evidenced by increased KL-Divergence across distributions. We argue that the VAE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine. Further design challenges around generalisation to different datasets have been identified.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1nr-HTFvdiIC2X5TbKLGL3ri8vZr_npdO)</b>",
      "authors": [
        "Andrea Martelloni (Queen Mary University of London)*",
        " Andrew McPherson (QMUL)",
        " Mathieu Barthet (Queen Mary University of London)"
      ],
      "authors_and_affil": [
        "Andrea Martelloni (Queen Mary University of London)*",
        " Andrew McPherson (QMUL)",
        " Mathieu Barthet (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410NQRK6",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production",
        "Human-centered MIR -> human-computer interaction",
        " Human-centered MIR -> music interfaces and services",
        " MIR tasks -> automatic classification",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000013.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Ergs4Ua6t4qmQvxxNk3auyatJkzI4HEn/view?usp=sharing ",
      "session": [
        "1"
      ],
      "slack_channel": "p1-13-martelloni",
      "title": "Real-Time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar",
      "video": "https://drive.google.com/uc?export=view&id=1nr-HTFvdiIC2X5TbKLGL3ri8vZr_npdO"
    },
    "forum": "48",
    "id": "48",
    "pic_id": "https://drive.google.com/file/d/1tvccEB0fozHYq3Mk_sf9jfX99sNeAH3o/view?usp=sharing ",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.",
      "abstract": "Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1cHkxzOuNBuD0Oc2L2lCZ_wy-gvfdp9hH)</b>",
      "authors": [
        "Hiromu Yakura (University of Tsukuba)*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "authors_and_affil": [
        "Hiromu Yakura (University of Tsukuba)*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B66AMJM",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production",
        "Human-centered MIR -> human-computer interaction",
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Human-centered MIR -> music interfaces and services"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000014.pdf",
      "poster_pdf": "https://drive.google.com/file/d/14DnHS_JotLJtFvUB75FB358BSX02a6kf/view",
      "session": [
        "1"
      ],
      "slack_channel": "p1-14-yakura",
      "title": "IteraTTA: An Interface for Exploring Both Text Prompts and Audio Priors in Generating Music With Text-to-Audio Models",
      "video": "https://drive.google.com/uc?export=view&id=1cHkxzOuNBuD0Oc2L2lCZ_wy-gvfdp9hH"
    },
    "forum": "80",
    "id": "80",
    "pic_id": "https://drive.google.com/file/d/1Obnp6RQare5i7zy4K-2BjMz-bnKaAnl3/view?usp=sharing ",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The directivity of a musical instrument is a function that describes the spatial characteristics of its sound radiation. The majority of the available literature focuses on measuring directivity patterns, with analysis mainly limited to visual inspections. Recently, some similarity metrics for directivity patterns have been introduced, yet their application has not being fully addressed. In this work, we introduce the problem of musical instrument retrieval based on the directivity pattern features.\nWe aim to exploit the available similarity metrics for directivity patterns in order to determine distances between instruments. We apply the methodology to a data set of violin directivities, including historical and modern high-quality instruments. Results show that the methodology facilitates the comparison of musical instruments and the navigation of databases of directivity patterns.",
      "abstract": "The directivity of a musical instrument is a function that describes the spatial characteristics of its sound radiation. The majority of the available literature focuses on measuring directivity patterns, with analysis mainly limited to visual inspections. Recently, some similarity metrics for directivity patterns have been introduced, yet their application has not being fully addressed. In this work, we introduce the problem of musical instrument retrieval based on the directivity pattern features.\nWe aim to exploit the available similarity metrics for directivity patterns in order to determine distances between instruments. We apply the methodology to a data set of violin directivities, including historical and modern high-quality instruments. Results show that the methodology facilitates the comparison of musical instruments and the navigation of databases of directivity patterns.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1DzQBvv4ftbSQDl1R0T7jL6U53ghxGgax)</b>",
      "authors": [
        "Mirco Pezzoli (Politecnicno di Milano)*",
        " Raffaele Malvermi (Politecnico di Milano)",
        " Fabio Antonacci (Politecnico di Milano)",
        " Augusto Sarti (Politecnico di Milano)"
      ],
      "authors_and_affil": [
        "Mirco Pezzoli (Politecnicno di Milano)*",
        " Raffaele Malvermi (Politecnico di Milano)",
        " Fabio Antonacci (Politecnico di Milano)",
        " Augusto Sarti (Politecnico di Milano)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YGUL2E6",
      "day": "2",
      "keywords": [
        "MIR and machine learning for musical acoustics",
        "MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis",
        " MIR tasks -> pattern matching and detection",
        " MIR tasks -> similarity metrics"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000015.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1ky9p0XdOSPFVcj4bJBtp-KfoHcYF7g01/view?usp=share_link",
      "session": [
        "1"
      ],
      "slack_channel": "p1-15-pezzoli",
      "title": "Similarity Evaluation of Violin Directivity Patterns for Musical Instrument Retrieval",
      "video": "https://drive.google.com/uc?export=view&id=1DzQBvv4ftbSQDl1R0T7jL6U53ghxGgax"
    },
    "forum": "174",
    "id": "174",
    "pic_id": "https://drive.google.com/file/d/1H_OImxj9fVzhHXVarGyY8QDqvp-HbeU_/view",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Computational models and analyses of musical rhythms are predominantly based on the subdivision of durations down to a common isochronous pulse, which plays a fundamental structural role in the organization of their durational patterns. Meter, the most widespread example of such a temporal scheme, consists of several hierarchically organized pulses. Deviations from isochrony found in musical patterns are considered to form an expressive, micro level of organization that is distinct from the structural macro-organization of the basic pulse. However, polyrhythmic structures, such as those found in music from West Africa or the African diaspora, challenge both the hierarchical subdivision of durations and the structural isochrony of the above models. Here we present a model that integrates the macro- and micro-organization of rhythms by generating non-isochronous girds from isochronous pulses within a polyrhythmic structure. Observed micro-timing patterns may then be generated from structural non-isochronous grids, rather than being understood as expressive deviations from isochrony. We examine the basic mathematical properties of the model and show that meter can be generated as a special case. Finally, we demonstrate the model in the analysis of micro-timing patterns observed in Brazilian samba performances.",
      "abstract": "Computational models and analyses of musical rhythms are predominantly based on the subdivision of durations down to a common isochronous pulse, which plays a fundamental structural role in the organization of their durational patterns. Meter, the most widespread example of such a temporal scheme, consists of several hierarchically organized pulses. Deviations from isochrony found in musical patterns are considered to form an expressive, micro level of organization that is distinct from the structural macro-organization of the basic pulse. However, polyrhythmic structures, such as those found in music from West Africa or the African diaspora, challenge both the hierarchical subdivision of durations and the structural isochrony of the above models. Here we present a model that integrates the macro- and micro-organization of rhythms by generating non-isochronous girds from isochronous pulses within a polyrhythmic structure. Observed micro-timing patterns may then be generated from structural non-isochronous grids, rather than being understood as expressive deviations from isochrony. We examine the basic mathematical properties of the model and show that meter can be generated as a special case. Finally, we demonstrate the model in the analysis of micro-timing patterns observed in Brazilian samba performances.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1MgX0h4s5c4Y36fk3tpKGAlynvXtU4A7H)</b>",
      "authors": [
        "George Sioros (University of Plymouth)*"
      ],
      "authors_and_affil": [
        "George Sioros (University of Plymouth)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B66GBKK",
      "day": "2",
      "keywords": [
        " MIR tasks -> music generation",
        " Computational musicology -> mathematical music theory",
        "Computational musicology",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Musical features and properties -> rhythm, beat, tempo",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000016.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1oZ6G_VvKnAb07HLlXwrGxypoGkiT9_7_/view?usp=sharing ",
      "session": [
        "1"
      ],
      "slack_channel": "p1-16-sioros",
      "title": "Polyrhythmic Modelling of Non-Isochronous and Microtiming Patterns",
      "video": "https://drive.google.com/uc?export=view&id=1MgX0h4s5c4Y36fk3tpKGAlynvXtU4A7H"
    },
    "forum": "82",
    "id": "82",
    "pic_id": "",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets. Our models and code are available at https://github.com/microsoft/muzic/tree/main/clamp.",
      "abstract": "We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets. Our models and code are available at https://github.com/microsoft/muzic/tree/main/clamp.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1wxx0BrnXEJSS5fFApirGRz1ZjZ4jFfNG)</b>",
      "authors": [
        "Shangda Wu (Central Conservatory of Music)",
        " Dingyao Yu (Peking University)",
        " Xu Tan (Microsoft Research Asia)",
        " Maosong Sun (Tsinghua University)*"
      ],
      "authors_and_affil": [
        "Shangda Wu (Central Conservatory of Music)",
        " Dingyao Yu (Peking University)",
        " Xu Tan (Microsoft Research Asia)",
        " Maosong Sun (Tsinghua University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410P6R28",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> multimodality",
        " MIR tasks -> indexing and querying",
        " MIR fundamentals and methodology -> symbolic music processing",
        " MIR tasks -> automatic classification",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Applications -> music retrieval systems"
      ],
      "long_presentation": "True",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000017.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1splCZQofFRIUpAVjijY7oNyIwKrzk8Ao/view?usp=sharing",
      "session": [
        "2"
      ],
      "slack_channel": "p2-01-sun",
      "title": "CLaMP: Contrastive Language-Music Pre-Training for Cross-Modal Symbolic Music Information Retrieval",
      "video": "https://drive.google.com/uc?export=view&id=1wxx0BrnXEJSS5fFApirGRz1ZjZ4jFfNG"
    },
    "forum": "91",
    "id": "91",
    "pic_id": "https://drive.google.com/file/d/1EK8OjR8ufm8Kte0qn8N2mgZShn-3ln7X/view?usp=sharing",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters.",
      "abstract": "Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=14XBWlSHxm8Z6cDKguzHtyqobvyS5xmTc)</b>",
      "authors": [
        "Luca Marinelli (Queen Mary University of London)*",
        " George Fazekas (QMUL)",
        " Charalampos Saitis (Queen Mary University of London)"
      ],
      "authors_and_affil": [
        "Luca Marinelli (Queen Mary University of London)*",
        " George Fazekas (QMUL)",
        " Charalampos Saitis (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTD8D5L",
      "day": "2",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " Musical features and properties -> musical affect, emotion and mood",
        "Applications -> business and marketing",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000018.pdf",
      "poster_pdf": "https://drive.google.com/file/d/14oC82QCZKHcnJYoG7X0PicHaE_iNhrSv/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-02-marinelli",
      "title": "Gender-Coded Sound: Analysing the Gendering of Music in Toy Commercials via Multi-Task Learning",
      "video": "https://drive.google.com/uc?export=view&id=14XBWlSHxm8Z6cDKguzHtyqobvyS5xmTc"
    },
    "forum": "238",
    "id": "238",
    "pic_id": "https://drive.google.com/file/d/1t0t9U2ApBUsYYGC_vorbh7E5bpWql_jk/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/94855972182)</b>",
      "abstract": "Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/94855972182)</b><br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1sx0VLxNPVR2yhVcF7oafzlE4dwxFYw11)</b>",
      "authors": [
        "Li-Yang Tseng (National Yang Ming Chiao Tung University)",
        " Tzu-Ling Lin (National Yang Ming Chiao Tung University)",
        " Hong-Han Shuai (National Yang Ming Chiao Tung University)*",
        " JEN-WEI HUANG (NYCU)",
        " Wen-Whei Chang (National Yang Ming Chiao Tung University)"
      ],
      "authors_and_affil": [
        "Li-Yang Tseng (National Yang Ming Chiao Tung University)",
        " Tzu-Ling Lin (National Yang Ming Chiao Tung University)",
        " Hong-Han Shuai (National Yang Ming Chiao Tung University)*",
        " JEN-WEI HUANG (NYCU)",
        " Wen-Whei Chang (National Yang Ming Chiao Tung University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTDB6PQ",
      "day": "2",
      "keywords": [
        "MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "False",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000019.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1V6Q7jnAKSukOFmkS81s_eamTEKHPDIEK/view?usp=sharing ",
      "session": [
        "2"
      ],
      "slack_channel": "p2-03-shuai",
      "title": "A Dataset and Baselines for Measuring and Predicting the Music Piece Memorability",
      "video": "https://drive.google.com/uc?export=view&id=1sx0VLxNPVR2yhVcF7oafzlE4dwxFYw11"
    },
    "forum": "56",
    "id": "56",
    "pic_id": "https://drive.google.com/file/d/1zbxwJflt2qQSiUWTzgr1kb08DcU36rRK/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Optical Music Recognition (OMR) is the field of research that studies how to computationally read music notation from written documents. Thanks to recent advances in computer vision and deep learning, there are successful approaches that can locate the music-notation elements from a given music score image. Once detected, these elements must be related to each other to reconstruct the musical notation itself, in the so-called notation assembly stage. However, despite its relevance in the eventual success of the OMR, this stage has been barely addressed in the literature. This work presents a set of neural approaches to perform this assembly stage. Taking into account the number of possible syntactic relationships in a music score, we give special importance to the efficiency of the process in order to obtain useful models in practice. Our experiments, using the MUSCIMA++ handwritten sheet music dataset, show that the considered approaches are capable of outperforming the existing state of the art in terms of efficiency with limited (or no) performance degradation. We believe that the conclusions of this work provide novel insights into the notation assembly step, while indicating clues on how to approach the previous stages of the OMR and improve the overall performance.",
      "abstract": "Optical Music Recognition (OMR) is the field of research that studies how to computationally read music notation from written documents. Thanks to recent advances in computer vision and deep learning, there are successful approaches that can locate the music-notation elements from a given music score image. Once detected, these elements must be related to each other to reconstruct the musical notation itself, in the so-called notation assembly stage. However, despite its relevance in the eventual success of the OMR, this stage has been barely addressed in the literature. This work presents a set of neural approaches to perform this assembly stage. Taking into account the number of possible syntactic relationships in a music score, we give special importance to the efficiency of the process in order to obtain useful models in practice. Our experiments, using the MUSCIMA++ handwritten sheet music dataset, show that the considered approaches are capable of outperforming the existing state of the art in terms of efficiency with limited (or no) performance degradation. We believe that the conclusions of this work provide novel insights into the notation assembly step, while indicating clues on how to approach the previous stages of the OMR and improve the overall performance.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=11CfMM9NabhgZnbd6yK3IOaivpeEA_5mE)</b>",
      "authors": [
        "Carlos Penarrubia (University of Alicante)",
        " Carlos Garrido-Munoz (University of Alicante)",
        " Jose J. Valero-Mas (Universitat Pompeu Fabra)",
        " Jorge Calvo-Zaragoza (University of Alicante)*"
      ],
      "authors_and_affil": [
        "Carlos Penarrubia (University of Alicante)",
        " Carlos Garrido-Munoz (University of Alicante)",
        " Jose J. Valero-Mas (Universitat Pompeu Fabra)",
        " Jorge Calvo-Zaragoza (University of Alicante)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGKUE1E",
      "day": "2",
      "keywords": [
        "",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000020.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1iqZnqyOnUMX_j5Dr0peH4QVEYHhnODbd/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-04-calvo-zaragoza",
      "title": "Efficient Notation Assembly in Optical Music Recognition",
      "video": "https://drive.google.com/uc?export=view&id=11CfMM9NabhgZnbd6yK3IOaivpeEA_5mE"
    },
    "forum": "38",
    "id": "38",
    "pic_id": "https://drive.google.com/file/d/1sxHVg-cit6-6hmsMKt8gi07jQPb01nXH/view",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Synthesizer parameter inference searches for a set of patch connections and parameters to generate audio that best matches a given target sound. Such optimization tasks benefit from access to accurate gradients. However, typical audio synths incorporate components with discontinuities \u2013 such as sawtooth or square waveforms, or a categorical search over discrete parameters like a choice among such waveforms \u2013 that thwart conventional automatic differentiation (AD). AD libraries in frameworks like TensorFlow and PyTorch typically ignore discontinuities, providing incorrect gradients at such locations. Thus, SOTA parameter inference methods avoid differentiating the synth directly, and resort to workarounds such as genetic search or neural proxies. Instead, we adapt and extend recent computer graphics methods for differentiable rendering to directly differentiate the synth as a white box program, and thereby optimize its parameters using gradient descent. We evaluate our framework using a generic FM synth with ADSR, noise, and IIR filters, adapting its parameters to match a variety of target audio clips. Our method outperforms baselines in both quantitative and qualitative evaluations.",
      "abstract": "Synthesizer parameter inference searches for a set of patch connections and parameters to generate audio that best matches a given target sound. Such optimization tasks benefit from access to accurate gradients. However, typical audio synths incorporate components with discontinuities \u2013 such as sawtooth or square waveforms, or a categorical search over discrete parameters like a choice among such waveforms \u2013 that thwart conventional automatic differentiation (AD). AD libraries in frameworks like TensorFlow and PyTorch typically ignore discontinuities, providing incorrect gradients at such locations. Thus, SOTA parameter inference methods avoid differentiating the synth directly, and resort to workarounds such as genetic search or neural proxies. Instead, we adapt and extend recent computer graphics methods for differentiable rendering to directly differentiate the synth as a white box program, and thereby optimize its parameters using gradient descent. We evaluate our framework using a generic FM synth with ADSR, noise, and IIR filters, adapting its parameters to match a variety of target audio clips. Our method outperforms baselines in both quantitative and qualitative evaluations.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1037kUfS31td5mzFzQ8M_gvlnrQpxUHLH)</b>",
      "authors": [
        "Yuting Yang (Princeton University)*",
        " Zeyu Jin (Adobe Research)",
        " Adam Finkelstein (Princeton University)",
        " Connelly Barnes (Adobe Research)"
      ],
      "authors_and_affil": [
        "Yuting Yang (Princeton University)*",
        " Zeyu Jin (Adobe Research)",
        " Adam Finkelstein (Princeton University)",
        " Connelly Barnes (Adobe Research)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YGV7KRQ",
      "day": "2",
      "keywords": [
        "Applications -> music composition, performance, and production",
        "MIR and machine learning for musical acoustics",
        " MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis",
        " MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000021.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1fao-b0_BcWYeBoCyAM5SPrdromGaVKlV/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-05-yang",
      "title": "White Box Search Over Audio Synthesizer Parameters",
      "video": "https://drive.google.com/uc?export=view&id=1037kUfS31td5mzFzQ8M_gvlnrQpxUHLH"
    },
    "forum": "59",
    "id": "59",
    "pic_id": "https://drive.google.com/file/d/1ZR2fsJ7Mh2YO0Q-mu-rHHV6VexG4inz8/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Brain decoding allows the read-out of stimulus and mental content from neural activity, and has been utilised in various neural-driven classification tasks related to the music information retrieval community. However, even the relatively simple task of instrument classification has only been demonstrated for single- or few-note stimuli when decoding from neural data recorded using functional magnetic resonance imaging (fMRI). Here, we show that drums, instrumentals, vocals, and mixed sources of naturalistic musical stimuli can be decoded from single-trial spatial patterns of auditory cortex activation as recorded using fMRI. Comparing classification based on convolutional neural networks (CNN), random forests (RF), and support vector machines (SVM) further revealed similar neural encoding of vocals and mixed sources, despite vocals being most easily identifiable. These results highlight the prominence of vocal information during music perception, and illustrate the potential of using neural representations towards evaluating music source separation performance and informing future algorithm design.",
      "abstract": "Brain decoding allows the read-out of stimulus and mental content from neural activity, and has been utilised in various neural-driven classification tasks related to the music information retrieval community. However, even the relatively simple task of instrument classification has only been demonstrated for single- or few-note stimuli when decoding from neural data recorded using functional magnetic resonance imaging (fMRI). Here, we show that drums, instrumentals, vocals, and mixed sources of naturalistic musical stimuli can be decoded from single-trial spatial patterns of auditory cortex activation as recorded using fMRI. Comparing classification based on convolutional neural networks (CNN), random forests (RF), and support vector machines (SVM) further revealed similar neural encoding of vocals and mixed sources, despite vocals being most easily identifiable. These results highlight the prominence of vocal information during music perception, and illustrate the potential of using neural representations towards evaluating music source separation performance and informing future algorithm design.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1xhJ5KjTYnpdTQB3oFl5CYxaLjUd1DVl3)</b>",
      "authors": [
        "Vincent K.M. Cheung (Sony Computer Science Laboratories, Inc.)*",
        " Lana Okuma (RIKEN)",
        " Kazuhisa Shibata (RIKEN)",
        " Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industr"
      ],
      "authors_and_affil": [
        "Vincent K.M. Cheung (Sony Computer Science Laboratories, Inc.)*",
        " Lana Okuma (RIKEN)",
        " Kazuhisa Shibata (RIKEN)",
        " Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industr"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTDKFFY",
      "day": "2",
      "keywords": [
        "Human-centered MIR -> human-computer interaction",
        "Human-centered MIR -> user-centered evaluation",
        " Human-centered MIR -> user behavior analysis and mining, user modeling",
        " Knowledge-driven approaches to MIR -> cognitive MIR"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000022.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1UndCVJ8d5hWou47TfEBwovykn9yq7sQc/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-06-cheung",
      "title": "Decoding Drums, Instrumentals, Vocals, and Mixed Sources in Music Using Human Brain Activity With fMRI",
      "video": "https://drive.google.com/uc?export=view&id=1xhJ5KjTYnpdTQB3oFl5CYxaLjUd1DVl3"
    },
    "forum": "63",
    "id": "63",
    "pic_id": "https://drive.google.com/file/d/1YV45v4F9uMKuPcNsJDJ3FiK2c6eJS2et/view",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Music Emotion Recognition (MER) refers to automatically extracting emotional information from music and predicting its perceived emotions, and it has social and psychological applications. This paper proposes a Dual Attention-based Multi-scale Feature Fusion (DAMFF) method and a newly developed dataset named MER1101 for Dynamic Music Emotion Recognition (DMER). Specifically, multi-scale features are first extracted from the log Mel-spectrogram by multiple parallel convolutional blocks. Then, a Dual Attention Feature Fusion (DAFF) module is utilized to achieve multi-scale context fusion and capture emotion-critical features in both spatial and channel dimensions. Finally, a BiLSTM-based sequence learning model is employed for dynamic music emotion prediction. To enrich existing music emotion datasets, we developed a high-quality dataset, MER1101, which has a balanced emotional distribution, covering over 10 genres, at least four languages, and more than a thousand song snippets. We demonstrate the effectiveness of our proposed DAMFF approach on both the developed MER1101 dataset, as well as on the established DEAM2015 dataset. Compared with other models, our model achieves a higher Consistency Correlation Coefficient (CCC), and has strong predictive power in arousal with comparable results in valence.",
      "abstract": "Music Emotion Recognition (MER) refers to automatically extracting emotional information from music and predicting its perceived emotions, and it has social and psychological applications. This paper proposes a Dual Attention-based Multi-scale Feature Fusion (DAMFF) method and a newly developed dataset named MER1101 for Dynamic Music Emotion Recognition (DMER). Specifically, multi-scale features are first extracted from the log Mel-spectrogram by multiple parallel convolutional blocks. Then, a Dual Attention Feature Fusion (DAFF) module is utilized to achieve multi-scale context fusion and capture emotion-critical features in both spatial and channel dimensions. Finally, a BiLSTM-based sequence learning model is employed for dynamic music emotion prediction. To enrich existing music emotion datasets, we developed a high-quality dataset, MER1101, which has a balanced emotional distribution, covering over 10 genres, at least four languages, and more than a thousand song snippets. We demonstrate the effectiveness of our proposed DAMFF approach on both the developed MER1101 dataset, as well as on the established DEAM2015 dataset. Compared with other models, our model achieves a higher Consistency Correlation Coefficient (CCC), and has strong predictive power in arousal with comparable results in valence.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1mC18LEQOFkb3GyHxXFDdUf5puYgjv_f8)</b>",
      "authors": [
        "Liyue Zhang ( Xi\u2019an Jiaotong University)*",
        " Xinyu Yang (Xi'an Jiaotong University)",
        " Yichi Zhang (Xi'an Jiaotong University)",
        " Jing Luo (Xi'an Jiaotong University)"
      ],
      "authors_and_affil": [
        "Liyue Zhang ( Xi\u2019an Jiaotong University)*",
        " Xinyu Yang (Xi'an Jiaotong University)",
        " Yichi Zhang (Xi'an Jiaotong University)",
        " Jing Luo (Xi'an Jiaotong University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGL7E7J",
      "day": "2",
      "keywords": [
        "MIR tasks -> automatic classification",
        "MIR fundamentals and methodology -> music signal processing",
        " Musical features and properties -> musical affect, emotion and mood"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000023.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1HgMEA6LvG9R0TJFZAuBAFK7uCemTWkJf/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-07-zhang",
      "title": "Dual Attention-Based Multi-Scale Feature Fusion Approach for Dynamic Music Emotion Recognition",
      "video": "https://drive.google.com/uc?export=view&id=1mC18LEQOFkb3GyHxXFDdUf5puYgjv_f8"
    },
    "forum": "68",
    "id": "68",
    "pic_id": "https://drive.google.com/file/d/1xPhQIFxNHwkfqy90YJFTvgrIUIlAQ_wh/view",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription.\nThis is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content.\nIn this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes.\nIn this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture.\nThe first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis.\nThe output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis.\nWe evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.",
      "abstract": "Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription.\nThis is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content.\nIn this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes.\nIn this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture.\nThe first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis.\nThe output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis.\nWe evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the F1-scores of the metrics among Frame, Note, Note with Offset, and Note with Offset and Velocity estimations.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Flog5weyWDCqIryWhj9XDpDo2kmLajwq)</b>",
      "authors": [
        "Keisuke Toyama (Sony Group Corporation)*",
        " Taketo Akama (Sony CSL)",
        " Yukara Ikemiya (Sony Research)",
        " Yuhta Takida (Sony Group Corporation)",
        " WeiHsiang Liao (Sony Group Corporation)",
        " Yuki Mitsufuji (Sony Group Corporation)"
      ],
      "authors_and_affil": [
        "Keisuke Toyama (Sony Group Corporation)*",
        " Taketo Akama (Sony CSL)",
        " Yukara Ikemiya (Sony Research)",
        " Yuhta Takida (Sony Group Corporation)",
        " WeiHsiang Liao (Sony Group Corporation)",
        " Yuki Mitsufuji (Sony Group Corporation)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTDRA5C",
      "day": "2",
      "keywords": [
        "",
        "MIR tasks -> music transcription and annotation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000024.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1VGhS9fshR57vmR2125mYzNEGbYGFmr49/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-08-toyama",
      "title": "Automatic Piano Transcription With Hierarchical Frequency-Time Transformer",
      "video": "https://drive.google.com/uc?export=view&id=1Flog5weyWDCqIryWhj9XDpDo2kmLajwq"
    },
    "forum": "72",
    "id": "72",
    "pic_id": "https://drive.google.com/file/d/1eJKauSWyuJxlw920Kba_NSnEPg6hb_wE/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "A descriptive transcription of a violin performance requires detecting not only the notes but also the fine-grained pitch variations, such as vibrato. Most existing deep learning methods for music transcription do not capture these variations and often need frame-level annotations, which are scarce for the violin. In this paper, we propose a novel method for high-resolution violin transcription that can leverage piece-level weak labels for training. Our conformer-based model works on the raw audio waveform and transcribes violin notes and their corresponding pitch deviations with 5.8 ms frame resolution and 10-cent frequency resolution. We demonstrate that our method (1) outperforms generic systems in the proxy tasks of violin transcription and pitch estimation, and (2) can automatically generate new training labels by aligning its feature representations with unseen scores. We share our model along with 34 hours of score-aligned solo violin performance dataset, notably including the 24 Paganini Caprices.",
      "abstract": "A descriptive transcription of a violin performance requires detecting not only the notes but also the fine-grained pitch variations, such as vibrato. Most existing deep learning methods for music transcription do not capture these variations and often need frame-level annotations, which are scarce for the violin. In this paper, we propose a novel method for high-resolution violin transcription that can leverage piece-level weak labels for training. Our conformer-based model works on the raw audio waveform and transcribes violin notes and their corresponding pitch deviations with 5.8 ms frame resolution and 10-cent frequency resolution. We demonstrate that our method (1) outperforms generic systems in the proxy tasks of violin transcription and pitch estimation, and (2) can automatically generate new training labels by aligning its feature representations with unseen scores. We share our model along with 34 hours of score-aligned solo violin performance dataset, notably including the 24 Paganini Caprices.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=19PEwJtC-35XcbPaJy5ukN0uBoIzi3d1f)</b>",
      "authors": [
        "Nazif Can Tamer (Universitat Pompeu Fabra)*",
        " Yigitcan \u00d6zer (International Audio Laboratories Erlangen)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "authors_and_affil": [
        "Nazif Can Tamer (Universitat Pompeu Fabra)*",
        " Yigitcan \u00d6zer (International Audio Laboratories Erlangen)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B67CE1F",
      "day": "2",
      "keywords": [
        " MIR tasks -> alignment, synchronization, and score following",
        " Musical features and properties -> representations of music",
        " MIR fundamentals and methodology -> music signal processing",
        "MIR tasks -> music transcription and annotation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000025.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1KRLWGk5HcUFNG9bYsywrSugL_a8PVh3J/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-09-tamer",
      "title": "High-Resolution Violin Transcription Using Weak Labels",
      "video": "https://drive.google.com/uc?export=view&id=19PEwJtC-35XcbPaJy5ukN0uBoIzi3d1f"
    },
    "forum": "223",
    "id": "223",
    "pic_id": "https://drive.google.com/file/d/1iBNLcgwzHEbulDBoDNAkAicD-WAITY4N/view",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.",
      "abstract": "We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1dGIexp74B_hpMGlqHfo5sfAk-9kHC3vP)</b>",
      "authors": [
        "Lejun Min (Shanghai Jiao Tong University)*",
        " Junyan Jiang (New York University Shanghai)",
        " Gus Xia (New York University Shanghai)",
        " Jingwei Zhao (National University of Singapore)"
      ],
      "authors_and_affil": [
        "Lejun Min (Shanghai Jiao Tong University)*",
        " Junyan Jiang (New York University Shanghai)",
        " Gus Xia (New York University Shanghai)",
        " Jingwei Zhao (National University of Singapore)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YGVMYCS",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        " Knowledge-driven approaches to MIR -> representations of music",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000026.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1LtBs1P-Uf63J208pU7ziT8o5PpmkHfz_/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-10-min",
      "title": "Polyffusion: A Diffusion Model for Polyphonic Score Generation With Internal and External Controls",
      "video": "https://drive.google.com/uc?export=view&id=1dGIexp74B_hpMGlqHfo5sfAk-9kHC3vP"
    },
    "forum": "51",
    "id": "51",
    "pic_id": "https://drive.google.com/file/d/1LpF3DpX_weJWjoczb27TLVUtPHTzrgbL/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This paper introduces a new corpus, CoCoPops: The Coordinated Corpus of Popular Musics. The corpus can be considered a \u201cmeta corpus\u201d in that it both extends and combines two existing corpora\u2014the widely-used McGill Bill-\nboard corpus the and RS200 corpus. Both the McGill Billboard corpus and the RS200 contain expert harmonic annotations using different encoding schemes and each\nrepresent harmony in fundamentally different ways: Billboard using a root-quality representation and the RS200 using Roman numerals. By combining these corpora\ninto a unified format, using the well-known **kern and**harm representations, we aim to facilitate research in computational musicology, which is frequently burdened\nby corpora spread across multiple encoding formats. The format will also facilitate cross-corpus comparison with the large body of existing works in **kern format. For a\n100-song subset of the CoCoPops-Billboard collection, we also provide participant ratings of continuous valence and arousal ratings, along with the RMS (Root Mean Square) signal level and associated timestamps. In this paper we describe the corpus and the procedures used to create it.",
      "abstract": "This paper introduces a new corpus, CoCoPops: The Coordinated Corpus of Popular Musics. The corpus can be considered a \u201cmeta corpus\u201d in that it both extends and combines two existing corpora\u2014the widely-used McGill Bill-\nboard corpus the and RS200 corpus. Both the McGill Billboard corpus and the RS200 contain expert harmonic annotations using different encoding schemes and each\nrepresent harmony in fundamentally different ways: Billboard using a root-quality representation and the RS200 using Roman numerals. By combining these corpora\ninto a unified format, using the well-known **kern and**harm representations, we aim to facilitate research in computational musicology, which is frequently burdened\nby corpora spread across multiple encoding formats. The format will also facilitate cross-corpus comparison with the large body of existing works in **kern format. For a\n100-song subset of the CoCoPops-Billboard collection, we also provide participant ratings of continuous valence and arousal ratings, along with the RMS (Root Mean Square) signal level and associated timestamps. In this paper we describe the corpus and the procedures used to create it.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=12W7e61WVW3bqyfuKWISRz9M2D1UQ_xNb)</b>",
      "authors": [
        "Claire Arthur (Georgia Institute of Technology)*",
        " Nathaniel Condit-Schultz (Georgia Institute of Technology)"
      ],
      "authors_and_affil": [
        "Claire Arthur (Georgia Institute of Technology)*",
        " Nathaniel Condit-Schultz (Georgia Institute of Technology)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B67JV2M",
      "day": "2",
      "keywords": [
        " Musical features and properties -> melody and motives",
        "Computational musicology -> digital musicology",
        " Knowledge-driven approaches to MIR -> cognitive MIR",
        " Musical features and properties -> harmony, chords and tonality",
        " Computational musicology -> systematic musicology",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000027.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Z0vMqYVaW7Hx_nSVl9ZtmYNbiUfJmKd2/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-11-arthur",
      "title": "The Coordinated Corpus of Popular Musics (CoCoPops): A Meta-Dataset of Melodic and Harmonic Transcriptions",
      "video": "https://drive.google.com/uc?export=view&id=12W7e61WVW3bqyfuKWISRz9M2D1UQ_xNb"
    },
    "forum": "104",
    "id": "104",
    "pic_id": "https://drive.google.com/file/d/1BpgecJ0c7FvybDh1WR11NSPBqYqgOoHT/view",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The research field of music therapy has witnessed a rising interest in recent years to develop and employ computational methods to support therapists in their daily practice. While Music Information Retrieval (MIR) research has identified the area of health and well-being as a promising application field for MIR methods to support health professionals, collaborations with experts in this field are as of today sparse. This paper provides an overview of potential applications of computational music analysis as developed in MIR for the field of active music therapy. We elaborate on the music therapy method of improvisation, with a particular focus on introducing therapeutic concepts that relate to musical structures. We identify application scenarios for analysing musical structures in improvisations, introduce existing analysis methods of therapists, and discuss the potential of MIR methods to support these analyses. Upon identifying a current gap between high-level concepts of therapists and low-level features from existing computational methods, the paper concludes further steps towards developing computational approaches to music analysis for music therapy in an interdisciplinary collaboration.",
      "abstract": "The research field of music therapy has witnessed a rising interest in recent years to develop and employ computational methods to support therapists in their daily practice. While Music Information Retrieval (MIR) research has identified the area of health and well-being as a promising application field for MIR methods to support health professionals, collaborations with experts in this field are as of today sparse. This paper provides an overview of potential applications of computational music analysis as developed in MIR for the field of active music therapy. We elaborate on the music therapy method of improvisation, with a particular focus on introducing therapeutic concepts that relate to musical structures. We identify application scenarios for analysing musical structures in improvisations, introduce existing analysis methods of therapists, and discuss the potential of MIR methods to support these analyses. Upon identifying a current gap between high-level concepts of therapists and low-level features from existing computational methods, the paper concludes further steps towards developing computational approaches to music analysis for music therapy in an interdisciplinary collaboration.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Y2c00qrsceqIemNOanPAnBklWPEowTds)</b>",
      "authors": [
        "Anja Volk (Utrecht University)*",
        " Tinka Veldhuis (Utrecht University)",
        " Katrien Foubert (LUCA School of Arts)",
        " Jos De Backer (LUCA School of Arts)"
      ],
      "authors_and_affil": [
        "Anja Volk (Utrecht University)*",
        " Tinka Veldhuis (Utrecht University)",
        " Katrien Foubert (LUCA School of Arts)",
        " Jos De Backer (LUCA School of Arts)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B67NGNM",
      "day": "2",
      "keywords": [
        "Applications",
        "Applications -> music and health, well-being and therapy"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000028.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1PglOD8-VretOEUQOKD9uBkEvbW_NDwOg/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-12-volk",
      "title": "Towards Computational Music Analysis for Music Therapy",
      "video": "https://drive.google.com/uc?export=view&id=1Y2c00qrsceqIemNOanPAnBklWPEowTds"
    },
    "forum": "153",
    "id": "153",
    "pic_id": "https://drive.google.com/file/d/1oRXNzZemPVUNgX7VN0k6H4bnZY3aiJet/view?usp=sharing",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such as melody and dynamics. Following their recent breakthroughs in deep learning-based generation, we apply Denoising Diffusion Models (DDMs) to perform timbre transfer. Specifically, we apply the recently proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate the sampling procedure. \nInspired by the recent application of DDMs to image translation problems we formulate the timbre transfer task similarly, by first converting the audio tracks into log mel spectrograms and by conditioning the generation of the desired timbre spectrogram through the input timbre spectrogram.  \nWe perform both one-to-one and many-to-many timbre transfer, by converting audio waveforms containing only single instruments and multiple instruments, respectively.\nWe compare the proposed technique with existing state-of-the-art methods both through listening tests and objective measures in order to demonstrate the effectiveness of the proposed model.",
      "abstract": "Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such as melody and dynamics. Following their recent breakthroughs in deep learning-based generation, we apply Denoising Diffusion Models (DDMs) to perform timbre transfer. Specifically, we apply the recently proposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate the sampling procedure. \nInspired by the recent application of DDMs to image translation problems we formulate the timbre transfer task similarly, by first converting the audio tracks into log mel spectrograms and by conditioning the generation of the desired timbre spectrogram through the input timbre spectrogram.  \nWe perform both one-to-one and many-to-many timbre transfer, by converting audio waveforms containing only single instruments and multiple instruments, respectively.\nWe compare the proposed technique with existing state-of-the-art methods both through listening tests and objective measures in order to demonstrate the effectiveness of the proposed model.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1qU345rZrSCXeeK_k3aylFUWrvOTuQFSW)</b>",
      "authors": [
        "Luca Comanducci (Politecnico di Milano)*",
        " Fabio Antonacci (Politecnico di Milano)",
        " Augusto Sarti (Politecnico di Milano)"
      ],
      "authors_and_affil": [
        "Luca Comanducci (Politecnico di Milano)*",
        " Fabio Antonacci (Politecnico di Milano)",
        " Augusto Sarti (Politecnico di Milano)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410QDPUL",
      "day": "2",
      "keywords": [
        "Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR tasks -> music synthesis and transformation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000029.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1h2BzyHiG6_5k1XCckQO4CnkWleWcmaCI/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-13-comanducci",
      "title": "Timbre Transfer Using Image-to-Image Denoising Diffusion Implicit Models",
      "video": "https://drive.google.com/uc?export=view&id=1qU345rZrSCXeeK_k3aylFUWrvOTuQFSW"
    },
    "forum": "197",
    "id": "197",
    "pic_id": "https://drive.google.com/file/d/10Y2cPu2ro6LmAIkvgdXX44MRmrSpSCRP/view",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Music structure analysis is a core topic in Music Information Retrieval and could be advanced through the inclusion of new data modalities. In this study we consider neural correlates of music structure processing using popular music - specifically choruses of Bollywood songs - and the {NMED-H} electroencephalographic (EEG) dataset. Motivated by recent findings that listeners' EEG responses correlate when hearing a shared music stimulus, we investigate whether responses correlate not only within single choruses but across pairs of chorus instances as well. We find statistically significant correlations within and across several chorus instances, suggesting that brain responses synchronize across structurally matched music segments even if they are not contextually or acoustically identical. Correlations were only occasionally higher within than across choruses. Our findings advance the state of the art of naturalistic music neuroscience, while also highlighting a novel approach for further studies of music structure analysis and audio understanding more broadly.",
      "abstract": "Music structure analysis is a core topic in Music Information Retrieval and could be advanced through the inclusion of new data modalities. In this study we consider neural correlates of music structure processing using popular music - specifically choruses of Bollywood songs - and the {NMED-H} electroencephalographic (EEG) dataset. Motivated by recent findings that listeners' EEG responses correlate when hearing a shared music stimulus, we investigate whether responses correlate not only within single choruses but across pairs of chorus instances as well. We find statistically significant correlations within and across several chorus instances, suggesting that brain responses synchronize across structurally matched music segments even if they are not contextually or acoustically identical. Correlations were only occasionally higher within than across choruses. Our findings advance the state of the art of naturalistic music neuroscience, while also highlighting a novel approach for further studies of music structure analysis and audio understanding more broadly.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1IEs2vHDAZxE76QV10AHpIX9h39tol-M_)</b>",
      "authors": [
        "Neha Rajagopalan (Stanford University)*",
        " Blair Kaneshiro (Stanford University)"
      ],
      "authors_and_affil": [
        "Neha Rajagopalan (Stanford University)*",
        " Blair Kaneshiro (Stanford University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGM0UJU",
      "day": "2",
      "keywords": [
        "Human-centered MIR",
        "Knowledge-driven approaches to MIR -> cognitive MIR",
        " MIR fundamentals and methodology -> multimodality",
        " Musical features and properties -> structure, segmentation, and form"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000030.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1UjDa6qx3fJHgfKkmK5PSzSUfz0Q_90B9/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-14-rajagopalan",
      "title": "Correlation of EEG Responses Reflects Structural Similarity of Choruses in Popular Music",
      "video": "https://drive.google.com/uc?export=view&id=1IEs2vHDAZxE76QV10AHpIX9h39tol-M_"
    },
    "forum": "259",
    "id": "259",
    "pic_id": "https://drive.google.com/file/d/1xn8-GkvSdeBcQZve5kgEwb5eyUfkmO5t/view",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "\u201cChromatic harmony\u201d is seen as a fundamental part of (extended) tonal music in the Western classical tradition (c.1700\u20131900). It routinely features in core curricula. Yet even in this globalised and data-driven age, 1) there are significant gaps between how different national \u201cschools\u201d identify important chords and progressions, label them, and shape the corresponding curricula; 2) even many common terms lack robust definition; and 3) empirical evidence rarely features, even in in discussions about \u201ctypical\u201d, \u201crepresentative\u201d practice. This paper addresses those three considerations by: 1) comparing English- and German-speaking traditions as an example of this divergence; 2) proposing a framework for defining common terms where that is lacking; and 3) surveying the actual usage of these chromatic chord categories using a computational corpus study of human harmonic analyses.",
      "abstract": "\u201cChromatic harmony\u201d is seen as a fundamental part of (extended) tonal music in the Western classical tradition (c.1700\u20131900). It routinely features in core curricula. Yet even in this globalised and data-driven age, 1) there are significant gaps between how different national \u201cschools\u201d identify important chords and progressions, label them, and shape the corresponding curricula; 2) even many common terms lack robust definition; and 3) empirical evidence rarely features, even in in discussions about \u201ctypical\u201d, \u201crepresentative\u201d practice. This paper addresses those three considerations by: 1) comparing English- and German-speaking traditions as an example of this divergence; 2) proposing a framework for defining common terms where that is lacking; and 3) surveying the actual usage of these chromatic chord categories using a computational corpus study of human harmonic analyses.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1vrmYxGG7OMYAndo77lzDq7XWkQ3IMGZm)</b>",
      "authors": [
        "Mark R H Gotham (Durham)*"
      ],
      "authors_and_affil": [
        "Mark R H Gotham (Durham)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J16E095",
      "day": "2",
      "keywords": [
        "Computational musicology -> digital musicology",
        " Musical features and properties -> rhythm, beat, tempo",
        " Computational musicology -> mathematical music theory",
        "Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " Musical features and properties -> harmony, chords and tonality",
        " Computational musicology -> systematic musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000031.pdf",
      "poster_pdf": "https://drive.google.com/file/d/16cyvIY6D-V582ZckDmzh1A1ZJZY1UioP/view",
      "session": [
        "2"
      ],
      "slack_channel": "p2-15-gotham",
      "title": "Chromatic Chords in Theory and Practice",
      "video": "https://drive.google.com/uc?export=view&id=1vrmYxGG7OMYAndo77lzDq7XWkQ3IMGZm"
    },
    "forum": "46",
    "id": "46",
    "pic_id": "https://drive.google.com/file/d/1zsZG_DIAhhbZa4qyebWH91uv98M7Qmpf/view?usp=sharing",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Intra-opus repeated pattern discovery in polyphonic symbolic music data has  challenges in both algorithm design and data annotation. To solve these challenges, we propose BPS-motif, a new symbolic music dataset containing the note-level annotation of motives and occurrences in Beethoven's piano sonatas. The size of the proposed dataset is larger than previous symbolic datasets for repeated pattern discovery. We report the process of dataset annotation, specifically a peer review process and discussion phase to improve the annotation quality. Finally, we propose a motif discovery method which is shown outperforming baseline methods on repeated pattern discovery.",
      "abstract": "Intra-opus repeated pattern discovery in polyphonic symbolic music data has  challenges in both algorithm design and data annotation. To solve these challenges, we propose BPS-motif, a new symbolic music dataset containing the note-level annotation of motives and occurrences in Beethoven's piano sonatas. The size of the proposed dataset is larger than previous symbolic datasets for repeated pattern discovery. We report the process of dataset annotation, specifically a peer review process and discussion phase to improve the annotation quality. Finally, we propose a motif discovery method which is shown outperforming baseline methods on repeated pattern discovery.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1yVFoMUKa7i5jE94EA1b_tDCqQ-AhfRT2)</b>",
      "authors": [
        "YO-WEI HSIAO (Academia Sinica)",
        " TZU-YUN Hung (National Taiwan Normal University)",
        " Tsung-Ping Chen (Academia Sinica)",
        " Li Su (Academia Sinica)*"
      ],
      "authors_and_affil": [
        "YO-WEI HSIAO (Academia Sinica)",
        " TZU-YUN Hung (National Taiwan Normal University)",
        " Tsung-Ping Chen (Academia Sinica)",
        " Li Su (Academia Sinica)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGM78JC",
      "day": "3",
      "keywords": [
        "MIR tasks -> pattern matching and detection",
        "MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "True",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000032.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1CIPZH9MrmtumY-M9GTEaTc9lNeyX2IGq/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-01-su",
      "title": "BPS-Motif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic Music",
      "video": "https://drive.google.com/uc?export=view&id=1yVFoMUKa7i5jE94EA1b_tDCqQ-AhfRT2"
    },
    "forum": "145",
    "id": "145",
    "pic_id": "https://drive.google.com/file/d/1K2sK-hgZPRH-72jMbxbd1hk23j0FUISY/view?usp=share_link",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Multi-pitch estimation (MPE), the task of detecting active pitches within a polyphonic music recording, has garnered significant research interest in recent years. Most state-of-the-art approaches for MPE are based on deep networks trained using pitch annotations as targets. The success of current methods is therefore limited by the difficulty of obtaining large amounts of accurate annotations.\nIn this paper, we propose a novel technique for learning MPE without any pitch annotations at all. Our approach exploits multiple recorded versions of a musical piece as surrogate targets. Given one version of a piece as input, we train a network to minimize the distance between its output and time-frequency representations of other versions of that piece. \nSince all versions are based on the same musical score, we hypothesize that the learned output corresponds to pitch estimates. To further ensure that this hypothesis holds, we incorporate domain knowledge about overtones and noise levels into the network.\nOverall, our method replaces strong pitch annotations with weaker and easier-to-obtain cross-version targets.\nIn our experiments, we show that our proposed approach yields viable multi-pitch estimates and outperforms two baselines.",
      "abstract": "Multi-pitch estimation (MPE), the task of detecting active pitches within a polyphonic music recording, has garnered significant research interest in recent years. Most state-of-the-art approaches for MPE are based on deep networks trained using pitch annotations as targets. The success of current methods is therefore limited by the difficulty of obtaining large amounts of accurate annotations.\nIn this paper, we propose a novel technique for learning MPE without any pitch annotations at all. Our approach exploits multiple recorded versions of a musical piece as surrogate targets. Given one version of a piece as input, we train a network to minimize the distance between its output and time-frequency representations of other versions of that piece. \nSince all versions are based on the same musical score, we hypothesize that the learned output corresponds to pitch estimates. To further ensure that this hypothesis holds, we incorporate domain knowledge about overtones and noise levels into the network.\nOverall, our method replaces strong pitch annotations with weaker and easier-to-obtain cross-version targets.\nIn our experiments, we show that our proposed approach yields viable multi-pitch estimates and outperforms two baselines.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1ta1EFfmQaRw6K2M6CwRyOo-YkNPZ-LU0)</b>",
      "authors": [
        "Michael Krause (International Audio Laboratories Erlangen)*",
        " Sebastian Strahl (Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "authors_and_affil": [
        "Michael Krause (International Audio Laboratories Erlangen)*",
        " Sebastian Strahl (Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE227EF",
      "day": "3",
      "keywords": [
        " MIR tasks -> alignment, synchronization, and score following",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "MIR tasks -> music transcription and annotation",
        " Knowledge-driven approaches to MIR -> representations of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000033.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1lGmk9yrSduurAmTnRTTu-POFNk1QwZC2/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-02-krause",
      "title": "Weakly Supervised Multi-Pitch Estimation Using Cross-Version Alignment",
      "video": "https://drive.google.com/uc?export=view&id=1ta1EFfmQaRw6K2M6CwRyOo-YkNPZ-LU0"
    },
    "forum": "81",
    "id": "81",
    "pic_id": "https://drive.google.com/file/d/16Dox9AzY9TJDz8mORKIF7bJ5t57DFtyn/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We present the Batik plays Mozart Corpus, a piano performance dataset\ncombining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B\u00f6sendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can\nfurther be connected to the musicological annotations (harmony, cadences,\nphrases) on these scores that were recently published by [1].\n\nThe result is a high-quality, high-precision corpus mapping scores and musical\nstructure annotations to precise note-level professional performance information.\nAs the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects.\n\nIn the paper, we outline the curation process of the alignment and conduct two\nexploratory experiments to demonstrate its usefulness in analyzing expressive performance.\n\n[1] Hentschel, J., Neuwirth, M., & Rohrmeier, M. (2021). The Annotated Mozart Sonatas: Score, Harmony, and Cadence. Transactions of the International Society for Music Information Retrieval (TISMIR), Vol. 4, No. 1, pp. 67-80.",
      "abstract": "We present the Batik plays Mozart Corpus, a piano performance dataset\ncombining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik on a computer-monitored B\u00f6sendorfer grand piano, and are available both as MIDI files and audio recordings. They have been precisely aligned, note by note, with a current standard edition of the corresponding scores (the New Mozart Edition) in such a way that they can\nfurther be connected to the musicological annotations (harmony, cadences,\nphrases) on these scores that were recently published by [1].\n\nThe result is a high-quality, high-precision corpus mapping scores and musical\nstructure annotations to precise note-level professional performance information.\nAs the first of its kind, it can serve as a valuable resource for studying various facets of expressive performance and their relationship with structural aspects.\n\nIn the paper, we outline the curation process of the alignment and conduct two\nexploratory experiments to demonstrate its usefulness in analyzing expressive performance.\n\n[1] Hentschel, J., Neuwirth, M., & Rohrmeier, M. (2021). The Annotated Mozart Sonatas: Score, Harmony, and Cadence. Transactions of the International Society for Music Information Retrieval (TISMIR), Vol. 4, No. 1, pp. 67-80.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1f4QHPE7FMDQO3v3qF2beiqsaujGwSPVw)</b>",
      "authors": [
        "Patricia Hu (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "authors_and_affil": [
        "Patricia Hu (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J16Q4LX",
      "day": "3",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> reproducibility",
        "Evaluation, datasets, and reproducibility",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Evaluation, datasets, and reproducibility -> annotation protocols",
        " Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000034.pdf",
      "poster_pdf": "https://drive.google.com/file/d/13CtnTbgJlUT0Wz_DtR5x0o4EHEo3DJmk/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-03-hu",
      "title": "The Batik-Plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations",
      "video": "https://drive.google.com/uc?export=view&id=1f4QHPE7FMDQO3v3qF2beiqsaujGwSPVw"
    },
    "forum": "92",
    "id": "92",
    "pic_id": "https://drive.google.com/file/d/1ExVUQUbUIa0FRn_fsD_BJrw0C0l6lu8q/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Generating a stereophonic presentation from a monophonic audio signal is a challenging open task, especially if the goal is to obtain a realistic spatial imaging with a specific panning of sound elements. In this work, we propose to convert mono to stereo by means of predicting parametric stereo (PS) parameters using both nearest neighbor and deep network approaches. In combination with PS, we also propose to model the task with generative approaches, allowing to synthesize multiple and equally-plausible stereo renditions from the same mono signal. To achieve this, we consider both autoregressive and masked token modelling approaches. We provide evidence that the proposed PS-based models outperform a competitive classical decorrelation baseline and that, within a PS prediction framework, modern generative models outshine equivalent non-generative counterparts. Overall, our work positions both PS and generative modelling as strong and appealing methodologies for mono-to-stereo upmixing. A discussion of the limitations of these approaches is also provided.",
      "abstract": "Generating a stereophonic presentation from a monophonic audio signal is a challenging open task, especially if the goal is to obtain a realistic spatial imaging with a specific panning of sound elements. In this work, we propose to convert mono to stereo by means of predicting parametric stereo (PS) parameters using both nearest neighbor and deep network approaches. In combination with PS, we also propose to model the task with generative approaches, allowing to synthesize multiple and equally-plausible stereo renditions from the same mono signal. To achieve this, we consider both autoregressive and masked token modelling approaches. We provide evidence that the proposed PS-based models outperform a competitive classical decorrelation baseline and that, within a PS prediction framework, modern generative models outshine equivalent non-generative counterparts. Overall, our work positions both PS and generative modelling as strong and appealing methodologies for mono-to-stereo upmixing. A discussion of the limitations of these approaches is also provided.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1qQz-iD-vnFqK9jd-WOnGhFUbsiWQ_oi5)</b>",
      "authors": [
        "Joan Serra (Dolby Laboratories)*",
        " Davide Scaini (Dolby Laboratories)",
        " Santiago Pascual (Dolby Laboratories)",
        " Daniel Arteaga (Dolby Laboratories)",
        " Jordi Pons (Dolby Laboratories)",
        " Jeroen Breebaart (Dolby Laboratories)",
        " Giulio Cengarle (Dolby Laboratories)"
      ],
      "authors_and_affil": [
        "Joan Serra (Dolby Laboratories)*",
        " Davide Scaini (Dolby Laboratories)",
        " Santiago Pascual (Dolby Laboratories)",
        " Daniel Arteaga (Dolby Laboratories)",
        " Jordi Pons (Dolby Laboratories)",
        " Jeroen Breebaart (Dolby Laboratories)",
        " Giulio Cengarle (Dolby Laboratories)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J16TLFR",
      "day": "3",
      "keywords": [
        "MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics",
        "MIR tasks -> music synthesis and transformation",
        " MIR tasks -> music generation",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000035.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1jV46u1KFaKVfBtpPoG-FTR1gAckO7j-c/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-04-serra",
      "title": "Mono-to-Stereo Through Parametric Stereo Generation",
      "video": "https://drive.google.com/uc?export=view&id=1qQz-iD-vnFqK9jd-WOnGhFUbsiWQ_oi5"
    },
    "forum": "100",
    "id": "100",
    "pic_id": "https://drive.google.com/file/d/10yWhpi-LUE35EOagxWXY4NNKLbGsqB4K/view",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.",
      "abstract": "Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Q5im0b7JCRsIYxzLriy1pW-zu_vqpUOt)</b>",
      "authors": [
        "Charilaos Papaioannou (School of ECE, National Technical University of Athens)*",
        " Emmanouil Benetos (Queen Mary University of London)",
        " Alexandros Potamianos (National Technical University of Athens)"
      ],
      "authors_and_affil": [
        "Charilaos Papaioannou (School of ECE, National Technical University of Athens)*",
        " Emmanouil Benetos (Queen Mary University of London)",
        " Alexandros Potamianos (National Technical University of Athens)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK62693",
      "day": "3",
      "keywords": [
        " MIR tasks -> automatic classification",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR -> computational ethnomusicology",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000036.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1ALR54HMKN2zggi7c8UaT7U0rxIb26i-E/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-05-papaioannou",
      "title": "From West to East: Who Can Understand the Music of the Others Better?",
      "video": "https://drive.google.com/uc?export=view&id=1Q5im0b7JCRsIYxzLriy1pW-zu_vqpUOt"
    },
    "forum": "101",
    "id": "101",
    "pic_id": "https://drive.google.com/file/d/1vs-muvttYYR82INpXW2rz08gmXI4w1eu/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Optical Music Recognition (OMR) has become a popular technology to retrieve information present in musical scores in conjunction with the increasing improvement of Deep Learning techniques, which represent the state-of-the-art in the field. However, its effectiveness is limited to cases where the target collection is similar in musical context and graphical appearance to the available training examples. To address this limitation, researchers have resorted to labeling examples for specific neural models, which is time-consuming and raises questions about usability. In this study, we propose a holistic and comprehensive study for dealing with new music collections in OMR, including extensive experiments to identify key aspects to have in mind that lead to better performance ratios. We resort to collections written in Mensural notation as specific use case, comprising 5 different corpora of training domains and up to 15 test collections. Our experiments report many interesting insights that will be important to create a manual of best practices when dealing with new collections in OMR systems.",
      "abstract": "Optical Music Recognition (OMR) has become a popular technology to retrieve information present in musical scores in conjunction with the increasing improvement of Deep Learning techniques, which represent the state-of-the-art in the field. However, its effectiveness is limited to cases where the target collection is similar in musical context and graphical appearance to the available training examples. To address this limitation, researchers have resorted to labeling examples for specific neural models, which is time-consuming and raises questions about usability. In this study, we propose a holistic and comprehensive study for dealing with new music collections in OMR, including extensive experiments to identify key aspects to have in mind that lead to better performance ratios. We resort to collections written in Mensural notation as specific use case, comprising 5 different corpora of training domains and up to 15 test collections. Our experiments report many interesting insights that will be important to create a manual of best practices when dealing with new collections in OMR systems.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1w9jaxLD0ZFatSOtQIh_czdrrtShP9pfH)</b>",
      "authors": [
        "Juan Carlos Martinez-Sevilla (University of Alicante)*",
        " Adri\u00e1n Rosell\u00f3 (Universidad de Alicante)",
        " David Rizo (Universidad de Alicante)",
        " Jorge Calvo-Zaragoza (University of Alicante)"
      ],
      "authors_and_affil": [
        "Juan Carlos Martinez-Sevilla (University of Alicante)*",
        " Adri\u00e1n Rosell\u00f3 (Universidad de Alicante)",
        " David Rizo (Universidad de Alicante)",
        " Jorge Calvo-Zaragoza (University of Alicante)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B68PN73",
      "day": "3",
      "keywords": [
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> annotation protocols",
        " Evaluation, datasets, and reproducibility -> evaluation methodology",
        "MIR tasks -> optical music recognition",
        "Applications -> music retrieval systems"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000037.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1eZPZUrFE9AUGQd_bba8ao7-KkZheW6HE/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-06-martinez-sevilla",
      "title": "On the Performance of Optical Music Recognition in the Absence of Specific Training Data",
      "video": "https://drive.google.com/uc?export=view&id=1w9jaxLD0ZFatSOtQIh_czdrrtShP9pfH"
    },
    "forum": "85",
    "id": "85",
    "pic_id": "https://drive.google.com/file/d/1oXTE-pEwab37bpwkZLtolcKUZCfhrmcc/view",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We introduce Composer\u2019s Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files.\n",
      "abstract": "We introduce Composer\u2019s Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of measures from a MIDI file, and we train a T5-like model to accomplish this task. Composer's Assistant consists of this model together with scripts that enable interaction with the model in REAPER. We conduct objective and subjective tests of our model. We release our complete system, consisting of source code, pretrained models, and REAPER scripts. Our models were trained only on permissively-licensed MIDI files.\n<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1_MuEV4JpOSaYscTf3Vm3f10g07XRwyU7)</b>",
      "authors": [
        "Martin E Malandro (Sam Houston State University)*"
      ],
      "authors_and_affil": [
        "Martin E Malandro (Sam Houston State University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTFDAG6",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "Human-centered MIR -> human-computer interaction",
        " MIR tasks -> music generation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000038.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1R7ShCQyyIiT004viq1WSHOZGENpn3MdV/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-07-malandro",
      "title": "Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling",
      "video": "https://drive.google.com/uc?export=view&id=1_MuEV4JpOSaYscTf3Vm3f10g07XRwyU7"
    },
    "forum": "113",
    "id": "113",
    "pic_id": "https://drive.google.com/file/d/1PFs40Nj02piatufF5cFyj_rGrnxr51UQ/view",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We introduce a novel audio corpus, the FAV Corpus, of over 400 favorite musical excerpts and pieces, formal analyses, and free-response comments. In a survey, 140 American university students (mostly music majors) were asked to provide three of their favorite 15-second musical excerpts, from any genre or time period. For each selection, respondents were asked: \u201cWhy do you love the excerpt? Try to be as specific and detailed as possible (music theory terms are encouraged but not required).\u201d Classical selections were dominated by a very small number of composers, while the pop and jazz artists were diverse. A thematic coding of the respondents\u2019 comments found that the most common themes were melody (34.2% of comments), harmony (27.2%), and sonic factors: texture (27.6%), instrumentation (24.3%), and timbre (12.5%). (Rhythm (19.5%) and meter (4.6%) were less present in the comments.) The comments cite simplicity three times more than complexity, and energy gain 14 times more than energy decrease, suggesting that people's favorite excerpts involve simple moments of energy gain or \"build-up\". The complete FAV Corpus is publicly available online at EthanLustig.com/FavCorpus. We will discuss future possibilities for the corpus, including potential directions in the spaces of machine learning and music recommendation.",
      "abstract": "We introduce a novel audio corpus, the FAV Corpus, of over 400 favorite musical excerpts and pieces, formal analyses, and free-response comments. In a survey, 140 American university students (mostly music majors) were asked to provide three of their favorite 15-second musical excerpts, from any genre or time period. For each selection, respondents were asked: \u201cWhy do you love the excerpt? Try to be as specific and detailed as possible (music theory terms are encouraged but not required).\u201d Classical selections were dominated by a very small number of composers, while the pop and jazz artists were diverse. A thematic coding of the respondents\u2019 comments found that the most common themes were melody (34.2% of comments), harmony (27.2%), and sonic factors: texture (27.6%), instrumentation (24.3%), and timbre (12.5%). (Rhythm (19.5%) and meter (4.6%) were less present in the comments.) The comments cite simplicity three times more than complexity, and energy gain 14 times more than energy decrease, suggesting that people's favorite excerpts involve simple moments of energy gain or \"build-up\". The complete FAV Corpus is publicly available online at EthanLustig.com/FavCorpus. We will discuss future possibilities for the corpus, including potential directions in the spaces of machine learning and music recommendation.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1nbsc_C380nLwUlgZQJRDJ7Pme8oIapqI)</b>",
      "authors": [
        "Ethan Lustig (Ethan Lustig)*",
        " David Temperley (Eastman School of Music)"
      ],
      "authors_and_affil": [
        "Ethan Lustig (Ethan Lustig)*",
        " David Temperley (Eastman School of Music)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE2NEQK",
      "day": "3",
      "keywords": [
        " Knowledge-driven approaches to MIR -> representations of music",
        "Knowledge-driven approaches to MIR -> cognitive MIR",
        " Musical features and properties -> musical affect, emotion and mood",
        "Human-centered MIR -> personalization",
        " Human-centered MIR -> user-centered evaluation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000039.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1bdzme3wv93SkvLyotfnMp72f8tt2jE-s/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-08-lustig",
      "title": "The FAV Corpus: An Audio Dataset of Favorite Pieces and Excerpts, With Formal Analyses and Music Theory Descriptors",
      "video": "https://drive.google.com/uc?export=view&id=1nbsc_C380nLwUlgZQJRDJ7Pme8oIapqI"
    },
    "forum": "114",
    "id": "114",
    "pic_id": "https://drive.google.com/file/d/1r5OfnK5jaNHBKre4Wk9uEkbgGr5ZmASG/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today\u2019s most performant chat-based large language model. In the proposed method, Whisper functions as the \u201cear\u201d by transcribing the audio, while GPT-4 serves as the \u201cbrain,\u201d acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copy-right license, based on MTG-Jamendo, and offer a human- annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.",
      "abstract": "We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today\u2019s most performant chat-based large language model. In the proposed method, Whisper functions as the \u201cear\u201d by transcribing the audio, while GPT-4 serves as the \u201cbrain,\u201d acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copy-right license, based on MTG-Jamendo, and offer a human- annotated subset for noise level estimation and evaluation. We anticipate that our proposed method and dataset will advance the development of multilingual lyrics transcription, a challenging and emerging task.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1h5vskyrQIKUo9cmat3NHqZ1g_a6Uo0-9)</b>",
      "authors": [
        "Le Zhuo (Beihang University)",
        " Ruibin Yuan (CMU)*",
        " Jiahao Pan (HKBU)",
        " Yinghao MA (Queen Mary University of London)",
        " Yizhi Li (The University  of Sheffield)",
        " Ge Zhang (University of Michigan)",
        " Si Liu (Beihang University)",
        " Roger B. Dannenberg (School of Comp"
      ],
      "authors_and_affil": [
        "Le Zhuo (Beihang University)",
        " Ruibin Yuan (CMU)*",
        " Jiahao Pan (HKBU)",
        " Yinghao MA (Queen Mary University of London)",
        " Yizhi Li (The University  of Sheffield)",
        " Ge Zhang (University of Michigan)",
        " Si Liu (Beihang University)",
        " Roger B. Dannenberg (School of Comp"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410RM5TN",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> lyrics and other textual data",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility -> annotation protocols"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000040.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1rx9VXZ4WlFb24S7awWdrD9cStrRUqLZs/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-09-yuan",
      "title": "LyricWhiz: Robust Multilingual Lyrics Transcription by Whispering to ChatGPT",
      "video": "https://drive.google.com/uc?export=view&id=1h5vskyrQIKUo9cmat3NHqZ1g_a6Uo0-9"
    },
    "forum": "117",
    "id": "117",
    "pic_id": "https://drive.google.com/file/d/1jKPwGZ-emmV1pjh9bqaN0rIq3OrVZeUy/view",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In piano performance, some mistakes stand out to listeners, whereas others may go unnoticed. Former research concluded that the salience of mistakes depended on factors including their contextual appropriateness and a listener\u2019s degree of familiarity to what is being performed. A conspicuous error is considered to be an area where there is something obviously wrong with the performance, which a listener can detect regardless of their degree of knowledge of what is being performed. Analogously, this paper attempts to build a score-independent conspicuous error detector for standard piano repertoire of beginner to intermediate students. We gather three qualitatively different piano playing MIDI data: (1) 103 sight-reading sessions for beginning and intermediate adult pianists with formal music training, (2) 245 performances by presumably late-beginner to early-advanced pianists on a digital piano, and (3) 50 etude performances by an advanced pianist. The data was annotated at the regions considered to contain conspicuous mistakes. Then, we use a Temporal Convolutional Network to detect the sites of such mistakes from the piano roll. We investigate the use of two pre-training methods to overcome data scarcity: (1) synthetic data with procedurally-generated mistakes, and (2) training a part of the model as a piano roll auto-encoder. Experimental evaluation shows that the TCN performs at an F-measure of 0.78 without pretraining for sight-reading data, but the proposed pretraining steps improve the F-measure on performance and etude data, approaching the agreement between human raters on conspicuous error labels. Importantly, we report on the lessons learned from this pilot study, and what should be addressed to continue this research direction.",
      "abstract": "In piano performance, some mistakes stand out to listeners, whereas others may go unnoticed. Former research concluded that the salience of mistakes depended on factors including their contextual appropriateness and a listener\u2019s degree of familiarity to what is being performed. A conspicuous error is considered to be an area where there is something obviously wrong with the performance, which a listener can detect regardless of their degree of knowledge of what is being performed. Analogously, this paper attempts to build a score-independent conspicuous error detector for standard piano repertoire of beginner to intermediate students. We gather three qualitatively different piano playing MIDI data: (1) 103 sight-reading sessions for beginning and intermediate adult pianists with formal music training, (2) 245 performances by presumably late-beginner to early-advanced pianists on a digital piano, and (3) 50 etude performances by an advanced pianist. The data was annotated at the regions considered to contain conspicuous mistakes. Then, we use a Temporal Convolutional Network to detect the sites of such mistakes from the piano roll. We investigate the use of two pre-training methods to overcome data scarcity: (1) synthetic data with procedurally-generated mistakes, and (2) training a part of the model as a piano roll auto-encoder. Experimental evaluation shows that the TCN performs at an F-measure of 0.78 without pretraining for sight-reading data, but the proposed pretraining steps improve the F-measure on performance and etude data, approaching the agreement between human raters on conspicuous error labels. Importantly, we report on the lessons learned from this pilot study, and what should be addressed to continue this research direction.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Cx0nL2HWni7ocLN0GUXBM3g-IZ0_oOAi)</b>",
      "authors": [
        "Alia Morsi (Universitat Pompeu Fabra)*",
        " Kana Tatsumi (Nagoya Institute of Technology)",
        " Akira Maezawa (Yamaha Corporation)",
        " Takuya Fujishima (Yamaha Corporation)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "authors_and_affil": [
        "Alia Morsi (Universitat Pompeu Fabra)*",
        " Kana Tatsumi (Nagoya Institute of Technology)",
        " Akira Maezawa (Yamaha Corporation)",
        " Takuya Fujishima (Yamaha Corporation)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J17J2DV",
      "day": "3",
      "keywords": [
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> expression and performative aspects of music",
        "Evaluation, datasets, and reproducibility -> annotation protocols",
        " MIR tasks -> automatic classification",
        "Applications -> music training and education"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000041.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1FNEeaNq4xjANe8Ui888VjxYaFHT3RO5M/view?usp=sharing",
      "session": [
        "3"
      ],
      "slack_channel": "p3-10-morsi",
      "title": "Sounds Out of Pl\u00e4ce? Score Independent Detection of Conspicuous Mistakes in Piano Performances",
      "video": "https://drive.google.com/uc?export=view&id=1Cx0nL2HWni7ocLN0GUXBM3g-IZ0_oOAi"
    },
    "forum": "118",
    "id": "118",
    "pic_id": "https://drive.google.com/file/d/15KebKdcDpidnwnwH_OwEdVeHXDBNZOnq/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. \nWe use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. \nWe use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1x7EP-4GCeM9fHEmrMHPozzRj9cldvYty)</b>",
      "authors": [
        "Hugo F  Flores Garcia (Northwestern University)*",
        " Prem Seetharaman (Northwestern University)",
        " Rithesh Kumar (Descript)",
        " Bryan Pardo (Northwestern University)"
      ],
      "authors_and_affil": [
        "Hugo F  Flores Garcia (Northwestern University)*",
        " Prem Seetharaman (Northwestern University)",
        " Rithesh Kumar (Descript)",
        " Bryan Pardo (Northwestern University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE3326P",
      "day": "3",
      "keywords": [
        "Applications -> music composition, performance, and production",
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " MIR tasks -> music synthesis and transformation",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000042.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Bt4nhwqEbz0rlNkKwwv_77wA7ThABDDM/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-11-garcia",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "video": "https://drive.google.com/uc?export=view&id=1x7EP-4GCeM9fHEmrMHPozzRj9cldvYty"
    },
    "forum": "125",
    "id": "125",
    "pic_id": "https://drive.google.com/file/d/1AYBS6z3-ZfjmS0YXALUbZfOAEj1m9cZ1/view",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Learning an instrument can be rewarding, but is unavoidably a huge undertaking. Receiving constructive feedback on one\u2019s playing is crucial for improvement. However, personal feedback from an expert instructor is seldom available on demand. The goal motivating this project is to build software that will provide comparably useful feedback to beginners, in order to supplement feedback from human instructors. To lay the groundwork for that, in this paper we investigate performance assessment criteria from both quantitative and qualitative perspectives. We gathered 83 piano performances from 21 players. Each recording was evaluated by both expert piano instructors and novice players. This dataset is unique in that the novice evaluators are also players, and that both quantitative and qualitative evaluations are collected. Our analysis of the evaluations indicates that the kind of specific, concrete piano techniques that are most elusive to novice evaluators are precisely the kind of characteristics that can be detected, measured, and visualized for learners by a well-designed software tool.",
      "abstract": "Learning an instrument can be rewarding, but is unavoidably a huge undertaking. Receiving constructive feedback on one\u2019s playing is crucial for improvement. However, personal feedback from an expert instructor is seldom available on demand. The goal motivating this project is to build software that will provide comparably useful feedback to beginners, in order to supplement feedback from human instructors. To lay the groundwork for that, in this paper we investigate performance assessment criteria from both quantitative and qualitative perspectives. We gathered 83 piano performances from 21 players. Each recording was evaluated by both expert piano instructors and novice players. This dataset is unique in that the novice evaluators are also players, and that both quantitative and qualitative evaluations are collected. Our analysis of the evaluations indicates that the kind of specific, concrete piano techniques that are most elusive to novice evaluators are precisely the kind of characteristics that can be detected, measured, and visualized for learners by a well-designed software tool.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1uHn0_0BtDAzHyx2lewrp2xuAMO6BWhWM)</b>",
      "authors": [
        "Yucong Jiang (University of Richmond)*"
      ],
      "authors_and_affil": [
        "Yucong Jiang (University of Richmond)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK6RDJ9",
      "day": "3",
      "keywords": [
        " MIR tasks -> alignment, synchronization, and score following",
        " MIR fundamentals and methodology -> lyrics and other textual data",
        " Musical features and properties -> expression and performative aspects of music",
        "Applications -> music training and education",
        "Human-centered MIR -> music interfaces and services",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000043.pdf",
      "poster_pdf": "https://drive.google.com/file/d/117YbHrFSD6J4L5i0bxrzfF02jqLJNkMv/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-12-jiang",
      "title": "Expert and Novice Evaluations of Piano Performances: Criteria for Computer-Aided Feedback",
      "video": "https://drive.google.com/uc?export=view&id=1uHn0_0BtDAzHyx2lewrp2xuAMO6BWhWM"
    },
    "forum": "129",
    "id": "129",
    "pic_id": "https://drive.google.com/file/d/1oPnn8Zl3n7s544Q3m1yvVAsfAfOQGx12/view",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate our method successfully combines complementary information from diverse modalities, and is more robust to missing modality data (i.e., it better handles the retrieval of artists with different modality embeddings than the query artist\u2019s).",
      "abstract": "Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally represented in several modalities, e.g., audio signals, user interaction data, or editorial data. However, data of any given modality might not be available for all items in any music dataset. In this work, we propose a method based on contrastive learning to combine embeddings from multiple modalities and explore the impact of the presence or absence of embeddings from diverse modalities in an artist similarity task. Experiments on two datasets suggest that our contrastive method outperforms single-modality embeddings and baseline algorithms for combining modalities, both in terms of artist retrieval accuracy and coverage. Improvements with respect to other methods are particularly significant for less popular query artists. We demonstrate our method successfully combines complementary information from diverse modalities, and is more robust to missing modality data (i.e., it better handles the retrieval of artists with different modality embeddings than the query artist\u2019s).<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1glaZPkG4r3hjat4frky06CPI2dJQu7Gv)</b>",
      "authors": [
        "Andres Ferraro (Pandora/SiriusXM)*",
        " Jaehun Kim (Pandora / SiriusXM)",
        " Andreas Ehmann (Pandora)",
        " Sergio Oramas (Pandora/SiriusXM)",
        " Fabien Gouyon (Pandora/SiriusXM)"
      ],
      "authors_and_affil": [
        "Andres Ferraro (Pandora/SiriusXM)*",
        " Jaehun Kim (Pandora / SiriusXM)",
        " Andreas Ehmann (Pandora)",
        " Sergio Oramas (Pandora/SiriusXM)",
        " Fabien Gouyon (Pandora/SiriusXM)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK6UPHT",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> multimodality",
        " Musical features and properties -> representations of music",
        "Applications -> music retrieval systems",
        "Applications -> music recommendation and playlist generation",
        " MIR tasks -> similarity metrics"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000044.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1uiop7gVPLVJrYbGF-v71nbYQBKSMEta2/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-13-ferraro",
      "title": "Contrastive Learning for Cross-Modal Artist Retrieval",
      "video": "https://drive.google.com/uc?export=view&id=1glaZPkG4r3hjat4frky06CPI2dJQu7Gv"
    },
    "forum": "147",
    "id": "147",
    "pic_id": "https://drive.google.com/file/d/13JN57EZ3ekgQYB9qN-lq9JQSsOIIoRJP/view",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "The concept of form in music encompasses a wide range of musical aspects, such as phrases and (hierarchical) segmentation, formal functions, cadences and voice-leading schemata, form templates, and repetition structure. In an effort towards a unified model of form, this paper proposes an integration of repetition structure\n  (i.e., which segments of a piece occur several times) and formal templates (such as AABA). While repetition structure can be modeled using context-free grammars,\n  most prior approaches allow for arbitrary grammar rules. Constraining the structure of the inferred rules to conform to a small set of templates (meta-rules) not only reduces the space of possible rules that need to be considered but also ensures that the resulting repetition grammar remains interpretable in the context of musical form.\n  The resulting formalism can be extended to cases of varied repetition and thus constitutes a building block for a larger model of form.",
      "abstract": "The concept of form in music encompasses a wide range of musical aspects, such as phrases and (hierarchical) segmentation, formal functions, cadences and voice-leading schemata, form templates, and repetition structure. In an effort towards a unified model of form, this paper proposes an integration of repetition structure\n  (i.e., which segments of a piece occur several times) and formal templates (such as AABA). While repetition structure can be modeled using context-free grammars,\n  most prior approaches allow for arbitrary grammar rules. Constraining the structure of the inferred rules to conform to a small set of templates (meta-rules) not only reduces the space of possible rules that need to be considered but also ensures that the resulting repetition grammar remains interpretable in the context of musical form.\n  The resulting formalism can be extended to cases of varied repetition and thus constitutes a building block for a larger model of form.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1_NLe1v6sPdZHtLwyG9EPrTKMr4NYPRxr)</b>",
      "authors": [
        "Christoph Finkensiep (EPFL)*",
        " Matthieu Haeberle (EPFL)",
        " Friedrich Eisenbrand (EPFL)",
        " Markus Neuwirth (Anton Bruckner Privatuniversit\u00e4t Linz)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "authors_and_affil": [
        "Christoph Finkensiep (EPFL)*",
        " Matthieu Haeberle (EPFL)",
        " Friedrich Eisenbrand (EPFL)",
        " Markus Neuwirth (Anton Bruckner Privatuniversit\u00e4t Linz)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410S8TBN",
      "day": "3",
      "keywords": [
        "Computational musicology",
        " Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " Musical features and properties -> structure, segmentation, and form",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Computational musicology -> mathematical music theory"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000045.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1g6z6blYSx82jkFjDSXELbDiatLGdIWnC/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-14-finkensiep",
      "title": "Repetition-Structure Inference With Formal Prototypes",
      "video": "https://drive.google.com/uc?export=view&id=1_NLe1v6sPdZHtLwyG9EPrTKMr4NYPRxr"
    },
    "forum": "139",
    "id": "139",
    "pic_id": "https://drive.google.com/file/d/1Ss8ym3rjM_ntIrTuqqOmIz0uGkcHnFBA/view",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Most melodies from the Western common practice period have a harmonic background, i.e., a succession of chords that fit the melody. In this paper we provide a novel approach to infer this harmonic background from the score notation of a melody. We first construct a pitch context vector for each note in the melody. This vector summarises the pitches that are in the preceding and following contexts of the note. Next, we use these pitch context vectors to generate a list of candidate chords for each note. The candidate chords fit the pitch context of a given note each with a computed strength. Finally, we find an optimal path through the chord candidates, employing a score function for the fitness of a given candidate chord. The algorithm chooses one chord for each note, optimizing the total score. A set of heuristics is incorporated in the score function. The system is heavily parameterised, extremely flexible, and does not need training. This creates a framework to experiment with harmonization of melodies. The output is evaluated by an expert survey, which yields convincing and positive results.",
      "abstract": "Most melodies from the Western common practice period have a harmonic background, i.e., a succession of chords that fit the melody. In this paper we provide a novel approach to infer this harmonic background from the score notation of a melody. We first construct a pitch context vector for each note in the melody. This vector summarises the pitches that are in the preceding and following contexts of the note. Next, we use these pitch context vectors to generate a list of candidate chords for each note. The candidate chords fit the pitch context of a given note each with a computed strength. Finally, we find an optimal path through the chord candidates, employing a score function for the fitness of a given candidate chord. The algorithm chooses one chord for each note, optimizing the total score. A set of heuristics is incorporated in the score function. The system is heavily parameterised, extremely flexible, and does not need training. This creates a framework to experiment with harmonization of melodies. The output is evaluated by an expert survey, which yields convincing and positive results.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1z4Ty51ZGrqT_IOXD08dqGK99PE3pHy-a)</b>",
      "authors": [
        "Peter Van Kranenburg (Utrecht University",
        " Meertens Institute)*",
        " Eoin J Kearns (Meertens Instituut)"
      ],
      "authors_and_affil": [
        "Peter Van Kranenburg (Utrecht University",
        " Meertens Institute)*",
        " Eoin J Kearns (Meertens Instituut)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J185MD5",
      "day": "3",
      "keywords": [
        "Computational musicology -> digital musicology",
        " Knowledge-driven approaches to MIR -> computational ethnomusicology",
        " MIR tasks -> music generation",
        " Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " Musical features and properties -> harmony, chords and tonality",
        "Musical features and properties -> melody and motives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000046.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1fti4fr4-IrECum8xM05AGxKQbzfPEd5H/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-15-kranenburg",
      "title": "Algorithmic Harmonization of Tonal Melodies Using Weighted Pitch Context Vectors",
      "video": "https://drive.google.com/uc?export=view&id=1z4Ty51ZGrqT_IOXD08dqGK99PE3pHy-a"
    },
    "forum": "140",
    "id": "140",
    "pic_id": "https://drive.google.com/file/d/1JZegifNNWqpB8I8YSPIMA7B3XB6gssy8/view",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "This paper proposes a text-to-lyrics generation method, aiming to provide lyric writing support by suggesting the generated lyrics to users who struggle to find the right words to convey their message. Previous studies on lyrics generation have focused on generating lyrics based on semantic constraints such as specific keywords, lyric style, and topics. However, these methods had limitations because users could not freely input their intentions as text. Even if such intentions can be given as input text, the lyrics generated from the input tend to contain similar wording, making it difficult to inspire the user. Our method is therefore developed to generate lyrics that (1) convey a message similar to the input text and (2) contain wording different from the input text. A straightforward approach of training a text-to-lyrics encoder-decoder is not feasible since there is no text-lyric paired data for this purpose. To overcome this issue, we divide the text-to-lyrics generation process into a two-step pipeline, eliminating the need for text-lyric paired data. (a) First, we use an existing text-to-image generation technique as a text analyzer to obtain an image that captures the meaning of the input text, ignoring the wording. (b) Next, we use our proposed image-to-lyrics encoder-decoder (I2L) to generate lyrics from the obtained image while preserving its meaning. The training of this I2L model only requires pairs of \"lyrics\" and \"images generated from lyrics\", which are readily prepared. In addition, we propose for the first time a lyrics generation method that reduces the risk of plagiarism by prohibiting the generation of uncommon phrases in the training data. Experimental results show that the proposed method can generate lyrics with different phrasing while conveying a message similar to the input text.",
      "abstract": "This paper proposes a text-to-lyrics generation method, aiming to provide lyric writing support by suggesting the generated lyrics to users who struggle to find the right words to convey their message. Previous studies on lyrics generation have focused on generating lyrics based on semantic constraints such as specific keywords, lyric style, and topics. However, these methods had limitations because users could not freely input their intentions as text. Even if such intentions can be given as input text, the lyrics generated from the input tend to contain similar wording, making it difficult to inspire the user. Our method is therefore developed to generate lyrics that (1) convey a message similar to the input text and (2) contain wording different from the input text. A straightforward approach of training a text-to-lyrics encoder-decoder is not feasible since there is no text-lyric paired data for this purpose. To overcome this issue, we divide the text-to-lyrics generation process into a two-step pipeline, eliminating the need for text-lyric paired data. (a) First, we use an existing text-to-image generation technique as a text analyzer to obtain an image that captures the meaning of the input text, ignoring the wording. (b) Next, we use our proposed image-to-lyrics encoder-decoder (I2L) to generate lyrics from the obtained image while preserving its meaning. The training of this I2L model only requires pairs of \"lyrics\" and \"images generated from lyrics\", which are readily prepared. In addition, we propose for the first time a lyrics generation method that reduces the risk of plagiarism by prohibiting the generation of uncommon phrases in the training data. Experimental results show that the proposed method can generate lyrics with different phrasing while conveying a message similar to the input text.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1DWTDGJ_tm3_Bcyy15p7Nd30HtcOWXSd7)</b>",
      "authors": [
        "Kento Watanabe (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "authors_and_affil": [
        "Kento Watanabe (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTGDX1U",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        "MIR fundamentals and methodology -> lyrics and other textual data",
        " MIR fundamentals and methodology -> web mining, and natural language processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000047.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Jg01Ghr0AaE6RPHudgG1cW6zqYC9J_hj/view",
      "session": [
        "3"
      ],
      "slack_channel": "p3-16-watanabe",
      "title": "Text-to-Lyrics Generation With Image-Based Semantics and Reduced Risk of Plagiarism",
      "video": "https://drive.google.com/uc?export=view&id=1DWTDGJ_tm3_Bcyy15p7Nd30HtcOWXSd7"
    },
    "forum": "142",
    "id": "142",
    "pic_id": "https://drive.google.com/file/d/1le4B70DjdhgRRPBBy7prQnoos-dVUOXB/view",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.",
      "abstract": "Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1PeK7SflyMaoIyGKdQQ5xnEVKslx39qqp)</b>",
      "authors": [
        "Seungheon Doh (KAIST)*",
        " Keunwoo Choi (Gaudio Lab, Inc.)",
        " Jongpil Lee (Neutune)",
        " Juhan Nam (KAIST)"
      ],
      "authors_and_affil": [
        "Seungheon Doh (KAIST)*",
        " Keunwoo Choi (Gaudio Lab, Inc.)",
        " Jongpil Lee (Neutune)",
        " Juhan Nam (KAIST)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK7DBSR",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> multimodality",
        " MIR tasks -> automatic classification",
        "MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web"
      ],
      "long_presentation": "True",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000048.pdf",
      "poster_pdf": "https://drive.google.com/file/d/14lE01yNAmpBWGastr-Ve09Cz1YSCiRRH/view?usp=share_link",
      "session": [
        "4"
      ],
      "slack_channel": "p4-01-doh",
      "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning",
      "video": "https://drive.google.com/uc?export=view&id=1PeK7SflyMaoIyGKdQQ5xnEVKslx39qqp"
    },
    "forum": "219",
    "id": "219",
    "pic_id": "https://drive.google.com/file/d/1COeBiC463MUcyj5fiL2FCvYrt7-DROaX/view?usp=share_link",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and therefore fail at encompassing this essential aspect of music structure. This work introduces a triplet mining method which explicitly considers repeating sequences occurring inside a music track by leveraging common audio descriptors. We study its impact on the learned representations through downstream music segmentation. Because musical repetitions can be of different natures, we give further insight on the role of the audio descriptors employed at the triplet mining stage as well as the trade-off existing between the quality of the triplets mined and the quantity of unlabelled data used for training. We observe that our method requires less non-annotated data while remaining competitive against other unsupervised methods trained on a larger corpus.",
      "abstract": "Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and therefore fail at encompassing this essential aspect of music structure. This work introduces a triplet mining method which explicitly considers repeating sequences occurring inside a music track by leveraging common audio descriptors. We study its impact on the learned representations through downstream music segmentation. Because musical repetitions can be of different natures, we give further insight on the role of the audio descriptors employed at the triplet mining stage as well as the trade-off existing between the quality of the triplets mined and the quantity of unlabelled data used for training. We observe that our method requires less non-annotated data while remaining competitive against other unsupervised methods trained on a larger corpus.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1R7OXYDdiBnHw0MOzYYS22E5I0BzAsudD)</b>",
      "authors": [
        "Morgan Buisson (Telecom-Paris)*",
        " Brian McFee (New York University)",
        " Slim Essid (Telecom Paris - Institut Polytechnique de Paris)",
        " Helene-Camille Crayencour (CNRS)"
      ],
      "authors_and_affil": [
        "Morgan Buisson (Telecom-Paris)*",
        " Brian McFee (New York University)",
        " Slim Essid (Telecom Paris - Institut Polytechnique de Paris)",
        " Helene-Camille Crayencour (CNRS)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTGLTK8",
      "day": "3",
      "keywords": [
        " Knowledge-driven approaches to MIR -> representations of music",
        " Musical features and properties -> representations of music",
        "Musical features and properties -> structure, segmentation, and form",
        " MIR tasks -> pattern matching and detection",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000049.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1u-UNDElMDqS_kKycvYlk-SzGI-pOCQHW/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-02-buisson",
      "title": "A Repetition-Based Triplet Mining Approach for Music Segmentation",
      "video": "https://drive.google.com/uc?export=view&id=1R7OXYDdiBnHw0MOzYYS22E5I0BzAsudD"
    },
    "forum": "146",
    "id": "146",
    "pic_id": "https://drive.google.com/file/d/17MzD1-iUXo7XGH97XV0-A-ntlQ2eQ6_j/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis.\u00a0The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information.\u00a0Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree.\nOne major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs.\u00a0We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods.",
      "abstract": "This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis.\u00a0The parsing involves two steps. First, the input sequence is passed through a transformer encoder to enrich it with contextual information.\u00a0Then, a classifier filters the graph of all possible dependency arcs to produce the dependency tree.\nOne major benefit of this system is that it can be easily integrated into modern deep-learning pipelines. Moreover, since it does not rely on any particular symbolic grammar, it can consider multiple musical features simultaneously, make use of sequential context information, and produce partial results for noisy inputs.\u00a0We test our approach on two datasets of musical trees -- time-span trees of monophonic note sequences and harmonic trees of jazz chord sequences -- and show that our approach outperforms previous methods.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1RDdU2rt3vymAbrxAePGUMcqIdsb9alVR)</b>",
      "authors": [
        "Francesco Foscarin (Johannes Kepler University Linz)*",
        " Daniel Harasim (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "authors_and_affil": [
        "Francesco Foscarin (Johannes Kepler University Linz)*",
        " Daniel Harasim (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTGQA5C",
      "day": "3",
      "keywords": [
        " Musical features and properties -> melody and motives",
        "Computational musicology -> digital musicology",
        " Musical features and properties -> structure, segmentation, and form",
        " Musical features and properties -> harmony, chords and tonality",
        "MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000050.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1nMXueZAbPiQ7mt4ULaRE4rKCIOc2nqb3/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-03-foscarin",
      "title": "Predicting Music Hierarchies With a Graph-Based Neural Decoder",
      "video": "https://drive.google.com/uc?export=view&id=1RDdU2rt3vymAbrxAePGUMcqIdsb9alVR"
    },
    "forum": "87",
    "id": "87",
    "pic_id": "https://drive.google.com/file/d/1L5mXxa9Uqnaw5ccYK7Yc-lhKO5DKiRa6/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between the training data and its weakly annotated targets. One major problem is that a mismatch between the estimated soft alignments and the reference alignments in the early training stage leads to incorrect parameter updates, making the overall training procedure unstable. In this paper, we investigate such stability issues by considering the task of pitch class estimation from music recordings as an illustrative case study. In particular, we introduce and discuss three conceptually different strategies (a hyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy) with the objective of stabilizing intermediate soft alignment results. Finally, we report on experiments that demonstrate the effectiveness of the strategies and discuss efficiency and implementation issues.",
      "abstract": "Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between the training data and its weakly annotated targets. One major problem is that a mismatch between the estimated soft alignments and the reference alignments in the early training stage leads to incorrect parameter updates, making the overall training procedure unstable. In this paper, we investigate such stability issues by considering the task of pitch class estimation from music recordings as an illustrative case study. In particular, we introduce and discuss three conceptually different strategies (a hyperparameter scheduling, a diagonal prior, and a sequence unfolding strategy) with the objective of stabilizing intermediate soft alignment results. Finally, we report on experiments that demonstrate the effectiveness of the strategies and discuss efficiency and implementation issues.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1-78moijWNgGdVggNJ7Y9-m0Zf-urbrCc)</b>",
      "authors": [
        "Johannes Zeitler (International Audio Laboratories Erlangen)*",
        " Simon Deniffel (International Audio Laboratories Erlangen)",
        " Michael Krause (International Audio Laboratories Erlangen)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "authors_and_affil": [
        "Johannes Zeitler (International Audio Laboratories Erlangen)*",
        " Simon Deniffel (International Audio Laboratories Erlangen)",
        " Michael Krause (International Audio Laboratories Erlangen)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J18NFHD",
      "day": "3",
      "keywords": [
        " MIR tasks -> music transcription and annotation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000051.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1CJoFlJz2D5RNrZGTpc38z21k2hmv8GNw/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-04-zeitler",
      "title": "Stabilizing Training With Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation With Weakly Aligned Targets",
      "video": "https://drive.google.com/uc?export=view&id=1-78moijWNgGdVggNJ7Y9-m0Zf-urbrCc"
    },
    "forum": "133",
    "id": "133",
    "pic_id": "https://drive.google.com/file/d/1MLqN1GfLKbUXUNEh7GwFKx8cZ8N1hV3E/view",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.\n\n",
      "abstract": "In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.\n\n<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=15pD_MIwtse6iGJYq87yB3clpOQF1yXUb)</b>",
      "authors": [
        "Danbinaerin Han (Sogang Univ.)",
        " Rafael Caro Repetto (Kunstuniversit\u00e4t Graz)",
        " Dasaem Jeong (Sogang University)*"
      ],
      "authors_and_affil": [
        "Danbinaerin Han (Sogang Univ.)",
        " Rafael Caro Repetto (Kunstuniversit\u00e4t Graz)",
        " Dasaem Jeong (Sogang University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGPFXDE",
      "day": "3",
      "keywords": [
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Musical features and properties -> melody and motives",
        "Knowledge-driven approaches to MIR -> computational ethnomusicology",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000052.pdf",
      "poster_pdf": "https://drive.google.com/file/d/17T2UdKBXGrBPEkuGEqXvNkIK7ZzyRdtC/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-05-jeong",
      "title": "Finding Tori: Self-Supervised Learning for Analyzing Korean Folk Song",
      "video": "https://drive.google.com/uc?export=view&id=15pD_MIwtse6iGJYq87yB3clpOQF1yXUb"
    },
    "forum": "149",
    "id": "149",
    "pic_id": "https://drive.google.com/file/d/1quJ5o6fdJ_S2CUXY8j9UJUsQOMdc2wGb/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "\nSignificant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas.",
      "abstract": "\nSignificant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and trained models to facilitate further research on singing voice and related areas.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1r37TEwXvewpSGLyZrd4H5H5x3qy3zoQG)</b>",
      "authors": [
        "Bernardo Torres (Telecom Paris, Institut polytechnique de Paris)*",
        " Stefan Lattner (Sony CSL)",
        " Ga\u00ebl Richard (Telecom Paris, Institut polytechnique de Paris)"
      ],
      "authors_and_affil": [
        "Bernardo Torres (Telecom Paris, Institut polytechnique de Paris)*",
        " Stefan Lattner (Sony CSL)",
        " Ga\u00ebl Richard (Telecom Paris, Institut polytechnique de Paris)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK80NP7",
      "day": "3",
      "keywords": [
        " MIR tasks -> indexing and querying",
        " MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "Knowledge-driven approaches to MIR -> representations of music",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " MIR tasks -> similarity metrics"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000053.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1B3Wp9yCZoTjOkhRhx831DlTksG2ErIM4/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-06-torres",
      "title": "Singer Identity Representation Learning Using Self-Supervised Techniques",
      "video": "https://drive.google.com/uc?export=view&id=1r37TEwXvewpSGLyZrd4H5H5x3qy3zoQG"
    },
    "forum": "204",
    "id": "204",
    "pic_id": "https://drive.google.com/file/d/1o44VOuMP9sJ60tGerTcJSSM-JyvkUmk2/view",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent models such as wav2vec2.0 have shown promise. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train 12 SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.",
      "abstract": "Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent models such as wav2vec2.0 have shown promise. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train 12 SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech. However, we identify the limitations of such existing speech-oriented designs, especially in modelling polyphonic information. Based on the experimental results, empirical suggestions are also given for designing future musical SSL strategies and paradigms.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=10mpCdKtSk5Yoru88f_Xb9G9ws8hWff3Z)</b>",
      "authors": [
        "Yinghao MA (Queen Mary University of London)*",
        " Ruibin Yuan (CMU)",
        " Yizhi Li (The University  of Sheffield)",
        " Ge Zhang (University of Michigan)",
        " Chenghua Lin (University of Sheffield)",
        " Xingran Chen (University of Michigan)",
        " Anton Ragni (University of Sheffie"
      ],
      "authors_and_affil": [
        "Yinghao MA (Queen Mary University of London)*",
        " Ruibin Yuan (CMU)",
        " Yizhi Li (The University  of Sheffield)",
        " Ge Zhang (University of Michigan)",
        " Chenghua Lin (University of Sheffield)",
        " Xingran Chen (University of Michigan)",
        " Anton Ragni (University of Sheffie"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6AMLG1",
      "day": "3",
      "keywords": [
        " Musical features and properties -> representations of music",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR -> representations of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000054.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1_LgOUP9QGXuJMS61lIa-LVW3U_SRovOP/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-07-ma",
      "title": "On the Effectiveness of Speech Self-Supervised Learning for Music",
      "video": "https://drive.google.com/uc?export=view&id=10mpCdKtSk5Yoru88f_Xb9G9ws8hWff3Z"
    },
    "forum": "154",
    "id": "154",
    "pic_id": "https://drive.google.com/file/d/1RqHXshWTsX5c4giQIs3sdUJXeG1Rn17I/view",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In this paper, we address the beat tracking task which is to predict beat times corresponding to the input audio. Due to the long sequential inputs, it is still challenging to model the global structure efficiently and to deal with the data imbalance between beats and no beats. In order to meet the above challenges, we propose a novel Transformer-based model consisting of a low-resolution encoder and a high-resolution decoder. The encoder with low temporal resolution is suited to capture global features with more balanced data. The decoder with high temporal resolution is designed to predict beat times at a desired resolution. In the decoder, the global structure is considered by the cross attention between the global features and high-dimensional features. There are two key modifications in the proposed model: (1) adding 1D convolutional layers in the encoder and (2) replacing positional embedding by the upsampled encoder features in the decoder. In the experiment, we achieved the state-of-the-art performance and showed that the decoder produced more precise and stable results.",
      "abstract": "In this paper, we address the beat tracking task which is to predict beat times corresponding to the input audio. Due to the long sequential inputs, it is still challenging to model the global structure efficiently and to deal with the data imbalance between beats and no beats. In order to meet the above challenges, we propose a novel Transformer-based model consisting of a low-resolution encoder and a high-resolution decoder. The encoder with low temporal resolution is suited to capture global features with more balanced data. The decoder with high temporal resolution is designed to predict beat times at a desired resolution. In the decoder, the global structure is considered by the cross attention between the global features and high-dimensional features. There are two key modifications in the proposed model: (1) adding 1D convolutional layers in the encoder and (2) replacing positional embedding by the upsampled encoder features in the decoder. In the experiment, we achieved the state-of-the-art performance and showed that the decoder produced more precise and stable results.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Q8k183DTK5whs7ysx-6HKSVhfpTuqhjK)</b>",
      "authors": [
        "Tian Cheng (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "authors_and_affil": [
        "Tian Cheng (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGPR77S",
      "day": "3",
      "keywords": [
        "",
        "Musical features and properties -> rhythm, beat, tempo"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000055.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1hPkF6xVjUmG1Y_yaqI3bZHbbSRSbS6C2/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-08-cheng",
      "title": "Transformer-Based Beat Tracking With Low-Resolution Encoder and High-Resolution Decoder",
      "video": "https://drive.google.com/uc?export=view&id=1Q8k183DTK5whs7ysx-6HKSVhfpTuqhjK"
    },
    "forum": "150",
    "id": "150",
    "pic_id": "https://drive.google.com/file/d/159NbL0joMwLVn2icPdD_E8jgBtBWstvk/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "The objective of pattern-matching topics is to gain insights into repetitive patterns within or across various music genres and cultures. This approach aims to shed light on the recurring instances present in diverse musical traditions. The paper presents a study analyzing folk songs using symbolic music representation, including melodic sequences and musical information. By examining a corpus of 400 monophonic Slovenian tunes, we are releasing annotations of structure, contour, and implied harmony. We propose an efficient algorithm based on suffix arrays and bit-vectors to match both music content (melodic sequence) and context (descriptors). Our study reveals that certain descriptors, such as contour types and harmonic \u201cstability\u201d exhibit variations based on phrase position within a tune. Additionally, combining melody and descriptors in pattern-matching queries enhances precision for classification tasks. We emphasize the importance of the interplay between melodic sequences and music descriptors, highlighting that different pattern queries may have varying levels of detail requirements. As a result, our approach promotes flexibility in computational music analysis. Lastly, our objective is to foster the knowledge of Slovenian folk songs.",
      "abstract": "The objective of pattern-matching topics is to gain insights into repetitive patterns within or across various music genres and cultures. This approach aims to shed light on the recurring instances present in diverse musical traditions. The paper presents a study analyzing folk songs using symbolic music representation, including melodic sequences and musical information. By examining a corpus of 400 monophonic Slovenian tunes, we are releasing annotations of structure, contour, and implied harmony. We propose an efficient algorithm based on suffix arrays and bit-vectors to match both music content (melodic sequence) and context (descriptors). Our study reveals that certain descriptors, such as contour types and harmonic \u201cstability\u201d exhibit variations based on phrase position within a tune. Additionally, combining melody and descriptors in pattern-matching queries enhances precision for classification tasks. We emphasize the importance of the interplay between melodic sequences and music descriptors, highlighting that different pattern queries may have varying levels of detail requirements. As a result, our approach promotes flexibility in computational music analysis. Lastly, our objective is to foster the knowledge of Slovenian folk songs.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=126Ahz0VboSi0Q92hJ9CKcnW9xLZW0AKQ)</b>",
      "authors": [
        "Vanessa Nina Borsan (Universit\u00e9 de Lille)*",
        " Mathieu Giraud (CNRS, Universit\u00e9 de Lille)",
        " Richard Groult (Universit\u00e9 de Rouen Normandie)",
        " Thierry Lecroq (Universit\u00e9 de Rouen Normandie )"
      ],
      "authors_and_affil": [
        "Vanessa Nina Borsan (Universit\u00e9 de Lille)*",
        " Mathieu Giraud (CNRS, Universit\u00e9 de Lille)",
        " Richard Groult (Universit\u00e9 de Rouen Normandie)",
        " Thierry Lecroq (Universit\u00e9 de Rouen Normandie )"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410TEZHA",
      "day": "3",
      "keywords": [
        " Musical features and properties -> melody and motives",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Knowledge-driven approaches to MIR -> computational ethnomusicology",
        " MIR tasks -> pattern matching and detection",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Computational musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000056.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1KnhvnhEOK0CZFBallZ1QasV1vJyWU3-p/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-09-borsan",
      "title": "Adding Descriptors to Melodies Improves Pattern Matching: A Study on Slovenian Folk Songs",
      "video": "https://drive.google.com/uc?export=view&id=126Ahz0VboSi0Q92hJ9CKcnW9xLZW0AKQ"
    },
    "forum": "156",
    "id": "156",
    "pic_id": "https://drive.google.com/file/d/1ICMtuG-1wFvmkPbjCbwh0HQLGOoXM-BR/view",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "As streaming services have become a main channel for music consumption, they significantly impact various stakeholders: users, artists who provide music, and other professionals working in the music industry. Therefore, it is essential to consider all stakeholders' goals and values when developing and evaluating the music recommender systems integrated into these services. One vital goal is treating artists fairly, thereby giving them a fair chance to have their music recommended and listened to, and subsequently building a fan base. Such artist fairness is often assumed to have a trade-off with user goals such as satisfaction. Using insights from two studies, this work shows the opposite: some goals from different stakeholders are complementary. Our first study, in which we interview music artists, demonstrates that they often see increased transparency and control for users as a means to also improve artist fairness. We expand with a second study asking other music industry professionals about these topics using a questionnaire. Its results indicate that transparency towards users is highly valued and should be increased.",
      "abstract": "As streaming services have become a main channel for music consumption, they significantly impact various stakeholders: users, artists who provide music, and other professionals working in the music industry. Therefore, it is essential to consider all stakeholders' goals and values when developing and evaluating the music recommender systems integrated into these services. One vital goal is treating artists fairly, thereby giving them a fair chance to have their music recommended and listened to, and subsequently building a fan base. Such artist fairness is often assumed to have a trade-off with user goals such as satisfaction. Using insights from two studies, this work shows the opposite: some goals from different stakeholders are complementary. Our first study, in which we interview music artists, demonstrates that they often see increased transparency and control for users as a means to also improve artist fairness. We expand with a second study asking other music industry professionals about these topics using a questionnaire. Its results indicate that transparency towards users is highly valued and should be increased.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1TFaGAyaFQK70F5rd7eXXeu-AlrKcsrzc)</b>",
      "authors": [
        "Karlijn Dinnissen (Utrecht University)*",
        " Christine Bauer (Paris Lodron University Salzburg)"
      ],
      "authors_and_affil": [
        "Karlijn Dinnissen (Utrecht University)*",
        " Christine Bauer (Paris Lodron University Salzburg)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTHGFHC",
      "day": "3",
      "keywords": [
        "Human-centered MIR",
        " Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",
        "Human-centered MIR -> human-computer interaction",
        " Human-centered MIR -> music interfaces and services"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000057.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1eL3Z3KjS4tNX58UGKSDJBmNCJVt3aRrr/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-10-dinnissen",
      "title": "How Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems",
      "video": "https://drive.google.com/uc?export=view&id=1TFaGAyaFQK70F5rd7eXXeu-AlrKcsrzc"
    },
    "forum": "159",
    "id": "159",
    "pic_id": "https://drive.google.com/file/d/1DU_DRny34LO8VqB7JAsZVh-2XCzbr4h5/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from traditional music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, notwithstanding the surge in the platform's utilization as a music consumption tool, there is a lack of thorough research on the phenomenon. To investigate its usability and interface satisfaction as a music listening tool, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. It has implications not only for YouTube but also for other streaming services for music.\n",
      "abstract": "In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from traditional music streaming services in that they provide a diverse range of music-related videos as well as soundtracks. However, notwithstanding the surge in the platform's utilization as a music consumption tool, there is a lack of thorough research on the phenomenon. To investigate its usability and interface satisfaction as a music listening tool, we conducted semi-structured interviews with 27 users who listen to music through YouTube more than three times a week. Our qualitative analysis found that YouTube has five main meanings for users as a music streaming service: 1) exploring musical diversity, 2) sharing unique playlists, 3) providing visual satisfaction, 4) facilitating user interaction, and 5) allowing free and easy access. We also propose wireframes of a video streaming service for better audio-visual music listening in two stages: search and listening. By these wireframes, we offer practical solutions to enhance user satisfaction with YouTube for music listening. It has implications not only for YouTube but also for other streaming services for music.\n<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1iJq_hG7isweMKvYvZluLrr9eTSThcwf2)</b>",
      "authors": [
        "Ahyeon Choi (Seoul National University)*",
        " Eunsik Shin (Seoul National University)",
        " Haesun Joung (Seoul National University)",
        " Joongseek Lee (Seoul National University)",
        " Kyogu Lee (Seoul National University)"
      ],
      "authors_and_affil": [
        "Ahyeon Choi (Seoul National University)*",
        " Eunsik Shin (Seoul National University)",
        " Haesun Joung (Seoul National University)",
        " Joongseek Lee (Seoul National University)",
        " Kyogu Lee (Seoul National University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410TLVDJ",
      "day": "3",
      "keywords": [
        "Applications -> music videos, multimodal music systems",
        " MIR fundamentals and methodology -> multimodality",
        " Human-centered MIR -> human-computer interaction",
        " Human-centered MIR -> user behavior analysis and mining, user modeling",
        "Human-centered MIR -> music interfaces and services"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000058.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1oHJYmdzFIa4qVWUCPl3RqvFNDsIZ2UCC/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-11-choi",
      "title": "Towards a New Interface for Music Listening: A User Experience Study on YouTube",
      "video": "https://drive.google.com/uc?export=view&id=1iJq_hG7isweMKvYvZluLrr9eTSThcwf2"
    },
    "forum": "165",
    "id": "165",
    "pic_id": "https://drive.google.com/file/d/1OhjCgBZQFMCRUtI_knEe9dDjBhJwldma/view",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "We present FiloBass: a novel corpus of music scores and annotations which focuses on the important but often overlooked role of the double bass in jazz accompaniment. Inspired by recent works that shed light on the role of the soloist, we offer a collection of 48 manually verified transcriptions of professional jazz bassists, comprising over 50,000 note events, which are based on the backing tracks used in the FiloSax dataset. For each recording we provide audio stems, scores, performance-aligned MIDI and associated metadata for beats, downbeats, chord symbols and markers for musical form.\n\nWe then use FiloBass to enrich our understanding of jazz bass lines, by conducting a corpus-based musical analysis with a contrastive study of existing instructional methods. Together with the original FiloSax dataset, our work represents a significant step toward a fully annotated performance dataset for a jazz quartet setting. By illuminating the critical role of the bass in jazz, this work contributes to a more nuanced and comprehensive understanding of the genre.",
      "abstract": "We present FiloBass: a novel corpus of music scores and annotations which focuses on the important but often overlooked role of the double bass in jazz accompaniment. Inspired by recent works that shed light on the role of the soloist, we offer a collection of 48 manually verified transcriptions of professional jazz bassists, comprising over 50,000 note events, which are based on the backing tracks used in the FiloSax dataset. For each recording we provide audio stems, scores, performance-aligned MIDI and associated metadata for beats, downbeats, chord symbols and markers for musical form.\n\nWe then use FiloBass to enrich our understanding of jazz bass lines, by conducting a corpus-based musical analysis with a contrastive study of existing instructional methods. Together with the original FiloSax dataset, our work represents a significant step toward a fully annotated performance dataset for a jazz quartet setting. By illuminating the critical role of the bass in jazz, this work contributes to a more nuanced and comprehensive understanding of the genre.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1YpN38YW55mYs9E4A7jGfwGCr0vyC-Xug)</b>",
      "authors": [
        "Xavier Riley (C4DM)*",
        " Simon Dixon (Queen Mary University of London)"
      ],
      "authors_and_affil": [
        "Xavier Riley (C4DM)*",
        " Simon Dixon (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGQ7GMN",
      "day": "3",
      "keywords": [
        " MIR tasks -> music transcription and annotation",
        "Computational musicology -> digital musicology",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000059.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1waVg_oTy_OJxKRZ-2X_9ifOnLd2Kk4m9/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-12-riley",
      "title": "FiloBass: A Dataset and Corpus Based Study of Jazz Basslines",
      "video": "https://drive.google.com/uc?export=view&id=1YpN38YW55mYs9E4A7jGfwGCr0vyC-Xug"
    },
    "forum": "236",
    "id": "236",
    "pic_id": "https://drive.google.com/file/d/1WCEw7f4sB5C7c6Sd0xIecmNcGYVW2-an/view",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose four different approaches to quantify similarities of compositional texture in symbolically encoded piano music. A melodic contour or harmonic progression can be shaped into a wide variety of different rhythms, densities, or combinations of layers.\u00a0Instead of describing these textural organizations only locally, using existing formalisms, we question how these parameters may evolve throughout a musical piece, and more specifically how much they change. Hence,\u00a0we define several distance functions to compare texture between two musical bars, based either on textural labels annotated with a dedicated syntax, or on symbolic scores. We propose an evaluation methodology based on textural heterogeneity and contrasts in classical Thema and Variations using the TAVERN dataset. Finally, we illustrate use cases of these tools to analyze long-term structure, and discuss the impact of these results on the understanding of musical texture.",
      "abstract": "In this paper, we propose four different approaches to quantify similarities of compositional texture in symbolically encoded piano music. A melodic contour or harmonic progression can be shaped into a wide variety of different rhythms, densities, or combinations of layers.\u00a0Instead of describing these textural organizations only locally, using existing formalisms, we question how these parameters may evolve throughout a musical piece, and more specifically how much they change. Hence,\u00a0we define several distance functions to compare texture between two musical bars, based either on textural labels annotated with a dedicated syntax, or on symbolic scores. We propose an evaluation methodology based on textural heterogeneity and contrasts in classical Thema and Variations using the TAVERN dataset. Finally, we illustrate use cases of these tools to analyze long-term structure, and discuss the impact of these results on the understanding of musical texture.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1hk8eiCacdB8iR0ZwJu-c5UjEEZ96MJ-w)</b>",
      "authors": [
        "Louis Couturier (MIS, Universit\u00e9 de Picardie Jules Verne)*",
        " Louis Bigo (Universit\u00e9 de Lille)",
        " Florence Leve (Universit\u00e9 de Picardie Jules Verne - Lab. MIS - Algomus)"
      ],
      "authors_and_affil": [
        "Louis Couturier (MIS, Universit\u00e9 de Picardie Jules Verne)*",
        " Louis Bigo (Universit\u00e9 de Lille)",
        " Florence Leve (Universit\u00e9 de Picardie Jules Verne - Lab. MIS - Algomus)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGQABAL",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " Musical features and properties",
        "MIR tasks -> similarity metrics"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000060.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1Ve5XrBWIIW976IRWGxgCD04HwEDX1MXM/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-13-couturier",
      "title": "Comparing Texture in Piano Scores",
      "video": "https://drive.google.com/uc?export=view&id=1hk8eiCacdB8iR0ZwJu-c5UjEEZ96MJ-w"
    },
    "forum": "170",
    "id": "170",
    "pic_id": "https://drive.google.com/file/d/1osooaWw-wYuQdjnUAxqY_R1JsaEi3Av3/view",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "As corpora of digital musical scores continue to grow, the need for research tools capable of manipulating such data efficiently, with an intuitive interface, and support for a diversity of file formats, becomes increasingly pressing. In response, this paper introduces the Digital Musicology Corpus Analysis Toolkit (DiMCAT), a Python library for processing large corpora of digitally encoded musical scores. Equally aimed at music-analytical corpus studies, MIR, and machine-learning research, DiMCAT performs common data transformations and analyses using dataframes. Dataframes reduce the inherent complexity of atomic score contents (e.g., notes), larger score entities (e.g., measures), and abstractions (e.g., chord symbols) into easily manipulable computational structures, whose vectorized operations scale to large quantities of musical material. The design of DiMCAT\u2019s API prioritizes computational speed and ease of use, thus aiming to cater to machine-learning practitioners and musicologists alike.",
      "abstract": "As corpora of digital musical scores continue to grow, the need for research tools capable of manipulating such data efficiently, with an intuitive interface, and support for a diversity of file formats, becomes increasingly pressing. In response, this paper introduces the Digital Musicology Corpus Analysis Toolkit (DiMCAT), a Python library for processing large corpora of digitally encoded musical scores. Equally aimed at music-analytical corpus studies, MIR, and machine-learning research, DiMCAT performs common data transformations and analyses using dataframes. Dataframes reduce the inherent complexity of atomic score contents (e.g., notes), larger score entities (e.g., measures), and abstractions (e.g., chord symbols) into easily manipulable computational structures, whose vectorized operations scale to large quantities of musical material. The design of DiMCAT\u2019s API prioritizes computational speed and ease of use, thus aiming to cater to machine-learning practitioners and musicologists alike.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1h9TxRIy3JM9Xq-zU8aDLzL-SykIampwO)</b>",
      "authors": [
        "Johannes Hentschel (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)*",
        " Andrew McLeod (Fraunhofer IDMT)",
        " Yannis Rammos (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "authors_and_affil": [
        "Johannes Hentschel (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)*",
        " Andrew McLeod (Fraunhofer IDMT)",
        " Yannis Rammos (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6BCKLH",
      "day": "3",
      "keywords": [
        " MIR tasks -> alignment, synchronization, and score following",
        "Computational musicology -> digital musicology",
        " Evaluation, datasets, and reproducibility -> reproducibility",
        " Knowledge-driven approaches to MIR -> representations of music",
        " Knowledge-driven approaches to MIR -> computational music theory and musicology",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000061.pdf",
      "poster_pdf": "https://drive.google.com/file/d/18IuVXr59wiqy3Qou-zlpV0bHhNQdSLzo/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-14-hentschel",
      "title": "Introducing DiMCAT for Processing and Analyzing Notated Music on a Very Large Scale",
      "video": "https://drive.google.com/uc?export=view&id=1h9TxRIy3JM9Xq-zU8aDLzL-SykIampwO"
    },
    "forum": "52",
    "id": "52",
    "pic_id": "https://drive.google.com/file/d/1byrcTFqiyyGpMDUoB_vyZYYNYTZNd2-1/view",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "We propose multiple methods for effectively training a sequence-to-sequence automatic guitar transcription model which uses tokenized music representation as an output. Our proposed method mainly consists of 1) a hybrid CTC-Attention model for sequence-to-sequence automatic guitar transcription that uses tokenized music representation, and 2) two data augmentation methods for training the model. Our proposed model is a generic encoder-decoder Transformer model but adopts multi-task learning with CTC from the encoder to speed up learning alignments between the output tokens and acoustic features. Our proposed data augmentation methods scale up the amount of training data by 1) creating bar overlap when splitting an excerpt to be used for network input, and 2) by utilizing MIDI-only data to synthetically create audio-MIDI pair data. We confirmed that 1) the proposed data augmentation methods were highly effective for training generic Transformer models that generate tokenized outputs, 2) our proposed hybrid CTC-Attention model outperforms conventional methods that transcribe guitar performance with tokens, and 3) the addition of multi-task learning with CTC in our proposed model is especially effective when there is an insufficient amount of training data.",
      "abstract": "We propose multiple methods for effectively training a sequence-to-sequence automatic guitar transcription model which uses tokenized music representation as an output. Our proposed method mainly consists of 1) a hybrid CTC-Attention model for sequence-to-sequence automatic guitar transcription that uses tokenized music representation, and 2) two data augmentation methods for training the model. Our proposed model is a generic encoder-decoder Transformer model but adopts multi-task learning with CTC from the encoder to speed up learning alignments between the output tokens and acoustic features. Our proposed data augmentation methods scale up the amount of training data by 1) creating bar overlap when splitting an excerpt to be used for network input, and 2) by utilizing MIDI-only data to synthetically create audio-MIDI pair data. We confirmed that 1) the proposed data augmentation methods were highly effective for training generic Transformer models that generate tokenized outputs, 2) our proposed hybrid CTC-Attention model outperforms conventional methods that transcribe guitar performance with tokens, and 3) the addition of multi-task learning with CTC in our proposed model is especially effective when there is an insufficient amount of training data.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1H-rOfrOD9_sWFJxM5q8-tM5ybEotVi6_)</b>",
      "authors": [
        "Sehun Kim (Nagoya University)*",
        " Kazuya Takeda (Nagoya University)",
        " Tomoki Toda (Nagoya University)"
      ],
      "authors_and_affil": [
        "Sehun Kim (Nagoya University)*",
        " Kazuya Takeda (Nagoya University)",
        " Tomoki Toda (Nagoya University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410U35QC",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "MIR tasks -> music transcription and annotation",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000062.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1nfV8jdToJLq9AjSHgV9299rCC7OtF3YC/view",
      "session": [
        "4"
      ],
      "slack_channel": "p4-15-kim",
      "title": "Sequence-to-Sequence Network Training Methods for Automatic Guitar Transcription With Tokenized Outputs",
      "video": "https://drive.google.com/uc?export=view&id=1H-rOfrOD9_sWFJxM5q8-tM5ybEotVi6_"
    },
    "forum": "161",
    "id": "161",
    "pic_id": "https://drive.google.com/file/d/1L3Kgc0udyncXQhq8m61wF-U9RYA28pp-/view?usp=share_link",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In this paper, we address the problem of pitch estimation using self-supervised learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset.\n\nWe use a lightweight (< 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its constant-Q transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.\n\nWe evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.",
      "abstract": "In this paper, we address the problem of pitch estimation using self-supervised learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset.\n\nWe use a lightweight (< 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its constant-Q transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.\n\nWe evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with low-resource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1IyGmxASQUh9-Vvokvlyh28TPkuWkhHTL)</b>",
      "authors": [
        "Alain Riou (T\u00e9l\u00e9com Paris, IP Paris, Sony CSL)*",
        " Stefan Lattner (Sony CSL)",
        " Ga\u00ebtan Hadjeres (Sony CSL)",
        " Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)"
      ],
      "authors_and_affil": [
        "Alain Riou (T\u00e9l\u00e9com Paris, IP Paris, Sony CSL)*",
        " Stefan Lattner (Sony CSL)",
        " Ga\u00ebtan Hadjeres (Sony CSL)",
        " Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGQJY00",
      "day": "4",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " MIR tasks -> music transcription and annotation"
      ],
      "long_presentation": "True",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000063.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1E38ECBFAzVt-Z1hvSyPoZb4TIShie6Yy/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-01-riou",
      "title": "PESTO: Pitch Estimation With Self-Supervised Transposition-Equivariant Objective",
      "video": "https://drive.google.com/uc?export=view&id=1IyGmxASQUh9-Vvokvlyh28TPkuWkhHTL"
    },
    "forum": "205",
    "id": "205",
    "pic_id": "https://drive.google.com/file/d/1-cJpDm-wpwO4_qHAzU5GJlZCDrkxYltb/view",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Throughout history, a consistent temporal and spatial gap has persisted between the inception of novel knowledge and technology and their subsequent adoption for extensive practical utilization. The article explores the dynamic interaction and exchange of methodologies between musicology and computational music research. It focuses on an analysis of ten years\u2019 worth of papers from the International Society for Music Information Retrieval (ISMIR) from 2012 to 2021. Over 1000 citations of ISMIR papers were reviewed, and out of these, 51 later works published in musicological venues drew from the findings of 28 ISMIR papers. Final results reveal that most contributions from ISMIR rarely make their way to musicology or humanities. Nevertheless, the paper highlights four examples of successful knowledge transfers between the fields and discusses best practices for collaborations while addressing potential causes for such disparities. In the epilogue, we address the interlaced origins of the problem as stemming from the language of new media, institutional restrictions, and the inability to engage in multidisciplinary communication.",
      "abstract": "Throughout history, a consistent temporal and spatial gap has persisted between the inception of novel knowledge and technology and their subsequent adoption for extensive practical utilization. The article explores the dynamic interaction and exchange of methodologies between musicology and computational music research. It focuses on an analysis of ten years\u2019 worth of papers from the International Society for Music Information Retrieval (ISMIR) from 2012 to 2021. Over 1000 citations of ISMIR papers were reviewed, and out of these, 51 later works published in musicological venues drew from the findings of 28 ISMIR papers. Final results reveal that most contributions from ISMIR rarely make their way to musicology or humanities. Nevertheless, the paper highlights four examples of successful knowledge transfers between the fields and discusses best practices for collaborations while addressing potential causes for such disparities. In the epilogue, we address the interlaced origins of the problem as stemming from the language of new media, institutional restrictions, and the inability to engage in multidisciplinary communication.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1qalUTMUdUqsIFLg8CNj01uDXllrInwEu)</b>",
      "authors": [
        "Vanessa Nina Borsan (Universit\u00e9 de Lille)*",
        " Mathieu Giraud (CNRS, Universit\u00e9 de Lille)",
        " Richard Groult (Universit\u00e9 de Rouen Normandie)"
      ],
      "authors_and_affil": [
        "Vanessa Nina Borsan (Universit\u00e9 de Lille)*",
        " Mathieu Giraud (CNRS, Universit\u00e9 de Lille)",
        " Richard Groult (Universit\u00e9 de Rouen Normandie)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6BM74Z",
      "day": "4",
      "keywords": [
        " Human-centered MIR -> user-centered evaluation",
        "Philosophical and ethical discussions",
        " Human-centered MIR -> human-computer interaction",
        "Computational musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000064.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1LeZsFax_jlsd1IS8-wmjaOhkaYhr9KlC/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-02-borsan",
      "title": "The Games We Play: Exploring the Impact of ISMIR on Musicology",
      "video": "https://drive.google.com/uc?export=view&id=1qalUTMUdUqsIFLg8CNj01uDXllrInwEu"
    },
    "forum": "158",
    "id": "158",
    "pic_id": "https://drive.google.com/file/d/1QBK2dKq6-Sz-UYq1lNEkOfkXlTQuwCQ4/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Supervised music source separation systems using deep learning are trained by minimizing a loss function between pairs of predicted separations and ground-truth isolated sources. However, open datasets comprising isolated sources are few, small, and restricted to a few music styles. At the same time, multi-track datasets with source bleeding are usually found larger in size, and are easier to compile. In this work, we address the task of singing voice separation when the ground-truth signals have bleeding and only the target vocals and the corresponding mixture are available. We train a cold diffusion model on the frequency domain to iteratively transform a mixture into the corresponding vocals with bleeding. Next, we build the final separation masks by clustering spectrogram bins according to their evolution along the transformation steps. We test our approach on a Carnatic music scenario for which solely datasets with bleeding exist, while current research on this repertoire commonly uses source separation models trained solely with Western commercial music. Our evaluation on a Carnatic test set shows that our system improves Spleeter on interference removal and it is competitive in terms of signal distortion. Code is open sourced",
      "abstract": "Supervised music source separation systems using deep learning are trained by minimizing a loss function between pairs of predicted separations and ground-truth isolated sources. However, open datasets comprising isolated sources are few, small, and restricted to a few music styles. At the same time, multi-track datasets with source bleeding are usually found larger in size, and are easier to compile. In this work, we address the task of singing voice separation when the ground-truth signals have bleeding and only the target vocals and the corresponding mixture are available. We train a cold diffusion model on the frequency domain to iteratively transform a mixture into the corresponding vocals with bleeding. Next, we build the final separation masks by clustering spectrogram bins according to their evolution along the transformation steps. We test our approach on a Carnatic music scenario for which solely datasets with bleeding exist, while current research on this repertoire commonly uses source separation models trained solely with Western commercial music. Our evaluation on a Carnatic test set shows that our system improves Spleeter on interference removal and it is competitive in terms of signal distortion. Code is open sourced<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Yqe3wjveVzIkww9Oo4ljGlszdTPvRqkD)</b>",
      "authors": [
        "Gen\u00eds Plaja-Roglans (Music Technology Group)*",
        " Marius Miron (Universitat Pompeu Fabra)",
        " Adithi Shankar (Universitat Pompeu Fabra)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "authors_and_affil": [
        "Gen\u00eds Plaja-Roglans (Music Technology Group)*",
        " Marius Miron (Universitat Pompeu Fabra)",
        " Adithi Shankar (Universitat Pompeu Fabra)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410UC97E",
      "day": "4",
      "keywords": [
        "MIR tasks -> sound source separation",
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR -> computational ethnomusicology",
        " Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000065.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1_ANxPnfRtsoefXwSSdezWmwiY54AQx0P/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-03-plaja-roglans",
      "title": "Carnatic Singing Voice Separation Using Cold Diffusion on Training Data With Bleeding",
      "video": "https://drive.google.com/uc?export=view&id=1Yqe3wjveVzIkww9Oo4ljGlszdTPvRqkD"
    },
    "forum": "176",
    "id": "176",
    "pic_id": "https://drive.google.com/file/d/1g2JQFuTdYhuXCy3WOs9CG90ko3Yt4cj-/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "When a user listens to a song for the first time, what musical factors (e.g., melody, tempo, and lyrics) influence the user's decision to like or dislike the song? An answer to this question would enable researchers to more deeply understand how people interact with music. Thus, in this paper, we report the results of an online survey involving 302 participants to investigate the influence of 10 musical factors. We also evaluate how a user's personal characteristics (i.e., personality traits and musical sophistication) relate to the importance of each factor for the user. Moreover, we propose and evaluate three factor-based functions that would enable more effectively browsing songs on a music streaming service. The user survey results provide several reusable insights, including the following: (1) for most participants, the melody and singing voice are important factors in judging whether they like a song on first listen; (2) personal characteristics do influence the important factors (e.g., participants who have high openness and are sensitive to beat deviations emphasize melody); and (3) the proposed functions each have a certain level of demand because they enable users to easily find music that fits their tastes. We have released part of the survey results as publicly available data so that other researchers can reproduce the results and analyze the data from their own viewpoints.",
      "abstract": "When a user listens to a song for the first time, what musical factors (e.g., melody, tempo, and lyrics) influence the user's decision to like or dislike the song? An answer to this question would enable researchers to more deeply understand how people interact with music. Thus, in this paper, we report the results of an online survey involving 302 participants to investigate the influence of 10 musical factors. We also evaluate how a user's personal characteristics (i.e., personality traits and musical sophistication) relate to the importance of each factor for the user. Moreover, we propose and evaluate three factor-based functions that would enable more effectively browsing songs on a music streaming service. The user survey results provide several reusable insights, including the following: (1) for most participants, the melody and singing voice are important factors in judging whether they like a song on first listen; (2) personal characteristics do influence the important factors (e.g., participants who have high openness and are sensitive to beat deviations emphasize melody); and (3) the proposed functions each have a certain level of demand because they enable users to easily find music that fits their tastes. We have released part of the survey results as publicly available data so that other researchers can reproduce the results and analyze the data from their own viewpoints.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1k3TdguuXpFUM-8BYGn6q_stKRRaIbvua)</b>",
      "authors": [
        "Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masahiro Hamasaki (National Institute of Advanced Industrial Science and"
      ],
      "authors_and_affil": [
        "Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masahiro Hamasaki (National Institute of Advanced Industrial Science and"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK9AMHT",
      "day": "4",
      "keywords": [
        "Human-centered MIR",
        ""
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000066.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1B9kmqixemKtVnFleUFKGTboodh8t2FFr/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-04-tsukuda",
      "title": "Unveiling the Impact of Musical Factors in Judging a Song on First Listen: Insights From a User Survey",
      "video": "https://drive.google.com/uc?export=view&id=1k3TdguuXpFUM-8BYGn6q_stKRRaIbvua"
    },
    "forum": "179",
    "id": "179",
    "pic_id": "https://drive.google.com/file/d/1NvvZYM7IVTorRFkOerJ_SMLAmO7tfeEC/view",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "The historical development of medieval plainchant melodies is an intriguing musicological topic that invites computational approaches to study it at scale. Plainchant melodies can be represented as strings from a limited alphabet, hence making it technically possible to apply bioinformatic tools that are used to study the relationships of biological sequences. We show that using phylogenetic trees to study relationships of plainchant sources is not merely possible, but that it can indeed produce meaningful results. We develop a simple plainchant substitution model for Multiple Sequence Alignment, adapt a Bayesian phylogenetic tree building method, and demonstrate the promise of this approach by validating the resultant phylogenetic tree built from a set of Divine Office sources for the Christmas Vespers against musicological knowledge.",
      "abstract": "The historical development of medieval plainchant melodies is an intriguing musicological topic that invites computational approaches to study it at scale. Plainchant melodies can be represented as strings from a limited alphabet, hence making it technically possible to apply bioinformatic tools that are used to study the relationships of biological sequences. We show that using phylogenetic trees to study relationships of plainchant sources is not merely possible, but that it can indeed produce meaningful results. We develop a simple plainchant substitution model for Multiple Sequence Alignment, adapt a Bayesian phylogenetic tree building method, and demonstrate the promise of this approach by validating the resultant phylogenetic tree built from a set of Divine Office sources for the Christmas Vespers against musicological knowledge.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1A3OyouqQ6G7G0bXYC5zUaX0-67Kc6WNn)</b>",
      "authors": [
        "Jan Haji_, jr. (Charles University)*",
        " Gustavo Ballen (dos Reis research group, School of Biological and Behavioural Sciences, Queen Mary University of London)",
        " Kl\u00e1ra M\u00fchlov\u00e1 (Institute of Musicology, Faculty of Arts, Masaryk University)",
        " Hana Vlhov\u00e1-W\u00f6rne"
      ],
      "authors_and_affil": [
        "Jan Haji_, jr. (Charles University)*",
        " Gustavo Ballen (dos Reis research group, School of Biological and Behavioural Sciences, Queen Mary University of London)",
        " Kl\u00e1ra M\u00fchlov\u00e1 (Institute of Musicology, Faculty of Arts, Masaryk University)",
        " Hana Vlhov\u00e1-W\u00f6rne"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGR54TA",
      "day": "4",
      "keywords": [
        " Knowledge-driven approaches to MIR -> computational music theory and musicology",
        "Computational musicology -> digital musicology",
        " Knowledge-driven approaches to MIR -> computational ethnomusicology",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000067.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1njr07WsSeq2iWcgYfvi2NwhY5aE-RDkt/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-05-haji\u010d",
      "title": "Towards Building a Phylogeny of Gregorian Chant Melodies",
      "video": "https://drive.google.com/uc?export=view&id=1A3OyouqQ6G7G0bXYC5zUaX0-67Kc6WNn"
    },
    "forum": "180",
    "id": "180",
    "pic_id": "https://drive.google.com/file/d/1xmnYaY-xg1BuYEeW22XyKuO_h7mx1IwF/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the increasing model complexity makes both training and inference computationally expensive. In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks. By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students. We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging. Results show that our method significantly improves the results in comparison to the identical model trained without the teacher\u2019s knowledge. This technique can also be combined with classical knowledge distillation approaches to further improve the model\u2019s performance.",
      "abstract": "Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the increasing model complexity makes both training and inference computationally expensive. In this paper, we integrate the ideas of transfer learning and feature-based knowledge distillation and systematically investigate using pre-trained audio embeddings as teachers to guide the training of low-complexity student networks. By regularizing the feature space of the student networks with the pre-trained embeddings, the knowledge in the teacher embeddings can be transferred to the students. We use various pre-trained audio embeddings and test the effectiveness of the method on the tasks of musical instrument classification and music auto-tagging. Results show that our method significantly improves the results in comparison to the identical model trained without the teacher\u2019s knowledge. This technique can also be combined with classical knowledge distillation approaches to further improve the model\u2019s performance.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1hh6L-FIc_3Ptt_MrZHeFD8-uQw88a4tm)</b>",
      "authors": [
        "Yiwei Ding (Georgia Institute of Technology)*",
        " Alexander Lerch (Georgia Institute of Technology)"
      ],
      "authors_and_affil": [
        "Yiwei Ding (Georgia Institute of Technology)*",
        " Alexander Lerch (Georgia Institute of Technology)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK9M545",
      "day": "4",
      "keywords": [
        " Knowledge-driven approaches to MIR -> representations of music",
        " Musical features and properties -> representations of music",
        " MIR tasks -> automatic classification",
        "Knowledge-driven approaches to MIR",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000068.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1jDmXNpEtDnYx13zWdgBWZxFv07--79Xw/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-06-ding",
      "title": "Audio Embeddings as Teachers for Music Classification",
      "video": "https://drive.google.com/uc?export=view&id=1hh6L-FIc_3Ptt_MrZHeFD8-uQw88a4tm"
    },
    "forum": "182",
    "id": "182",
    "pic_id": "https://drive.google.com/file/d/1yHE7wVW21ORQmtobdGp71QvM9lfIrxS6/view",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We present ScorePerformer, an encoder-decoder transformer with hierarchical style encoding heads for controllable rendering of expressive piano music performances. We design a tokenized representation of symbolic score and performance music, the Score Performance Music tuple (SPMuple), and validate a novel way to encode the local performance tempo in a local note time window. Along with the encoding, we extend a transformer encoder with multi-level maximum mean discrepancy variational autoencoder style modeling heads that learn performance style at the global, bar, beat, and onset levels for fine-grained performance control. To offer an interpretation of the learned latent spaces, we introduce performance direction marking classifiers that associate vectors in the latent space with direction markings to guide performance rendering through the model. Evaluation results show the importance of the architectural design choices and demonstrate that ScorePerformer produces diverse and coherent piano performances that follow the control input.",
      "abstract": "We present ScorePerformer, an encoder-decoder transformer with hierarchical style encoding heads for controllable rendering of expressive piano music performances. We design a tokenized representation of symbolic score and performance music, the Score Performance Music tuple (SPMuple), and validate a novel way to encode the local performance tempo in a local note time window. Along with the encoding, we extend a transformer encoder with multi-level maximum mean discrepancy variational autoencoder style modeling heads that learn performance style at the global, bar, beat, and onset levels for fine-grained performance control. To offer an interpretation of the learned latent spaces, we introduce performance direction marking classifiers that associate vectors in the latent space with direction markings to guide performance rendering through the model. Evaluation results show the importance of the architectural design choices and demonstrate that ScorePerformer produces diverse and coherent piano performances that follow the control input.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1vbIybJB4N66A0xQ7yAWzKkquZYAmJtTm)</b>",
      "authors": [
        "Ilya Borovik (Skolkovo Institute of Science and Technology)*",
        " Vladimir Viro (Peachnote)"
      ],
      "authors_and_affil": [
        "Ilya Borovik (Skolkovo Institute of Science and Technology)*",
        " Vladimir Viro (Peachnote)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VK9TH8V",
      "day": "4",
      "keywords": [
        " MIR tasks -> alignment, synchronization, and score following",
        " Musical features and properties -> representations of music",
        "Musical features and properties -> expression and performative aspects of music",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000069.pdf",
      "poster_pdf": "https://drive.google.com/file/d/13j1Zkl2_WFOTKRA-WgzOy_2G3UII5ipa/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-07-borovik",
      "title": "ScorePerformer: Expressive Piano Performance Rendering With Fine-Grained Control",
      "video": "https://drive.google.com/uc?export=view&id=1vbIybJB4N66A0xQ7yAWzKkquZYAmJtTm"
    },
    "forum": "183",
    "id": "183",
    "pic_id": "https://drive.google.com/file/d/1nk1g0AAilKaTQaBbUu3WUkZFyruTUNRl/view",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. \nThis paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. \nThe proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. \nOur results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. \nIn addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn",
      "abstract": "Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. \nThis paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. \nThe proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. \nOur results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. \nIn addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=11FgFY287Aemntt0pXL5tCnLJ-WtRRXVA)</b>",
      "authors": [
        "Emmanouil Karystinaios (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "authors_and_affil": [
        "Emmanouil Karystinaios (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YH110BC",
      "day": "4",
      "keywords": [
        "Musical features and properties -> harmony, chords and tonality",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Knowledge-driven approaches to MIR -> computational music theory and musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000070.pdf",
      "poster_pdf": "https://drive.google.com/file/d/16X_gLqPUC64Oi48kfzaSCe0MErT9aY5h/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-08-karystinaios",
      "title": "Roman Numeral Analysis With Graph Neural Networks: Onset-Wise Predictions From Note-Wise Features",
      "video": "https://drive.google.com/uc?export=view&id=11FgFY287Aemntt0pXL5tCnLJ-WtRRXVA"
    },
    "forum": "89",
    "id": "89",
    "pic_id": "https://drive.google.com/file/d/13jGWJNT1KC01cosmgMgCJ9haPcGJOVUz/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We present a system to assist Subject Matter Experts (SMEs) to curate large online music catalogs. The system detects releases that are incorrectly attributed to an artist discography (misattribution), when the discography of a single artist is incorrectly separated (duplication), and predicts suitable relocations of misattributed releases. We use historical discography corrections to train and evaluate our system's component models. These models combine vector representations of audio with metadata-based features, which outperform models based on audio or metadata alone. We conduct three experiments with SMEs in which our system detects misattribution in artist discographies with precision greater than 77%, duplication with precision greater than 71%, and by combining the approaches, predicts a correct relocation for misattributed releases with precision up to 45%.\nThese results demonstrate the potential of such proactive curation systems in saving valuable human time and effort by directing attention where it is most needed.",
      "abstract": "We present a system to assist Subject Matter Experts (SMEs) to curate large online music catalogs. The system detects releases that are incorrectly attributed to an artist discography (misattribution), when the discography of a single artist is incorrectly separated (duplication), and predicts suitable relocations of misattributed releases. We use historical discography corrections to train and evaluate our system's component models. These models combine vector representations of audio with metadata-based features, which outperform models based on audio or metadata alone. We conduct three experiments with SMEs in which our system detects misattribution in artist discographies with precision greater than 77%, duplication with precision greater than 71%, and by combining the approaches, predicts a correct relocation for misattributed releases with precision up to 45%.\nThese results demonstrate the potential of such proactive curation systems in saving valuable human time and effort by directing attention where it is most needed.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=16_7tMqQrmgDBP_AGtaBTo9DEsTD1XgV0)</b>",
      "authors": [
        "Brian Regan (Spotify)*",
        " Desislava Hristova (Spotify)",
        " Mariano Beguerisse-D\u00edaz (Spotify)"
      ],
      "authors_and_affil": [
        "Brian Regan (Spotify)*",
        " Desislava Hristova (Spotify)",
        " Mariano Beguerisse-D\u00edaz (Spotify)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTKGCAJ",
      "day": "4",
      "keywords": [
        " MIR fundamentals and methodology -> multimodality",
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        "Human-centered MIR -> user-centered evaluation",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000071.pdf",
      "poster_pdf": "https://drive.google.com/file/d/11bJaG1XGVRnokhKwaR47howB-BdQakK_/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-09-regan",
      "title": "Semi-Automated Music Catalog Curation Using Audio and Metadata",
      "video": "https://drive.google.com/uc?export=view&id=16_7tMqQrmgDBP_AGtaBTo9DEsTD1XgV0"
    },
    "forum": "199",
    "id": "199",
    "pic_id": "https://drive.google.com/file/d/1yTPpHfuqjLvRoKFjVoXHZlfHWnzqp_SN/view",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Musical instrument recognition enables applications such as instrument-based music search and audio manipulation, which are highly sought-after processes in everyday music consumption and production. Despite continuous progresses, advances in automatic musical instrument recognition is hindered by the lack of large, diverse and publicly available annotated datasets. As studies have shown, there is potential to scale up music data annotation processes through crowdsourcing. However, it is still unclear the extent to which untrained crowdworkers can effectively detect when a musical instrument is active in an audio excerpt. In this study, we explore the performance of non-experts on online crowdsourcing platforms, to detect temporal activity of instruments on audio extracts of selected genres. We study the factors that can affect their performance, while we also analyse user characteristics that could predict their performance. Our results bring further insights into the general crowd's capabilities to detect instruments.",
      "abstract": "Musical instrument recognition enables applications such as instrument-based music search and audio manipulation, which are highly sought-after processes in everyday music consumption and production. Despite continuous progresses, advances in automatic musical instrument recognition is hindered by the lack of large, diverse and publicly available annotated datasets. As studies have shown, there is potential to scale up music data annotation processes through crowdsourcing. However, it is still unclear the extent to which untrained crowdworkers can effectively detect when a musical instrument is active in an audio excerpt. In this study, we explore the performance of non-experts on online crowdsourcing platforms, to detect temporal activity of instruments on audio extracts of selected genres. We study the factors that can affect their performance, while we also analyse user characteristics that could predict their performance. Our results bring further insights into the general crowd's capabilities to detect instruments.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=19KmwqtzdT_U7Hmx7Is-_WjRsm7qDP7Qv)</b>",
      "authors": [
        "Ioannis Petros Samiotis (Delft University of Technology)*",
        " Alessandro  Bozzon (Delft University of Technology)",
        " Christoph Lofi (TU Delft)"
      ],
      "authors_and_affil": [
        "Ioannis Petros Samiotis (Delft University of Technology)*",
        " Alessandro  Bozzon (Delft University of Technology)",
        " Christoph Lofi (TU Delft)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J1BH2T1",
      "day": "4",
      "keywords": [
        "Human-centered MIR -> human-computer interaction",
        " MIR tasks -> music transcription and annotation",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        " Human-centered MIR -> music interfaces and services",
        "Human-centered MIR",
        " MIR tasks -> sound source separation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000072.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1FzPdsSh7yjCQQ6JPZHd0ZSlK_ue6tfjb/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-10-samiotis",
      "title": "Crowd's Performance on Temporal Activity Detection of Musical Instruments in Polyphonic Music",
      "video": "https://drive.google.com/uc?export=view&id=19KmwqtzdT_U7Hmx7Is-_WjRsm7qDP7Qv"
    },
    "forum": "202",
    "id": "202",
    "pic_id": "https://drive.google.com/file/d/1LwNRWoWsIIYeECYsJx7cE_v1yoswxU8h/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. \nFor each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. \nThis will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. \nTo facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB.\nAlongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results. ",
      "abstract": "In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. \nFor each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. \nThis will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. \nTo facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB.\nAlongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results. <br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1tpXSRd_xGFPNBldII75K1wZsAsoQ068d)</b>",
      "authors": [
        "Igor G. Pereira (Moises.AI)*",
        " Felipe Araujo (Moises.AI)",
        " Filip Korzeniowski (Moises.AI)",
        " Richard Vogl (moises.ai)"
      ],
      "authors_and_affil": [
        "Igor G. Pereira (Moises.AI)*",
        " Felipe Araujo (Moises.AI)",
        " Filip Korzeniowski (Moises.AI)",
        " Richard Vogl (moises.ai)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YH1JYEN",
      "day": "4",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR tasks -> sound source separation",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000073.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1x1nTGSfK6jUcphVYGlgKgXXVZAmsKItX/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-11-pereira",
      "title": "MoisesDB: A Dataset for Source Separation Beyond 4 Stems",
      "video": "https://drive.google.com/uc?export=view&id=1tpXSRd_xGFPNBldII75K1wZsAsoQ068d"
    },
    "forum": "160",
    "id": "160",
    "pic_id": "https://drive.google.com/file/d/1V1bBCq2K2HBY0ULXBjuFENZg4q30Y7oy/view",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Modeling the temporal unfolding of musical events and its interpretation in terms of hierarchical relations is a common theme in music theory, cognition, and composition. To faithfully encode such relations, we need an elegant way to represent both the semantics of prolongation, where a single event is elaborated into multiple events, and process, where the connection from one event to another is elaborated into multiple connections. In existing works, trees are used to capture the former and graphs for the latter. Each such model has the potential to either encode relations between events (e.g., an event being a repetition of another), or relations between processes (e.g., two consecutive steps making up a larger skip), but not both together explicitly. To model meaningful relations between musical events and processes and combine the semantic expressiveness of trees and graphs, we propose a structured representation using  algebraic datatype (ADT) with dependent type. We demonstrate its applications towards encoding functional interpretations of harmonic progressions, and large scale organizations of key regions. This paper offers two contributions. First, we provide a novel unifying hierarchical framework for musical processes and events. Second, we provide a structured data type encoding such interpretations, which could facilitate computational approaches in music theory and generation.",
      "abstract": "Modeling the temporal unfolding of musical events and its interpretation in terms of hierarchical relations is a common theme in music theory, cognition, and composition. To faithfully encode such relations, we need an elegant way to represent both the semantics of prolongation, where a single event is elaborated into multiple events, and process, where the connection from one event to another is elaborated into multiple connections. In existing works, trees are used to capture the former and graphs for the latter. Each such model has the potential to either encode relations between events (e.g., an event being a repetition of another), or relations between processes (e.g., two consecutive steps making up a larger skip), but not both together explicitly. To model meaningful relations between musical events and processes and combine the semantic expressiveness of trees and graphs, we propose a structured representation using  algebraic datatype (ADT) with dependent type. We demonstrate its applications towards encoding functional interpretations of harmonic progressions, and large scale organizations of key regions. This paper offers two contributions. First, we provide a novel unifying hierarchical framework for musical processes and events. Second, we provide a structured data type encoding such interpretations, which could facilitate computational approaches in music theory and generation.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=19qDR-nUFtQUAxJE0km1U2Kzvm-HQvO7o)</b>",
      "authors": [
        "Zeng Ren (EPFL)*",
        " Wulfram Gerstner (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "authors_and_affil": [
        "Zeng Ren (EPFL)*",
        " Wulfram Gerstner (EPFL)",
        " Martin A Rohrmeier (Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE7B1CK",
      "day": "4",
      "keywords": [
        " Musical features and properties -> structure, segmentation, and form",
        " Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " Musical features and properties -> harmony, chords and tonality",
        "Knowledge-driven approaches to MIR -> representations of music",
        "Computational musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000074.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1MnOQegGQp3oL4vayaPQrNvkGMMfmjf7e/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-12-ren",
      "title": "Music as Flow: A Formal Representation of Hierarchical Processes in Music",
      "video": "https://drive.google.com/uc?export=view&id=19qDR-nUFtQUAxJE0km1U2Kzvm-HQvO7o"
    },
    "forum": "206",
    "id": "206",
    "pic_id": "https://drive.google.com/file/d/1eO7LLD33O0ycx-w_RRKBtASn3kshtHz0/view",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Symbolic Music Alignment is the process of matching\nperformed MIDI notes to corresponding score notes. In\nthis paper, we introduce a reinforcement learning (RL)-\nbased online symbolic music alignment technique. The\nRL agent \u2014 an attention-based neural network \u2014 itera-\ntively estimates the current score position from local score\nand performance contexts. For this symbolic alignment\ntask, environment states can be sampled exhaustively and\nthe reward is dense, rendering a formulation as a simpli-\nfied offline RL problem straightforward. We evaluate the\ntrained agent in three ways. First, in its capacity to identify\ncorrect score positions for sampled test contexts; second,\nas the core technique of a complete algorithm for symbolic\nonline note-wise alignment; and finally, as a real-time sym-\nbolic score follower. We further investigate the pitch-based\nscore and performance representations used as the agent\u2019s\ninputs. To this end, we develop a second model, a two-\nstep Dynamic Time Warping (DTW)-based offline align-\nment algorithm leveraging the same input representation.\nThe proposed model outperforms a state-of-the-art refer-\nence model of offline symbolic music alignment.",
      "abstract": "Symbolic Music Alignment is the process of matching\nperformed MIDI notes to corresponding score notes. In\nthis paper, we introduce a reinforcement learning (RL)-\nbased online symbolic music alignment technique. The\nRL agent \u2014 an attention-based neural network \u2014 itera-\ntively estimates the current score position from local score\nand performance contexts. For this symbolic alignment\ntask, environment states can be sampled exhaustively and\nthe reward is dense, rendering a formulation as a simpli-\nfied offline RL problem straightforward. We evaluate the\ntrained agent in three ways. First, in its capacity to identify\ncorrect score positions for sampled test contexts; second,\nas the core technique of a complete algorithm for symbolic\nonline note-wise alignment; and finally, as a real-time sym-\nbolic score follower. We further investigate the pitch-based\nscore and performance representations used as the agent\u2019s\ninputs. To this end, we develop a second model, a two-\nstep Dynamic Time Warping (DTW)-based offline align-\nment algorithm leveraging the same input representation.\nThe proposed model outperforms a state-of-the-art refer-\nence model of offline symbolic music alignment.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1ZV51EZdGgtBusGXIBV4fGjiTy9lexm-I)</b>",
      "authors": [
        "Silvan Peter (JKU)*"
      ],
      "authors_and_affil": [
        "Silvan Peter (JKU)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VKB7H7F",
      "day": "4",
      "keywords": [
        "MIR tasks -> alignment, synchronization, and score following",
        "MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000075.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1g3T20qaG1EU4Py-guad-sUdCr4Tkd14s/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-13-peter",
      "title": "Online Symbolic Music Alignment With Offline Reinforcement Learning",
      "video": "https://drive.google.com/uc?export=view&id=1ZV51EZdGgtBusGXIBV4fGjiTy9lexm-I"
    },
    "forum": "208",
    "id": "208",
    "pic_id": "https://drive.google.com/file/d/1_obDs8CHpqCJJyW1j0WiJ7wNNciCcT_5/view",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Synthesizers are widely used electronic musical instruments. Given an input sound, inferring the underlying synthesizer's parameters to reproduce it is a difficult task known as sound-matching. In this work, we tackle the problem of automatic sound matching, which is otherwise performed manually by professional audio experts. The novelty of our work stems from the introduction of a novel differentiable synthesizer-proxy that enables gradient-based optimization by comparing the input and reproduced audio signals. Additionally, we introduce a novel self-supervised finetuning mechanism that further refines the prediction at inference time. Both contributions lead to state-of-the-art results, outperforming previous methods across various metrics. Our code is available at: https://github.com/inversynth/InverSynth2.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/94855840719)</b>",
      "abstract": "Synthesizers are widely used electronic musical instruments. Given an input sound, inferring the underlying synthesizer's parameters to reproduce it is a difficult task known as sound-matching. In this work, we tackle the problem of automatic sound matching, which is otherwise performed manually by professional audio experts. The novelty of our work stems from the introduction of a novel differentiable synthesizer-proxy that enables gradient-based optimization by comparing the input and reproduced audio signals. Additionally, we introduce a novel self-supervised finetuning mechanism that further refines the prediction at inference time. Both contributions lead to state-of-the-art results, outperforming previous methods across various metrics. Our code is available at: https://github.com/inversynth/InverSynth2.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/94855840719)</b><br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video]()</b>",
      "authors": [
        "Oren Barkan (Microsoft)",
        " Shlomi Shvartzamn (Tel Aviv University )",
        " Noy Uzrad  (Tel Aviv University )",
        " Moshe Laufer  (Tel Aviv University)",
        " Almog Elharar (Tel Aviv University)",
        " Noam Koenigstein (Tel Aviv University)*"
      ],
      "authors_and_affil": [
        "Oren Barkan (Microsoft)",
        " Shlomi Shvartzamn (Tel Aviv University )",
        " Noy Uzrad  (Tel Aviv University )",
        " Moshe Laufer  (Tel Aviv University)",
        " Almog Elharar (Tel Aviv University)",
        " Noam Koenigstein (Tel Aviv University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGT3CHW",
      "day": "4",
      "keywords": [
        "MIR tasks -> music synthesis and transformation",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "False",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000076.pdf",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-14-koenigstein",
      "title": "Inversynth II: Sound Matching via Self-Supervised Synthesizer-Proxy and Inference-Time Finetuning",
      "video": ""
    },
    "forum": "209",
    "id": "209",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Query-by-Humming (QbH) is a task that involves finding the most relevant song based on a hummed or sung fragment. Despite recent successful commercial solutions, implementing QbH systems remains challenging due to the lack of high-quality datasets for training machine learning models. In this paper, we propose a deep learning data collection technique and introduce Covers and Hummings Aligned Dataset (CHAD), a novel dataset that contains 18 hours of short music fragments, paired with time-aligned hummed versions. To expand our dataset, we employ a semi-supervised model training pipeline that leverages the QbH task as a specialized case of cover song identification (CSI) task. Starting with a model trained on the initial dataset, we iteratively collect groups of fragments of cover versions of the same song and retrain the model on the extended data. Using this pipeline, we collect over 308 hours of additional music fragments, paired with time-aligned cover versions. The final model is successfully applied to the QbH task and achieves competitive results on benchmark datasets. Our study shows that the proposed dataset and training pipeline can effectively facilitate the implementation of QbH systems.",
      "abstract": "Query-by-Humming (QbH) is a task that involves finding the most relevant song based on a hummed or sung fragment. Despite recent successful commercial solutions, implementing QbH systems remains challenging due to the lack of high-quality datasets for training machine learning models. In this paper, we propose a deep learning data collection technique and introduce Covers and Hummings Aligned Dataset (CHAD), a novel dataset that contains 18 hours of short music fragments, paired with time-aligned hummed versions. To expand our dataset, we employ a semi-supervised model training pipeline that leverages the QbH task as a specialized case of cover song identification (CSI) task. Starting with a model trained on the initial dataset, we iteratively collect groups of fragments of cover versions of the same song and retrain the model on the extended data. Using this pipeline, we collect over 308 hours of additional music fragments, paired with time-aligned cover versions. The final model is successfully applied to the QbH task and achieves competitive results on benchmark datasets. Our study shows that the proposed dataset and training pipeline can effectively facilitate the implementation of QbH systems.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=10IuEp3DDBqV0F0WM4u3nodR54J9fjajt)</b>",
      "authors": [
        "Amantur Amatov (Higher School of Economics)*",
        " Dmitry Lamanov (Huawei Noah's Ark Lab)",
        " Maksim Titov (Huawei Noah's Ark Lab)",
        " Ivan Vovk (Huawei Noah's Ark Lab)",
        " Ilya Makarov (AI Center, NUST MISiS)",
        " Mikhail Kudinov (Huawei Noah's Ark Lab)"
      ],
      "authors_and_affil": [
        "Amantur Amatov (Higher School of Economics)*",
        " Dmitry Lamanov (Huawei Noah's Ark Lab)",
        " Maksim Titov (Huawei Noah's Ark Lab)",
        " Ivan Vovk (Huawei Noah's Ark Lab)",
        " Ilya Makarov (AI Center, NUST MISiS)",
        " Mikhail Kudinov (Huawei Noah's Ark Lab)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J1CKS6B",
      "day": "4",
      "keywords": [
        " MIR tasks -> fingerprinting",
        " MIR tasks -> indexing and querying",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Applications -> music retrieval systems"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000077.pdf",
      "poster_pdf": "https://drive.google.com/file/d/10HbIvg_hiwfZmJB1HVd-v8MI-yOLmF-2/view",
      "session": [
        "5"
      ],
      "slack_channel": "p5-15-amatov",
      "title": "A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-by-Humming Task",
      "video": "https://drive.google.com/uc?export=view&id=10IuEp3DDBqV0F0WM4u3nodR54J9fjajt"
    },
    "forum": "210",
    "id": "210",
    "pic_id": "https://drive.google.com/file/d/1PEvsaujw2Rjs6gVj3n09ox3I_Puiszh2/view",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/97195833199)</b>",
      "abstract": "In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/97195833199)</b><br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1rPO9dd5FjAqQ76wSk7iYdjBQC7srhTMm)</b>",
      "authors": [
        "Keren Shao (UCSD)*",
        " Ke Chen (University of California San Diego)",
        " Taylor Berg-Kirkpatrick (UCSD)",
        " Shlomo Dubnov (UC San Diego)"
      ],
      "authors_and_affil": [
        "Keren Shao (UCSD)*",
        " Ke Chen (University of California San Diego)",
        " Taylor Berg-Kirkpatrick (UCSD)",
        " Shlomo Dubnov (UC San Diego)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE86HDZ",
      "day": "4",
      "keywords": [
        " MIR tasks -> automatic classification",
        "MIR fundamentals and methodology -> music signal processing",
        "Musical features and properties -> melody and motives"
      ],
      "long_presentation": "False",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000078.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1ngsFC-qCkvoari60sAE3U9e7OGV6taz4/view?usp=share_link",
      "session": [
        "5"
      ],
      "slack_channel": "p5-16-shao",
      "title": "Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction",
      "video": "https://drive.google.com/uc?export=view&id=1rPO9dd5FjAqQ76wSk7iYdjBQC7srhTMm"
    },
    "forum": "212",
    "id": "212",
    "pic_id": "https://drive.google.com/file/d/1V9iJvFXFF38FppM-7nhlYT1e_CSYX8mF/view",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmonic source and IIR filters to simulate the vocal tract, resulting in an interpretable and efficient approach. We show it is competitive with state-of-the-art singing voice vocoders, requiring fewer synthesis parameters and less memory to train, and runs an order of magnitude faster for inference. Additionally, we demonstrate that GOLF can model the phase components of the human voice, which has immense potential for rendering and analysing singing voices in a differentiable manner. Our results highlight the effectiveness of incorporating the physical properties of the human voice mechanism into SVS and underscore the advantages of signal-processing-based approaches, which offer greater interpretability and efficiency in synthesis.",
      "abstract": "This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmonic source and IIR filters to simulate the vocal tract, resulting in an interpretable and efficient approach. We show it is competitive with state-of-the-art singing voice vocoders, requiring fewer synthesis parameters and less memory to train, and runs an order of magnitude faster for inference. Additionally, we demonstrate that GOLF can model the phase components of the human voice, which has immense potential for rendering and analysing singing voices in a differentiable manner. Our results highlight the effectiveness of incorporating the physical properties of the human voice mechanism into SVS and underscore the advantages of signal-processing-based approaches, which offer greater interpretability and efficiency in synthesis.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1i46u1z9bGukUxzCAxTxrIgyix-uKQ0X4)</b>",
      "authors": [
        "Chin-Yun Yu (Queen Mary University of London)*",
        " George Fazekas (QMUL)"
      ],
      "authors_and_affil": [
        "Chin-Yun Yu (Queen Mary University of London)*",
        " George Fazekas (QMUL)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063J1CV40P",
      "day": "4",
      "keywords": [
        "MIR tasks -> music synthesis and transformation",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "True",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000079.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1gZIPuuU2QzKivJaIGpM2Uh2mgBND_ia6/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-01-yu",
      "title": "Singing Voice Synthesis Using Differentiable LPC and Glottal-Flow-Inspired Wavetables",
      "video": "https://drive.google.com/uc?export=view&id=1i46u1z9bGukUxzCAxTxrIgyix-uKQ0X4"
    },
    "forum": "220",
    "id": "220",
    "pic_id": "https://drive.google.com/file/d/1esbWBjCD4bL-1b1R334yin24FFIZZVwo/view",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Automatic harmonic analysis of symbolic music is an important\nand useful task for both composers and listeners.\nThe task consists of two components: recognizing harmony\nlabels and finding their time boundaries. Most of the\nprevious attempts focused on the first component, while\ntime boundaries were rarely modeled explicitly. Lack of\nboundary modeling in the objective function could lead to\nsegmentation errors. In this paper, we introduce a novel\napproach named Harana, to jointly detect the labels and\nboundaries of harmonic regions using neural semi-CRF\n(conditional random field). In contrast to rule-based scores\nused in traditional semi-CRF, a neural score function is\nproposed to incorporate features with more representational\npower. To improve the robustness of the model to\nimperfect harmony profiles, we design an additional score\ncomponent to penalize the match between the candidate\nharmony label and the absent notes in the music. Quantitative\nresults from our experiments demonstrate that the proposed\napproach improves segmentation quality as well as\nframe-level accuracy compared to previous methods.",
      "abstract": "Automatic harmonic analysis of symbolic music is an important\nand useful task for both composers and listeners.\nThe task consists of two components: recognizing harmony\nlabels and finding their time boundaries. Most of the\nprevious attempts focused on the first component, while\ntime boundaries were rarely modeled explicitly. Lack of\nboundary modeling in the objective function could lead to\nsegmentation errors. In this paper, we introduce a novel\napproach named Harana, to jointly detect the labels and\nboundaries of harmonic regions using neural semi-CRF\n(conditional random field). In contrast to rule-based scores\nused in traditional semi-CRF, a neural score function is\nproposed to incorporate features with more representational\npower. To improve the robustness of the model to\nimperfect harmony profiles, we design an additional score\ncomponent to penalize the match between the candidate\nharmony label and the absent notes in the music. Quantitative\nresults from our experiments demonstrate that the proposed\napproach improves segmentation quality as well as\nframe-level accuracy compared to previous methods.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1nmlSKLZSPGNvc7LmNLfNGa5RkDpdZI1G)</b>",
      "authors": [
        "Qiaoyu Yang (University of Rochester)*",
        " Frank Cwitkowitz (University of Rochester)",
        " Zhiyao Duan (Unversity of Rochester)"
      ],
      "authors_and_affil": [
        "Qiaoyu Yang (University of Rochester)*",
        " Frank Cwitkowitz (University of Rochester)",
        " Zhiyao Duan (Unversity of Rochester)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YH2V4US",
      "day": "4",
      "keywords": [
        "Musical features and properties",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Musical features and properties -> harmony, chords and tonality"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000080.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1o-KmlLu-lsRgMxrSIRF-Hf-zaY8299hY/view?usp=sharing",
      "session": [
        "6"
      ],
      "slack_channel": "p6-02-yang",
      "title": "Harmonic Analysis With Neural Semi-CRF",
      "video": "https://drive.google.com/uc?export=view&id=1nmlSKLZSPGNvc7LmNLfNGa5RkDpdZI1G"
    },
    "forum": "264",
    "id": "264",
    "pic_id": "https://drive.google.com/file/d/1w-loLMW-_BmPFrdJcciLHoltDTHCKPVS/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Music Performance Analysis is based on the evaluation of performance parameters such as pitch, dynamics, timbre, tempo and timing. While timbre is the least specific parameter among these and is often only implicitly understood, prominent brass pedagogues have reported that the presence of excessive muscle tension and inefficiency in playing by a musician is reflected in the timbre quality of the sound produced. In this work, we explore the application of machine learning to automatically assess timbre quality in trumpet playing, given both its educational value and connection to performance quality. An extensive dataset consisting of more than 19,000 tones played by 110 trumpet players of different expertise has been collected. A subset of 1,481 tones from this dataset was labeled by eight professional graders on a scale of 1 to 4 based on the perceived efficiency of sound production. Statistical analysis is performed to identify the correlation among the assigned ratings by the expert graders. A Random Forest classifier is trained using the mode of the ratings and its accuracy and variability is assessed with respect to the variability in human graders as a reference. An analysis of the important discriminatory features identifies stability of spectral peaks as a critical factor in trumpet timbre quality.",
      "abstract": "Music Performance Analysis is based on the evaluation of performance parameters such as pitch, dynamics, timbre, tempo and timing. While timbre is the least specific parameter among these and is often only implicitly understood, prominent brass pedagogues have reported that the presence of excessive muscle tension and inefficiency in playing by a musician is reflected in the timbre quality of the sound produced. In this work, we explore the application of machine learning to automatically assess timbre quality in trumpet playing, given both its educational value and connection to performance quality. An extensive dataset consisting of more than 19,000 tones played by 110 trumpet players of different expertise has been collected. A subset of 1,481 tones from this dataset was labeled by eight professional graders on a scale of 1 to 4 based on the perceived efficiency of sound production. Statistical analysis is performed to identify the correlation among the assigned ratings by the expert graders. A Random Forest classifier is trained using the mode of the ratings and its accuracy and variability is assessed with respect to the variability in human graders as a reference. An analysis of the important discriminatory features identifies stability of spectral peaks as a critical factor in trumpet timbre quality.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1wd_U2mEHtz_d_6P8HsDPcxC-voNPH4TJ)</b>",
      "authors": [
        "Ninad Puranik (McGill University )",
        " Alberto Acquilino (McGill University)*",
        " Ichiro Fujinaga (McGill University)",
        " Gary Scavone (McGill University)"
      ],
      "authors_and_affil": [
        "Ninad Puranik (McGill University )",
        " Alberto Acquilino (McGill University)*",
        " Ichiro Fujinaga (McGill University)",
        " Gary Scavone (McGill University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE8K0UB",
      "day": "4",
      "keywords": [
        "Musical features and properties -> timbre, instrumentation, and singing voice",
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics",
        " MIR tasks -> automatic classification",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000081.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1yO2gg3CZlQDiddv2qQVZQLPXz2S9bDSm/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-03-acquilino",
      "title": "A Dataset and Baseline for Automated Assessment of Timbre Quality in Trumpet Sound",
      "video": "https://drive.google.com/uc?export=view&id=1wd_U2mEHtz_d_6P8HsDPcxC-voNPH4TJ"
    },
    "forum": "213",
    "id": "213",
    "pic_id": "https://drive.google.com/file/d/178jwgniWfuX8pqTIDgNAKf1zs3NnbG48/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall similarity between sections or bars to colors. For these mappings, we use dimensionality reduction or clustering to assign similar segments to similar colors and vice versa. To provide a better overview, we further designed simplified music notation representations, including hierarchical and compressed encodings. These overviews allow users to display whole pieces more compactly on a single screen without clutter and to find and navigate to distant segments more quickly. Our preliminary evaluation with guitarists and tablature shows that our design supports users in tasks such as analyzing structure, finding repetitions, and determining the similarity of specific segments to others.",
      "abstract": "We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall similarity between sections or bars to colors. For these mappings, we use dimensionality reduction or clustering to assign similar segments to similar colors and vice versa. To provide a better overview, we further designed simplified music notation representations, including hierarchical and compressed encodings. These overviews allow users to display whole pieces more compactly on a single screen without clutter and to find and navigate to distant segments more quickly. Our preliminary evaluation with guitarists and tablature shows that our design supports users in tasks such as analyzing structure, finding repetitions, and determining the similarity of specific segments to others.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1QqrLFOACnxQebUM1qdQT2AmCTegxTCmz)</b>",
      "authors": [
        "Frank Heyen (VISUS, University of Stuttgart)*",
        " Quynh Quang Ngo (VISUS, University of Stuttgart)",
        " Michael Sedlmair (Uni Stuttgart)"
      ],
      "authors_and_affil": [
        "Frank Heyen (VISUS, University of Stuttgart)*",
        " Quynh Quang Ngo (VISUS, University of Stuttgart)",
        " Michael Sedlmair (Uni Stuttgart)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGU2H08",
      "day": "4",
      "keywords": [
        "Musical features and properties -> structure, segmentation, and form",
        " MIR tasks -> similarity metrics",
        "MIR tasks -> pattern matching and detection"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000082.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1w8dCKsqJcsnqocFs_85czyjyKre-gtkJ/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-04-heyen",
      "title": "Visual Overviews for Sheet Music Structure",
      "video": "https://drive.google.com/uc?export=view&id=1QqrLFOACnxQebUM1qdQT2AmCTegxTCmz"
    },
    "forum": "216",
    "id": "216",
    "pic_id": "https://drive.google.com/file/d/1eP7DTcz1w1CQzdCdZkDkFWNSRLn8Ppil/view",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo deviations. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio - sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations.",
      "abstract": "Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo deviations. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio - sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=15qRSYgL-w431GrhVbmHAjeBfhJE2FK2k)</b>",
      "authors": [
        "Luis Carvalho (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "authors_and_affil": [
        "Luis Carvalho (Johannes Kepler University)*",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YH3DABC",
      "day": "4",
      "keywords": [
        " MIR fundamentals and methodology -> multimodality",
        " MIR tasks -> indexing and querying",
        " Musical features and properties -> representations of music",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Applications -> music retrieval systems"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000083.pdf",
      "poster_pdf": "https://drive.google.com/file/d/16aq0qsq19INHfi84Sin02FuzEjB-Xmtg/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-05-carvalho",
      "title": "Passage Summarization With Recurrent Models for Audio \u2013 Sheet Music Retrieval",
      "video": "https://drive.google.com/uc?export=view&id=15qRSYgL-w431GrhVbmHAjeBfhJE2FK2k"
    },
    "forum": "217",
    "id": "217",
    "pic_id": "https://drive.google.com/file/d/1RAPcV6AWFbK_fcH4Nhtxo_cCjFeFRmuE/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the music information retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing a different encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets---more than 7500 scores with up to 9 difficulty levels---, two being mainly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.3\\% and a mean square error of 1.3. Finally, we provide access to our code, data, and models for transparency and reproducibility.",
      "abstract": "Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the music information retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing a different encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets---more than 7500 scores with up to 9 difficulty levels---, two being mainly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.3\\% and a mean square error of 1.3. Finally, we provide access to our code, data, and models for transparency and reproducibility.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1QBmb6yaCFABAGc5IwYkOjqB7PKkCWBLY)</b>",
      "authors": [
        "Pedro Ramoneda (Universitat Pompeu Fabra)*",
        " Dasaem Jeong (Sogang University)",
        " Jose J. Valero-Mas (Universitat Pompeu Fabra)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "authors_and_affil": [
        "Pedro Ramoneda (Universitat Pompeu Fabra)*",
        " Dasaem Jeong (Sogang University)",
        " Jose J. Valero-Mas (Universitat Pompeu Fabra)",
        " Xavier Serra (Universitat Pompeu Fabra )"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE93MGT",
      "day": "4",
      "keywords": [
        "Applications",
        " Applications -> music training and education",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000084.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1H8IC8Nh5hP1n4RMFKRI_J9sE9-oe50QB/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-06-ramoneda",
      "title": "Predicting Performance Difficulty From Piano Sheet Music Images",
      "video": "https://drive.google.com/uc?export=view&id=1QBmb6yaCFABAGc5IwYkOjqB7PKkCWBLY"
    },
    "forum": "218",
    "id": "218",
    "pic_id": "https://drive.google.com/file/d/1pyIv2gmhW7ESVie5XjZhA9e4lypN4Idc/view",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Music source separation (MSS) faces challenges due to limited availability and potential noise in correctly labeled individual instrument tracks. In this paper, we propose an automated approach for refining mislabeled instrument tracks in a partially noisy-labeled dataset. The proposed self-refining technique with noisy-labeled dataset results in only a 1% accuracy degradation for multi-label instrument recognition compared to a classifier trained with a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data for training MSS models and shows that utilizing the refined dataset for MSS leads to comparable results to a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on self-refined datasets even outperformed those trained on datasets refined with a classifier trained on clean labels.",
      "abstract": "Music source separation (MSS) faces challenges due to limited availability and potential noise in correctly labeled individual instrument tracks. In this paper, we propose an automated approach for refining mislabeled instrument tracks in a partially noisy-labeled dataset. The proposed self-refining technique with noisy-labeled dataset results in only a 1% accuracy degradation for multi-label instrument recognition compared to a classifier trained with a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data for training MSS models and shows that utilizing the refined dataset for MSS leads to comparable results to a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on self-refined datasets even outperformed those trained on datasets refined with a classifier trained on clean labels.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1y4ZGkF2Jlhi-sn4Ci2OnoFTHEm_lshRA)</b>",
      "authors": [
        "Junghyun Koo (Seoul National University)",
        " Yunkee Chae (Seoul National University)*",
        " Chang-Bin Jeon (Seoul National University)",
        " Kyogu Lee (Seoul National University)"
      ],
      "authors_and_affil": [
        "Junghyun Koo (Seoul National University)",
        " Yunkee Chae (Seoul National University)*",
        " Chang-Bin Jeon (Seoul National University)",
        " Kyogu Lee (Seoul National University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YH3NK1Q",
      "day": "4",
      "keywords": [
        "MIR tasks -> automatic classification",
        " MIR tasks -> sound source separation",
        "Evaluation, datasets, and reproducibility -> annotation protocols"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000085.pdf",
      "poster_pdf": "https://drive.google.com/file/d/139i9K8i6jx5hs0D6oPwD9gUUuN4XEzpz/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-07-chae",
      "title": "Self-Refining of Pseudo Labels for Music Source Separation With Noisy Labeled Data",
      "video": "https://drive.google.com/uc?export=view&id=1y4ZGkF2Jlhi-sn4Ci2OnoFTHEm_lshRA"
    },
    "forum": "288",
    "id": "288",
    "pic_id": "https://drive.google.com/file/d/1wz5TKBDBZF5HXfuJiHr_ouDVLW2xQ9RU/view",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Quantifying the difficulty of playing songs has recently gained traction in the MIR community. While previous work has mostly focused on piano, this paper concentrates on rhythm guitar, which is especially popular with amateur musicians and has a broad skill spectrum. This paper proposes a rubric-based \u2018playability\u2019 metric to formalise this spectrum. The rubric comprises seven criteria that contribute to a single playability score, representing the overall difficulty of a song. The rubric was created through interviewing and incorporating feedback from guitar teachers and experts. Additionally, we introduce the playability prediction task by adding annotations to a subset of 200 songs from the McGill Billboard dataset, labelled by a guitar expert using the proposed rubric. We use this dataset to weight each rubric criterion for maximal reliability. Finally, we create a rule-based baseline to score each rubric criterion automatically from chord annotations and timings, and compare this baseline against simple deep learning models trained on chord symbols and textual representations of guitar tablature. The rubric, dataset, and baselines lay a foundation for understanding what makes songs easy or difficult for guitar players and how we can use MIR tools to match amateurs with songs closer to their skill level.",
      "abstract": "Quantifying the difficulty of playing songs has recently gained traction in the MIR community. While previous work has mostly focused on piano, this paper concentrates on rhythm guitar, which is especially popular with amateur musicians and has a broad skill spectrum. This paper proposes a rubric-based \u2018playability\u2019 metric to formalise this spectrum. The rubric comprises seven criteria that contribute to a single playability score, representing the overall difficulty of a song. The rubric was created through interviewing and incorporating feedback from guitar teachers and experts. Additionally, we introduce the playability prediction task by adding annotations to a subset of 200 songs from the McGill Billboard dataset, labelled by a guitar expert using the proposed rubric. We use this dataset to weight each rubric criterion for maximal reliability. Finally, we create a rule-based baseline to score each rubric criterion automatically from chord annotations and timings, and compare this baseline against simple deep learning models trained on chord symbols and textual representations of guitar tablature. The rubric, dataset, and baselines lay a foundation for understanding what makes songs easy or difficult for guitar players and how we can use MIR tools to match amateurs with songs closer to their skill level.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=18wnkJGsvsQoydKJCUe1OFNoaK_fS0W35)</b>",
      "authors": [
        "Marcel A V\u00e9lez V\u00e1squez (University of Amsterdam)*",
        " Mari\u00eblle  Baelemans (University of Amsterdam)",
        " Jonathan Driedger (Chordify)",
        " Willem Zuidema (ILLC, UvA)",
        " John Ashley Burgoyne (University of Amsterdam)"
      ],
      "authors_and_affil": [
        "Marcel A V\u00e9lez V\u00e1squez (University of Amsterdam)*",
        " Mari\u00eblle  Baelemans (University of Amsterdam)",
        " Jonathan Driedger (Chordify)",
        " Willem Zuidema (ILLC, UvA)",
        " John Ashley Burgoyne (University of Amsterdam)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6FLBDF",
      "day": "4",
      "keywords": [
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> annotation protocols",
        " Musical features and properties -> harmony, chords and tonality",
        " Human-centered MIR -> user-centered evaluation",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Applications -> music training and education"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000086.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1sesOh351ItbTXHGgEfFTCQY5U7P4oNg7/view?usp=sharing ",
      "session": [
        "6"
      ],
      "slack_channel": "p6-08-v\u00e1squez",
      "title": "Quantifying the Ease of Playing Song Chords on the Guitar",
      "video": "https://drive.google.com/uc?export=view&id=18wnkJGsvsQoydKJCUe1OFNoaK_fS0W35"
    },
    "forum": "225",
    "id": "225",
    "pic_id": "https://drive.google.com/file/d/1cDyf7XFifNl7ojPkqYTlZhr63H5LuovV/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Alignment algorithms like DTW and subsequence DTW assume specific boundary conditions on where an alignment path can begin and end in the cost matrix.  In practice, the boundary conditions may not be known a priori or may not satisfy such strict assumptions.  This paper introduces an alignment algorithm called FlexDTW that is designed to handle a wide range of boundary conditions.  FlexDTW allows alignment paths to start anywhere on the bottom or left edge of the cost matrix (adjacent to the origin) and to end anywhere on the top or right edge.  In order to properly compare paths of very different lengths, we use a goodness measure that normalizes the cumulative path cost by the path length.  The key insight of FlexDTW is that the Manhattan length of a path can be computed by simply knowing the starting point of the path, which can be computed recursively during dynamic programming.  We artificially generate a suite of 16 benchmarks based on the Chopin Mazurka dataset in order to characterize audio alignment performance under a variety of boundary conditions.  We show that FlexDTW has consistently strong performance that is comparable or better than commonly used alignment algorithms, and it is the only system with strong performance in some boundary conditions.\n",
      "abstract": "Alignment algorithms like DTW and subsequence DTW assume specific boundary conditions on where an alignment path can begin and end in the cost matrix.  In practice, the boundary conditions may not be known a priori or may not satisfy such strict assumptions.  This paper introduces an alignment algorithm called FlexDTW that is designed to handle a wide range of boundary conditions.  FlexDTW allows alignment paths to start anywhere on the bottom or left edge of the cost matrix (adjacent to the origin) and to end anywhere on the top or right edge.  In order to properly compare paths of very different lengths, we use a goodness measure that normalizes the cumulative path cost by the path length.  The key insight of FlexDTW is that the Manhattan length of a path can be computed by simply knowing the starting point of the path, which can be computed recursively during dynamic programming.  We artificially generate a suite of 16 benchmarks based on the Chopin Mazurka dataset in order to characterize audio alignment performance under a variety of boundary conditions.  We show that FlexDTW has consistently strong performance that is comparable or better than commonly used alignment algorithms, and it is the only system with strong performance in some boundary conditions.\n<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1X4iAg6IrkXmFmOODNbJSbq71M-aMlR7H)</b>",
      "authors": [
        "Irmak Bukey (Pomona College)",
        " Jason Zhang (University of Michigan)",
        " Timothy Tsai (Harvey Mudd College)*"
      ],
      "authors_and_affil": [
        "Irmak Bukey (Pomona College)",
        " Jason Zhang (University of Michigan)",
        " Timothy Tsai (Harvey Mudd College)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410YBZT6",
      "day": "4",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        "MIR tasks -> alignment, synchronization, and score following"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000087.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1qNpxCnv167R5lwPIetpbOveI_VK3QKsh/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-09-tsai",
      "title": "FlexDTW: Dynamic Time Warping With Flexible Boundary Conditions",
      "video": "https://drive.google.com/uc?export=view&id=1X4iAg6IrkXmFmOODNbJSbq71M-aMlR7H"
    },
    "forum": "235",
    "id": "235",
    "pic_id": "https://drive.google.com/file/d/1BN51rFgI2RdzQOar5_fjQ-Ll3Rvqnwwt/view",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends. This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard.\n In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 and a limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of\nnon-guitar music into guitar tablatures.",
      "abstract": "Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends. This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard.\n In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 and a limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of\nnon-guitar music into guitar tablatures.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1XXZNSayceHF8YHyjGkUCzeF3CHWeNJIs)</b>",
      "authors": [
        "Alexandre D'Hooge (Universit\u00e9 de Lille)*",
        " Louis Bigo (Universit\u00e9 de Lille)",
        " Ken D\u00e9guernel (CNRS)"
      ],
      "authors_and_affil": [
        "Alexandre D'Hooge (Universit\u00e9 de Lille)*",
        " Louis Bigo (Universit\u00e9 de Lille)",
        " Ken D\u00e9guernel (CNRS)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B7CPKA5",
      "day": "4",
      "keywords": [
        " Musical features and properties -> expression and performative aspects of music",
        "Applications -> music composition, performance, and production",
        "Knowledge-driven approaches to MIR",
        " MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000088.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1K3KRVVWiVIuj59AeMspoHofr2mAQUCFa/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-10-dhooge",
      "title": "Modeling Bends in Popular Music Guitar Tablatures",
      "video": "https://drive.google.com/uc?export=view&id=1XXZNSayceHF8YHyjGkUCzeF3CHWeNJIs"
    },
    "forum": "166",
    "id": "166",
    "pic_id": "https://drive.google.com/file/d/1CRKc3eJrIAxxfuM8dnUPP8uWTQC9XibN/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. \nIn this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. \nFor this we jointly optimize \n- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and \n- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. \nWe also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. \nFinally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.",
      "abstract": "Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. \nIn this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. \nFor this we jointly optimize \n- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and \n- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. \nWe also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. \nFinally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1xqLMnWUj3hT_sjnqvwheaofzbZXjVOzJ)</b>",
      "authors": [
        "Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)*"
      ],
      "authors_and_affil": [
        "Geoffroy Peeters (LTCI - T\u00e9l\u00e9com Paris, IP Paris)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410YG1JQ",
      "day": "4",
      "keywords": [
        "Musical features and properties -> structure, segmentation, and form",
        "MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000089.pdf",
      "poster_pdf": "https://drive.google.com/file/d/121wH3eycz4zZ9mNNgiw8YeOnAsGmPnAe/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-11-peeters",
      "title": "Self-Similarity-Based and Novelty-Based Loss for Music Structure Analysis",
      "video": "https://drive.google.com/uc?export=view&id=1xqLMnWUj3hT_sjnqvwheaofzbZXjVOzJ"
    },
    "forum": "279",
    "id": "279",
    "pic_id": "https://drive.google.com/file/d/1PzC9ayjSkxw8Bjpfo5WumSCw27LCT4Qe/view",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "In jazz, measuring harmonic similarity is complicated by the common practice of reharmonization -- the altering or substitution of chords without fundamentally changing the piece's harmonic identity. This is analogous to natural language processing tasks where synonymous terms can be used interchangeably without significantly modifying the meaning of a text.  Our approach to modeling harmonic similarity borrows from NLP techniques, such as distributional semantics, by embedding chords into a vector space using a co-occurrence matrix.  We show that the method can robustly detect harmonic similarity between songs, even when reharmonized.  The co-occurrence matrix is computed from a corpus of symbolic jazz-chord progressions, and the result is a map from chords into vectors. A song's harmony can then be represented as a piecewise-linear path constructed from the cumulative sum of its chord vectors.  For any two songs, their harmonic similarity can be measured as the minimal surface membrane area between their vector paths.  Using a dataset of jazz contrafacts, we show that our approach reduces the median rank of matches from 318 to 18 compared to a baseline approach using pitch class vectors.",
      "abstract": "In jazz, measuring harmonic similarity is complicated by the common practice of reharmonization -- the altering or substitution of chords without fundamentally changing the piece's harmonic identity. This is analogous to natural language processing tasks where synonymous terms can be used interchangeably without significantly modifying the meaning of a text.  Our approach to modeling harmonic similarity borrows from NLP techniques, such as distributional semantics, by embedding chords into a vector space using a co-occurrence matrix.  We show that the method can robustly detect harmonic similarity between songs, even when reharmonized.  The co-occurrence matrix is computed from a corpus of symbolic jazz-chord progressions, and the result is a map from chords into vectors. A song's harmony can then be represented as a piecewise-linear path constructed from the cumulative sum of its chord vectors.  For any two songs, their harmonic similarity can be measured as the minimal surface membrane area between their vector paths.  Using a dataset of jazz contrafacts, we show that our approach reduces the median rank of matches from 318 to 18 compared to a baseline approach using pitch class vectors.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1lNhHbatsqrN-i2u-FddjD2ziHofHHhCg)</b>",
      "authors": [
        "Carey Bunks (City, University London)*",
        " Simon Dixon (Queen Mary University of London)",
        " Tillman Weyde (City, University of London)",
        " Bruno Di Giorgi (Apple)"
      ],
      "authors_and_affil": [
        "Carey Bunks (City, University London)*",
        " Simon Dixon (Queen Mary University of London)",
        " Tillman Weyde (City, University of London)",
        " Bruno Di Giorgi (Apple)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064MGV379N",
      "day": "4",
      "keywords": [
        " MIR tasks -> similarity metrics",
        " Musical features and properties -> harmony, chords and tonality",
        "MIR fundamentals and methodology -> symbolic music processing",
        "Musical features and properties -> representations of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000090.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1ciE5bkv_nwm_kw_WaV6GLaqgN3gPT1JG/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-12-bunks",
      "title": "Modeling Harmonic Similarity for Jazz Using Co-occurrence Vectors and the Membrane Area",
      "video": "https://drive.google.com/uc?export=view&id=1lNhHbatsqrN-i2u-FddjD2ziHofHHhCg"
    },
    "forum": "239",
    "id": "239",
    "pic_id": "https://drive.google.com/file/d/1DrWsujLUQNd6LlP-WwBq1SMYX-lvV7qd/view",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.",
      "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1zUOje_v-rxU2buZJAE78g0UeOoZu8aex)</b>",
      "authors": [
        "Shuqi Dai (Carnegie Mellon University)*",
        " Siqi Chen (University of South California)",
        " Yuxuan Wu (Carnegie Mellon University)",
        " Roy Huang (Carnegie Mellon University)",
        " Roger B. Dannenberg (School of Computer Science, Carnegie Mellon University)"
      ],
      "authors_and_affil": [
        "Shuqi Dai (Carnegie Mellon University)*",
        " Siqi Chen (University of South California)",
        " Yuxuan Wu (Carnegie Mellon University)",
        " Roy Huang (Carnegie Mellon University)",
        " Roger B. Dannenberg (School of Computer Science, Carnegie Mellon University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YE9TT7D",
      "day": "4",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000091.pdf",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-13-dai",
      "title": "SingStyle111: A Multilingual Singing Dataset With Style Transfer",
      "video": "https://drive.google.com/uc?export=view&id=1zUOje_v-rxU2buZJAE78g0UeOoZu8aex"
    },
    "forum": "25",
    "id": "25",
    "pic_id": "https://drive.google.com/file/d/162X-YscQ5gJDBkLRCsHHhfmXBoxs9c_y/view",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Lyric translation plays a pivotal role in amplifying the global resonance of music, bridging cultural divides, and fostering universal connections. Translating lyrics, unlike conventional translation tasks, requires a delicate balance between singability and semantics. In this paper, we present a computational framework for the quantitative evaluation of singable lyric translation, which seamlessly integrates musical, linguistic, and cultural dimensions of lyrics. Our comprehensive framework consists of four metrics that measure syllable count distance, phoneme repetition similarity, musical structure distance, and semantic similarity. To substantiate the efficacy of our framework, we collected a singable lyrics dataset, which precisely aligns English, Japanese, and Korean lyrics on a line-by-line and section-by-section basis, and conducted a comparative analysis between singable and non-singable lyrics. Our multidisciplinary approach provides insights into the key components that underlie the art of lyric translation and establishes a solid groundwork for the future of computational lyric translation assessment.",
      "abstract": "Lyric translation plays a pivotal role in amplifying the global resonance of music, bridging cultural divides, and fostering universal connections. Translating lyrics, unlike conventional translation tasks, requires a delicate balance between singability and semantics. In this paper, we present a computational framework for the quantitative evaluation of singable lyric translation, which seamlessly integrates musical, linguistic, and cultural dimensions of lyrics. Our comprehensive framework consists of four metrics that measure syllable count distance, phoneme repetition similarity, musical structure distance, and semantic similarity. To substantiate the efficacy of our framework, we collected a singable lyrics dataset, which precisely aligns English, Japanese, and Korean lyrics on a line-by-line and section-by-section basis, and conducted a comparative analysis between singable and non-singable lyrics. Our multidisciplinary approach provides insights into the key components that underlie the art of lyric translation and establishes a solid groundwork for the future of computational lyric translation assessment.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1u2Zp6skZxD3j5rwmlFHi_D1JkQKcDifA)</b>",
      "authors": [
        "Haven Kim (KAIST)*",
        " Kento Watanabe (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Juhan Nam (KAIST)"
      ],
      "authors_and_affil": [
        "Haven Kim (KAIST)*",
        " Kento Watanabe (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Juhan Nam (KAIST)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VKDLWR3",
      "day": "4",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> evaluation metrics",
        " Evaluation, datasets, and reproducibility -> evaluation methodology",
        " Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology -> lyrics and other textual data",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        "Computational musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000092.pdf",
      "poster_pdf": "https://drive.google.com/file/d/13EbuMQL3bXnAMgI9Rjm37m0_u4Npg8VC/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-14-kim",
      "title": "A Computational Evaluation Framework for Singable Lyric Translation",
      "video": "https://drive.google.com/uc?export=view&id=1u2Zp6skZxD3j5rwmlFHi_D1JkQKcDifA"
    },
    "forum": "255",
    "id": "255",
    "pic_id": "https://drive.google.com/file/d/1bsNIzDKIKvA3XDvcrXjhLr8UiYlhuATe/view",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "When people listen to playlists on a music streaming service, they typically listen to each song from start to end in order. However, what if it were possible to use a function to listen to only the choruses of each song in a playlist one after another? In this paper, we call this music listening concept \"chorus-playlist,\" and we investigate its potential impact from various perspectives such as the demand and the objectives for listening to music with chorus-playlist. To this end, we conducted a questionnaire-based online user survey involving 214 participants. Our analysis results suggest reusable insights, including the following: (1) We show a high demand for listening to existing playlists with the chorus-playlist approach. We also reveal preferred options for chorus playback, such as adding crossfade transitions between choruses. (2) People listen to playlists with chorus-playlist for various objectives. For example, when they listen to their own self-made playlists, they want to boost a mood or listen to music in a specific context such as work or driving. (3) There is also a high demand for playlist creation on the premise of continuous listening to only the choruses of the songs in a playlist. The diversities of artists, genres, and moods are more important when creating such a playlist than when creating a usual playlist.",
      "abstract": "When people listen to playlists on a music streaming service, they typically listen to each song from start to end in order. However, what if it were possible to use a function to listen to only the choruses of each song in a playlist one after another? In this paper, we call this music listening concept \"chorus-playlist,\" and we investigate its potential impact from various perspectives such as the demand and the objectives for listening to music with chorus-playlist. To this end, we conducted a questionnaire-based online user survey involving 214 participants. Our analysis results suggest reusable insights, including the following: (1) We show a high demand for listening to existing playlists with the chorus-playlist approach. We also reveal preferred options for chorus playback, such as adding crossfade transitions between choruses. (2) People listen to playlists with chorus-playlist for various objectives. For example, when they listen to their own self-made playlists, they want to boost a mood or listen to music in a specific context such as work or driving. (3) There is also a high demand for playlist creation on the premise of continuous listening to only the choruses of the songs in a playlist. The diversities of artists, genres, and moods are more important when creating such a playlist than when creating a usual playlist.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1Yg9P6EWBZlriwOMFyjsR268Y7X5waAGn)</b>",
      "authors": [
        "Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and T"
      ],
      "authors_and_affil": [
        "Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST))",
        " Masataka Goto (National Institute of Advanced Industrial Science and T"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6GD36V",
      "day": "4",
      "keywords": [
        "Human-centered MIR -> human-computer interaction",
        "Human-centered MIR -> music interfaces and services"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000093.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1D_S3hsdk6Gd_ypaU1QNEIZUYZ5J1F-nn/view",
      "session": [
        "6"
      ],
      "slack_channel": "p6-15-tsukuda",
      "title": "Chorus-Playlist: Exploring the Impact of Listening to Only Choruses in a Playlist",
      "video": "https://drive.google.com/uc?export=view&id=1Yg9P6EWBZlriwOMFyjsR268Y7X5waAGn"
    },
    "forum": "257",
    "id": "257",
    "pic_id": "https://drive.google.com/file/d/1a7HsyxwthWwKit7Ukc0ZFxF2onE1pY_9/view",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Digital musicology research often proceeds by extending and enriching its evidence base as it progresses, rather than starting with a complete corpus of data and metadata, as a consequence of an emergent research need.\n\nIn this paper, we consider a research workflow which assumes an incremental approach to data gathering and annotation. We describe tooling which implements parts of this workflow, developed to support the study of nineteenth-century music arrangements, and evaluate the applicability of our approach through interviews with musicologists and music editors who have used the tools. We conclude by considering extensions of this approach and the wider implications for digital musicology and music information retrieval.",
      "abstract": "Digital musicology research often proceeds by extending and enriching its evidence base as it progresses, rather than starting with a complete corpus of data and metadata, as a consequence of an emergent research need.\n\nIn this paper, we consider a research workflow which assumes an incremental approach to data gathering and annotation. We describe tooling which implements parts of this workflow, developed to support the study of nineteenth-century music arrangements, and evaluate the applicability of our approach through interviews with musicologists and music editors who have used the tools. We conclude by considering extensions of this approach and the wider implications for digital musicology and music information retrieval.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1hhHR2gBnuA_U-ur1la-5g_Gdjigaj-IA)</b>",
      "authors": [
        "David Lewis (University of Oxford eResearch Centre)*",
        " Elisabete Shibata (Beethoven-Haus Bonn)",
        " Andrew Hankinson (RISM Digital)",
        " Johannes Kepper (Paderborn University)",
        " Kevin R Page (University of Oxford)",
        " Lisa Rosendahl (Paderborn University)",
        " Mark Saccom"
      ],
      "authors_and_affil": [
        "David Lewis (University of Oxford eResearch Centre)*",
        " Elisabete Shibata (Beethoven-Haus Bonn)",
        " Andrew Hankinson (RISM Digital)",
        " Johannes Kepper (Paderborn University)",
        " Kevin R Page (University of Oxford)",
        " Lisa Rosendahl (Paderborn University)",
        " Mark Saccom"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTP2S3Y",
      "day": "5",
      "keywords": [
        "Computational musicology -> digital musicology",
        " Evaluation, datasets, and reproducibility -> annotation protocols",
        " MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " Human-centered MIR -> music interfaces and services",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000094.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1SfhuAl2LxICb6Tqh9VHB4MhrkLYy7LXZ/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-01-lewis",
      "title": "Supporting Musicological Investigations With Information Retrieval Tools: An Iterative Approach to Data Collection",
      "video": "https://drive.google.com/uc?export=view&id=1hhHR2gBnuA_U-ur1la-5g_Gdjigaj-IA"
    },
    "forum": "172",
    "id": "172",
    "pic_id": "https://drive.google.com/file/d/1TJAiDOT_z9XkW0iAvJiyOyO6otFfN__i/view",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks.",
      "abstract": "This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1O6IL5DwvLC1w389b5agKpwZG-c8kTz8N)</b>",
      "authors": [
        "Federico Simonetta (Instituto Complutense de Ciencias Musicales)*",
        " Ana Llorens (Universidad Complutense de Madrid)",
        " Mart\u00edn Serrano (Instituto Complutense de Ciencias Musicales)",
        " Eduardo Garc\u00eda-Portugu\u00e9s (Universidad Carlos III de Madrid)",
        " \u00c1lvaro Torrente "
      ],
      "authors_and_affil": [
        "Federico Simonetta (Instituto Complutense de Ciencias Musicales)*",
        " Ana Llorens (Universidad Complutense de Madrid)",
        " Mart\u00edn Serrano (Instituto Complutense de Ciencias Musicales)",
        " Eduardo Garc\u00eda-Portugu\u00e9s (Universidad Carlos III de Madrid)",
        " \u00c1lvaro Torrente "
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6GKVND",
      "day": "5",
      "keywords": [
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties",
        "Computational musicology -> systematic musicology",
        "Computational musicology"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000095.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1pMp8Jvp4vA8rIN2SrAmF8-9sXIpOnGiO/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-02-simonetta",
      "title": "Optimizing Feature Extraction for Symbolic Music",
      "video": "https://drive.google.com/uc?export=view&id=1O6IL5DwvLC1w389b5agKpwZG-c8kTz8N"
    },
    "forum": "96",
    "id": "96",
    "pic_id": "https://drive.google.com/file/d/1opJZYW_qoczWG31bwyX_A-xMhutI0V8g/view",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed \"typical sampling\", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model\u2019s performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.",
      "abstract": "Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed \"typical sampling\", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model\u2019s performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1F_A6d1BGTWa4n-v1osN5z9gROll7r5Lj)</b>",
      "authors": [
        "Mathias Rose Bjare (Johannes Kepler University Linz)*",
        " Stefan Lattner (Sony CSL)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "authors_and_affil": [
        "Mathias Rose Bjare (Johannes Kepler University Linz)*",
        " Stefan Lattner (Sony CSL)",
        " Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6GPRG9",
      "day": "5",
      "keywords": [
        "Applications -> music composition, performance, and production",
        " Musical features and properties -> melody and motives",
        " MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> structure, segmentation, and form",
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000096.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1_ZolJIkB1I7G0S5K_-DsqR7_-9MnWtfa/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-03-bjare",
      "title": "Exploring Sampling Techniques for Generating Melodies With a Transformer Language Model",
      "video": "https://drive.google.com/uc?export=view&id=1F_A6d1BGTWa4n-v1osN5z9gROll7r5Lj"
    },
    "forum": "274",
    "id": "274",
    "pic_id": "https://drive.google.com/file/d/1rbDRvHNLeKUN7NvnFjCaU6HU6b3HRFZz/view",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Every year, several dozen, primarily European, countries, send performers to compete on live television at the Eurovision Song Contest, with the goal of entertaining an international audience of more than 150 million viewers. Each participating country is able to evaluate every other country's performance via a combination of rankings from professional jurors and telephone votes from viewers. Between fan sites and the official Song Contest organisation, a complete historical record of musical performances and country-to-country contest scores is available, back to the very first edition in 1956, and for the most recent contests, there is also information about each individual juror's rankings. In this paper, we introduce MIRoVision, a set of scripts which collates the data from these sources into a single, easy-to-use dataset, and a discrete-choice model to convert the raw contest scores into a stable, interval-scale measure of the quality of Eurovision Song Contest entries across the years. We use this model to simulate contest outcomes from previous editions and compare the results to the implied win probabilities from bookmakers at various online betting markets. We also assess how successful content-based MIR could be at predicting Eurovision outcomes, using state-of-the-art music foundation models. Given its annual recurrence, emphasis on new music and lesser-known artists, and sophisticated voting structure, the Eurovision Song Contest is an outstanding testing ground for MIR algorithms, and we hope that this paper will inspire the community to use the contest as a regular assessment of the strength of modern MIR.",
      "abstract": "Every year, several dozen, primarily European, countries, send performers to compete on live television at the Eurovision Song Contest, with the goal of entertaining an international audience of more than 150 million viewers. Each participating country is able to evaluate every other country's performance via a combination of rankings from professional jurors and telephone votes from viewers. Between fan sites and the official Song Contest organisation, a complete historical record of musical performances and country-to-country contest scores is available, back to the very first edition in 1956, and for the most recent contests, there is also information about each individual juror's rankings. In this paper, we introduce MIRoVision, a set of scripts which collates the data from these sources into a single, easy-to-use dataset, and a discrete-choice model to convert the raw contest scores into a stable, interval-scale measure of the quality of Eurovision Song Contest entries across the years. We use this model to simulate contest outcomes from previous editions and compare the results to the implied win probabilities from bookmakers at various online betting markets. We also assess how successful content-based MIR could be at predicting Eurovision outcomes, using state-of-the-art music foundation models. Given its annual recurrence, emphasis on new music and lesser-known artists, and sophisticated voting structure, the Eurovision Song Contest is an outstanding testing ground for MIR algorithms, and we hope that this paper will inspire the community to use the contest as a regular assessment of the strength of modern MIR.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1SNBc25R-WRiJe2FfQwkaOs4mobBPyBeY)</b>",
      "authors": [
        "John Ashley Burgoyne (University of Amsterdam)*",
        " Janne Spijkervet (University of Amsterdam)",
        " David J Baker (University of Amsterdam)"
      ],
      "authors_and_affil": [
        "John Ashley Burgoyne (University of Amsterdam)*",
        " Janne Spijkervet (University of Amsterdam)",
        " David J Baker (University of Amsterdam)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YEAKF7V",
      "day": "5",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation metrics",
        " Knowledge-driven approaches to MIR -> computational ethnomusicology",
        " MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " Human-centered MIR -> user behavior analysis and mining, user modeling",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> expression and performative aspects of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000097.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1ExJ_3GbHnTVrMHr4k2V5z8MlqoTf7jwH/view?usp=sharing ",
      "session": [
        "7"
      ],
      "slack_channel": "p7-04-burgoyne",
      "title": "Measuring the Eurovision Song Contest: A Living Dataset for Real-World MIR",
      "video": "https://drive.google.com/uc?export=view&id=1SNBc25R-WRiJe2FfQwkaOs4mobBPyBeY"
    },
    "forum": "276",
    "id": "276",
    "pic_id": "https://drive.google.com/file/d/1wmhfQvRplqCxxrW458_2ItbgBSwK97bc/view?usp=sharing ",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In contrast to previous works, we study how specific design decisions affect downstream music tagging tasks instead of focusing on the training task. We assess the impact of initializing the training with different existing weights, using various input audio segment lengths, using learned representations from different blocks and tokens of the transformer for downstream tasks, and applying patchout at inference to speed up feature extraction. We find that 1) initializing the audio training from ImageNet or AudioSet weights and longer input segments are beneficial both for the training and downstream tasks, 2) the best representations for the downstream tasks are located in the middle blocks of the transformer, and 3) using patchout at inference allows faster processing than our convolutional baselines while maintaining superior performance. The resulting models, MAEST, are publicly available and obtain the best performance among open models in music tagging tasks.",
      "abstract": "In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In contrast to previous works, we study how specific design decisions affect downstream music tagging tasks instead of focusing on the training task. We assess the impact of initializing the training with different existing weights, using various input audio segment lengths, using learned representations from different blocks and tokens of the transformer for downstream tasks, and applying patchout at inference to speed up feature extraction. We find that 1) initializing the audio training from ImageNet or AudioSet weights and longer input segments are beneficial both for the training and downstream tasks, 2) the best representations for the downstream tasks are located in the middle blocks of the transformer, and 3) using patchout at inference allows faster processing than our convolutional baselines while maintaining superior performance. The resulting models, MAEST, are publicly available and obtain the best performance among open models in music tagging tasks.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1pYJ_xgEA1kqqYQ9WvqzVPHnRewmz-hzJ)</b>",
      "authors": [
        "Pablo Alonso-Jim\u00e9nez (Universitat Pompeu Fabra)*",
        " Xavier Serra (Universitat Pompeu Fabra )",
        " Dmitry Bogdanov (Universitat Pompeu Fabra)"
      ],
      "authors_and_affil": [
        "Pablo Alonso-Jim\u00e9nez (Universitat Pompeu Fabra)*",
        " Xavier Serra (Universitat Pompeu Fabra )",
        " Dmitry Bogdanov (Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063VKECFV3",
      "day": "5",
      "keywords": [
        " Musical features and properties -> musical affect, emotion and mood",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        " Musical features and properties -> musical style and genre",
        " MIR tasks -> automatic classification",
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        "Musical features and properties -> representations of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000098.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1RSGNN1xzfXuzyjHy1ZIkWmUWF8qOS-Ml/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-05-alonso-jim\u00e9nez",
      "title": "Efficient Supervised Training of Audio Transformers for Music Representation Learning",
      "video": "https://drive.google.com/uc?export=view&id=1pYJ_xgEA1kqqYQ9WvqzVPHnRewmz-hzJ"
    },
    "forum": "248",
    "id": "248",
    "pic_id": "https://drive.google.com/file/d/1OfoD4hN9VyyTdAfWv0mFF8gIZfUteVNU/view",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Deep learning systems have become popular for tackling a variety of music information retrieval tasks. However, these systems often require large amounts of labeled data for supervised training, which can be very costly to obtain. To alleviate this problem, recent papers on learning music audio representations employ alternative training strategies that utilize unannotated data.\nIn this paper, we introduce a novel cross-version approach to audio representation learning that can be used with music datasets containing several versions (performances) of a musical work. Our method exploits the correspondences that exist between two versions of the same musical section.\nWe evaluate our proposed cross-version approach qualitatively and quantitatively on complex orchestral music recordings and show that it can better capture aspects of instrumentation compared to techniques that do not use cross-version information.",
      "abstract": "Deep learning systems have become popular for tackling a variety of music information retrieval tasks. However, these systems often require large amounts of labeled data for supervised training, which can be very costly to obtain. To alleviate this problem, recent papers on learning music audio representations employ alternative training strategies that utilize unannotated data.\nIn this paper, we introduce a novel cross-version approach to audio representation learning that can be used with music datasets containing several versions (performances) of a musical work. Our method exploits the correspondences that exist between two versions of the same musical section.\nWe evaluate our proposed cross-version approach qualitatively and quantitatively on complex orchestral music recordings and show that it can better capture aspects of instrumentation compared to techniques that do not use cross-version information.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1m4gubvlJ3XO25aLLlSbhY3_O7c960PXe)</b>",
      "authors": [
        "Michael Krause (International Audio Laboratories Erlangen)*",
        " Christof Wei\u00df (University of W\u00fcrzburg)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "authors_and_affil": [
        "Michael Krause (International Audio Laboratories Erlangen)*",
        " Christof Wei\u00df (University of W\u00fcrzburg)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTPKUNA",
      "day": "5",
      "keywords": [
        "Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR tasks -> similarity metrics",
        " Musical features and properties -> representations of music"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000099.pdf",
      "poster_pdf": "https://drive.google.com/file/d/116v-81IMZNYlMz551ahs-D7xmnKdCvkn/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-06-krause",
      "title": "A Cross-Version Approach to Audio Representation Learning for Orchestral Music",
      "video": "https://drive.google.com/uc?export=view&id=1m4gubvlJ3XO25aLLlSbhY3_O7c960PXe"
    },
    "forum": "79",
    "id": "79",
    "pic_id": "https://drive.google.com/file/d/17lmKrbUhR_PxD6RwdUpKmqRLcSvWShDt/view",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "This paper proposes a new music source separation (MSS) model based on an architecture with MLP-Mixer that leverages multilayer perceptrons (MLPs). Most of the recent MSS techniques are based on architectures with CNNs, RNNs, and attention-based transformers that take waveforms or complex spectrograms or both as inputs. For the growth of the research field, we believe it is important to study not only the current established methodologies but also diverse perspectives. Therefore, since the MLP-Mixer-based architecture has been reported to perform as well as or better than architectures with CNNs and transformers in the computer vision field despite the MLP's simple computation, we report a way to effectively apply such an architecture to MSS as a reusable insight. In this paper we propose a model called TFC-MLP, which is a variant of the MLP-Mixer architecture that preserves time-frequency positional relationships and mixes time, frequency, and channel dimensions separately, using complex spectrograms as input. The TFC-MLP was evaluated with source-to-distortion ratio (SDR) using the MUSDB18-HQ dataset. Experimental results showed that the proposed model can achieve competitive SDRs when compared with state-of-the-art MSS models.",
      "abstract": "This paper proposes a new music source separation (MSS) model based on an architecture with MLP-Mixer that leverages multilayer perceptrons (MLPs). Most of the recent MSS techniques are based on architectures with CNNs, RNNs, and attention-based transformers that take waveforms or complex spectrograms or both as inputs. For the growth of the research field, we believe it is important to study not only the current established methodologies but also diverse perspectives. Therefore, since the MLP-Mixer-based architecture has been reported to perform as well as or better than architectures with CNNs and transformers in the computer vision field despite the MLP's simple computation, we report a way to effectively apply such an architecture to MSS as a reusable insight. In this paper we propose a model called TFC-MLP, which is a variant of the MLP-Mixer architecture that preserves time-frequency positional relationships and mixes time, frequency, and channel dimensions separately, using complex spectrograms as input. The TFC-MLP was evaluated with source-to-distortion ratio (SDR) using the MUSDB18-HQ dataset. Experimental results showed that the proposed model can achieve competitive SDRs when compared with state-of-the-art MSS models.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1KdU5UBRHHI0q6HregZMPOAcTJy9pw4qQ)</b>",
      "authors": [
        "Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "authors_and_affil": [
        "Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))*",
        " Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C06410ZQ5FE",
      "day": "5",
      "keywords": [
        "MIR tasks -> sound source separation",
        "MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000100.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1TKRGV4b7Y0azKq1EzyvgDm79s6ZHhpay/view?usp=sharing ",
      "session": [
        "7"
      ],
      "slack_channel": "p7-07-nakano",
      "title": "Music Source Separation With MLP Mixing of Time, Frequency, and Channel",
      "video": "https://drive.google.com/uc?export=view&id=1KdU5UBRHHI0q6HregZMPOAcTJy9pw4qQ"
    },
    "forum": "278",
    "id": "278",
    "pic_id": "https://drive.google.com/file/d/1sWxuc6WyVgK8VgS_7qPiMVZR9laUr3HX/view?usp=sharing ",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language-like fashion. However, symbolic music is neither an image nor a sentence intrinsically, and research in the symbolic domain is lacking a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/98209165970)</b>",
      "abstract": "Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language-like fashion. However, symbolic music is neither an image nor a sentence intrinsically, and research in the symbolic domain is lacking a comprehensive overview of the different available representations. In this paper, we investigate matrix (piano roll), sequence, and graph representations and their corresponding neural architectures, in combination with symbolic scores and performances on three piece-level classification tasks. We also introduce a novel graph representation for symbolic performances and explore the capability of graph representations in global classification tasks. Our systematic evaluation shows advantages and limitations of each input representation.<br><br><b>[Poster session Zoom meeting](https://polimi-it.zoom.us/j/98209165970)</b><br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1uMhKpUkVAiNaaUTFoFzAhxOQZBsHnu4Z)</b>",
      "authors": [
        "Huan Zhang (Queen Mary University of London)*",
        " Emmanouil Karystinaios (Johannes Kepler University)",
        " Simon Dixon (Queen Mary University of London)",
        " Gerhard Widmer (Johannes Kepler University)",
        " Carlos Eduardo Cancino-Chac\u00f3n (Johannes Kepler University Linz)"
      ],
      "authors_and_affil": [
        "Huan Zhang (Queen Mary University of London)*",
        " Emmanouil Karystinaios (Johannes Kepler University)",
        " Simon Dixon (Queen Mary University of London)",
        " Gerhard Widmer (Johannes Kepler University)",
        " Carlos Eduardo Cancino-Chac\u00f3n (Johannes Kepler University Linz)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063YEB286P",
      "day": "5",
      "keywords": [
        " Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",
        " Knowledge-driven approaches to MIR -> representations of music",
        " Musical features and properties -> representations of music",
        "MIR fundamentals and methodology -> symbolic music processing",
        " MIR tasks -> automatic classification",
        "Evaluation, datasets, and reproducibility -> evaluation methodology"
      ],
      "long_presentation": "False",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000101.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1_57q-qGsRO9druNfRi3YHndX4LqLll4r/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-08-zhang",
      "title": "Symbolic Music Representations for Classification Tasks: A Systematic Evaluation",
      "video": "https://drive.google.com/uc?export=view&id=1uMhKpUkVAiNaaUTFoFzAhxOQZBsHnu4Z"
    },
    "forum": "54",
    "id": "54",
    "pic_id": "https://drive.google.com/file/d/1Drbq0ENSx89p_s9JtB9LDo7ITkDV8hkT/view",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods \u2013 standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation.",
      "abstract": "The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods \u2013 standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1sWmDSmjUDTcma5qxam4u_qSe1tQZXY6K)</b>",
      "authors": [
        "Valentina Carriero (University of Bologna)",
        " Jacopo de Berardinis (King's College London)",
        " Albert Mero\u00f1o-Pe\u00f1uela (King's College London)",
        " Andrea Poltronieri (University of Bologna)*",
        " Valentina Presutti (University of Bologna)"
      ],
      "authors_and_affil": [
        "Valentina Carriero (University of Bologna)",
        " Jacopo de Berardinis (King's College London)",
        " Albert Mero\u00f1o-Pe\u00f1uela (King's College London)",
        " Andrea Poltronieri (University of Bologna)*",
        " Valentina Presutti (University of Bologna)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C064B6HD32M",
      "day": "5",
      "keywords": [
        "MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " Knowledge-driven approaches to MIR -> representations of music",
        "Applications -> digital libraries and archives"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000102.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1lJSkNd_OzlU73QswUv1Ca93O_UY4BA-c/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-09-poltronieri",
      "title": "The Music Meta Ontology: A Flexible Semantic Model for the Interoperability of Music Metadata",
      "video": "https://drive.google.com/uc?export=view&id=1sWmDSmjUDTcma5qxam4u_qSe1tQZXY6K"
    },
    "forum": "283",
    "id": "283",
    "pic_id": "https://drive.google.com/file/d/1BjDSWYba5EKYc3zxqWTFJdMroDD7OWVm/view ",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Large-scale studies of musical harmony are often hampered by lack of suitably labelled data. It would be highly advantageous if an algorithm were able to autonomously describe chords, scales, etc. in a consistent and musically informative way. In this paper, we revisit tonal interval vectors (TIVs), which reveal certain insights as to the interval and tonal nature of pitch class sets. We then describe the qualities and criteria required to comprehensively and consistently measure displacements between TIVs. Next, we present the Polar Manhattan Displacement (PMD), a compound magnitude and phase measure for describing the displacements between pitch class sets in a tonally-informed manner. We end by providing examples of how PMD can be used in automated harmonic sequence analysis over a complex chord vocabulary.",
      "abstract": "Large-scale studies of musical harmony are often hampered by lack of suitably labelled data. It would be highly advantageous if an algorithm were able to autonomously describe chords, scales, etc. in a consistent and musically informative way. In this paper, we revisit tonal interval vectors (TIVs), which reveal certain insights as to the interval and tonal nature of pitch class sets. We then describe the qualities and criteria required to comprehensively and consistently measure displacements between TIVs. Next, we present the Polar Manhattan Displacement (PMD), a compound magnitude and phase measure for describing the displacements between pitch class sets in a tonally-informed manner. We end by providing examples of how PMD can be used in automated harmonic sequence analysis over a complex chord vocabulary.<br><br> <b><p align=\"center\"> If the video does not load properly please use the [direct link to video](https://drive.google.com/uc?export=view&id=1wRsgtl_jBL20Cm317Yp1879UHKs7-d5b)</b>",
      "authors": [
        "Jeffrey K Miller (Queen Mary University of London)*",
        " Johan Pauwels (Queen Mary University of London)",
        " Mark B Sandler (Queen Mary University of London)"
      ],
      "authors_and_affil": [
        "Jeffrey K Miller (Queen Mary University of London)*",
        " Johan Pauwels (Queen Mary University of London)",
        " Mark B Sandler (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C063RTQ2BPY",
      "day": "5",
      "keywords": [
        " Knowledge-driven approaches to MIR -> representations of music",
        "Knowledge-driven approaches to MIR -> computational music theory and musicology",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties -> harmony, chords and tonality",
        " MIR tasks -> similarity metrics",
        "Computational musicology -> mathematical music theory"
      ],
      "long_presentation": "False",
      "paper_presentation": "In Person",
      "pdf_path": "https://archives.ismir.net/ismir2023/paper/000103.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1BdVT7RIJJIrCPVhUBL5JoFYJHoUjzZWd/view",
      "session": [
        "7"
      ],
      "slack_channel": "p7-10-miller",
      "title": "Polar Manhattan Displacement: Measuring Tonal Distances Between Chords Based on Intervallic Content",
      "video": "https://drive.google.com/uc?export=view&id=1wRsgtl_jBL20Cm317Yp1879UHKs7-d5b"
    },
    "forum": "294",
    "id": "294",
    "pic_id": "https://drive.google.com/file/d/1zJyH-rDZuVL4AdIP8p25WRbLLtizMkEf/view",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  }
]
