uid,title,abstract,primary_author,primary_email,Authors,authors,authors_emails,Status,affiliation,session,paper_link,poster_link,youtube_id,thumbnail_link,channel_name,channel_url,position
312,Virtuoso Strings: A Dataset of String Ensemble Recordings and Onset Annotations for Timing Analysis,"In this paper we present Virtuoso Strings, a dataset for timing analysis and automatic music transcription (AMT) tasks requiring note onset annotations. This dataset takes advantage of real-world recordings in multitrack format and is curated as part of the Augmented Reality Music Ensemble (ARME) project which investigates musician synchronisation and multimodal music analysis. The dataset is comprised of repeated recordings of quartet, trio, duet and solo ensemble performances with different temporal expressions and leadership role assignments, providing new possibilities for developing and evaluating AMT models with respect to diverse musical performance styles. To reduce the cost of the labour-intensive manual annotation, a semi-automatic method was utilised for both annotation and quality control. The presented dataset consists of 746 tracks with a total of 68,728 onsets. Every track includes onset annotations for a single string instrument, enabling the creation of audio files with different combinations of instruments to be used in the AMT evaluation process.",Maciej Tomczak,maciej.tomczak@bcu.ac.uk,Maciej Tomczak (Birmingham City University)*; Min Susan Li (University of Birmingham); Massimiliano Di Luca (University of Birmingham),"Tomczak, Maciej*; Li, Min Susan; Di Luca, Massimiliano",maciej.tomczak@bcu.ac.uk*; m.li.9@bham.ac.uk; M.DiLuca@bham.ac.uk,Accept,Birmingham City University | University of Birmingham,Physical,https://drive.google.com/file/d/1FrjpqyGu4Wl3Oj5WsxA6_1fkmVqRjrEu/preview,,,emoji_music.001.png,,,1
315,Music Scope Pad,"We present Music Scope Pad, an application for efficiently discovering what one wants to view among many unknown music videos (MVs). Current video players only enable the user to view one MV at a time, so to explore what the user wants to view among many unknown MVs, they need to play each MV individually, which requires a large amount of manipulation. Our app features artificial-intelligence processing of video acquired with the device's front camera to detect the natural movements of the user's head and hands while listening to music, enabling the user to explore MVs. Ten MVs are played simultaneously on the iPad screen and through the spatial acoustics of the device. The user can then explore the MVs they want to view by moving their head left or right. The volume of each MV through the spatial acoustics is automatically adjusted so that the MVs closer to the center of the screen are louder. If the user then cups their hands around their ears, as if they were listening carefully, they can hear the MVs directly in front of them on the screen through the spatial acoustics with more emphasis. If the user keeps their focus on one MV for more than three seconds, that MV will be selected and only that video will be played from the beginning. ",Masatoshi Hamanaka,masatoshi.hamanaka@riken.jp,Masatoshi Hamanaka (RIKEN)*,"Hamanaka, Masatoshi*",masatoshi.hamanaka@riken.jp*,Accept,RIKEN,Physical,https://drive.google.com/file/d/1m8FshdzWL7Enl8ASaVDxms__LAlWbypR/preview,,,emoji_music.002.png,,,2
316,The Chordinator: Chord Progression Modeling and Generation using Transformers,"This paper presents a transformer machine-learning model trained with a large dataset of chord sequences. The dataset includes several styles, such as jazz, rock, pop, blues, or music for cinema, among others. We investigated three modeling strategies: 1) We started the tokenization method by treating all different chords as unique elements, which resulted in a vocabulary of 5202 independent chords as tokens. 2) We expressed the chords as a tuple describing root, nature (e.g., major, minor, diminished, major seventh), and extensions (e.g., additions, alterations), which produces a vocabulary of 59 tokens. 3) We extended the second model by complementing the transformer model with chord information containing eight MIDI notes added to the positional embeddings. We analyze sequences generated by comparing them with the training dataset using trigram analysis, which reveals common chord progressions and source duplications. Secondly, we compared the generated sequences from a musical perspective, rating their plausibility concerning the training data. The third strategy reported lower validation loss and more musical consistency in the suggested progressions.",David Cabrera Dalmazzo,dalmazzo@kth.se,"David Cabrera Dalmazzo (KTH)*; Ken Déguernel (Univ. Lille, CNRS, Centrale Lille); Bob L. T.  Sturm (KTH Royal Institute of Technology)","Cabrera Dalmazzo, David*; Déguernel, Ken; Sturm, Bob L. T. ",dalmazzo@kth.se*; ken.deguernel@cnrs.fr; bobs@kth.se,Accept,"Univ. Lille, CNRS, Centrale Lille | KTH Royal Institute of Technology | KTH",Physical,https://drive.google.com/file/d/14xEoj_bGwcHCBlzX0zhZpmWIz9y2v816/preview,,,emoji_music.003.png,,,3
318,Cosine Contours: A Case Study with Melodies from Irish Traditional Dance Music,"Cornelissen et al. (2021) propose representing melodic contour using a linear combination of few cosine functions for a variety of tasks such as visualization, classification and summarization. The authors apply this representation to studying melodies from four traditions (Gregorian chant, German, Chinese and Sioux folk songs) and claim that it provides a common ground for comparing contours across traditions due to its being data independent, i.e., culturally neutral. We study this claim in the domain of Irish Traditional Dance Music and find evidence that challenges its veracity. Specifically, we find that the cosine representation of the melodic contours of this music is not nearly as efficient as it is for the other traditions. We contrast this representation with a principal component analysis of the melodic contours. Our work contributes to a more nuanced understanding of cosine-based representations of melodic contour.",Laura Cros Vila,lcros@kth.se,Laura Cros Vila (KTH)*; Bob L. T.  Sturm (KTH Royal Institute of Technology),"Cros Vila, Laura*; Sturm, Bob L. T. ",lcros@kth.se*; bobs@kth.se,Accept,KTH Royal Institute of Technology | KTH,Physical,https://drive.google.com/file/d/1PBHtPvmTiGQjxySUe0ZDa-4IoWvNANLI/preview,,,emoji_music.004.png,,,4
319,AutoOsu: Audio-Aware Action Generation for Rhythm Games,"Rhythm-based video games challenge players to match their actions with musical cues, turning songs into interactive experiences. The design of the game charts, which dictate the timing and placement of on-screen notes, are manually crafted by players and developers. With AutoOsu, we introduce a CRNN-based model for generating rhythm game charts for a given audio track, conditioned on an intended difficulty level. In previous studies, this task is often divided into two: onset detection, which determines timing points for notes; and action generation, where notes are distributed among a set of available keys. These two sub-tasks are typically handled with two separately trained models, and audio information is only given to the onset detection model. We instead jointly train the two recurrent layers who both receive audio information, which streamlines the training process and helps better utilize musical features.",Sihun Lee,tastyvb@sogang.ac.kr,Sihun Lee (Sogang University)*; Dasaem Jeong (Sogang University),"Lee, Sihun*; Jeong, Dasaem",tastyvb@sogang.ac.kr*; dasaemj@sogang.ac.kr,Accept,Sogang University,Physical,https://drive.google.com/file/d/1zdgVPPRPBEueRfVDUl5yBUWrh_eWAJ3O/preview,,,emoji_music.005.png,,,5
320,Can MusicGen Create Training Data for MIR Tasks?,"We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre-conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.",Helena Cuesta,helena@daaci.com,Nadine Kroher (Time Machine Capital 2 ); Helena Cuesta (DAACI)*; Aggelos Pikrakis (Time Machine Capital 2),"Kroher, Nadine; Cuesta, Helena*; Pikrakis, Aggelos",nadine@tmc2.ai; helena@daaci.com*; aggelos@tmc2.ai,Accept,Time Machine Capital 2 | DAACI | Time Machine Capital 2 ,Physical,https://drive.google.com/file/d/16LUDN42VsaUa1Nhp106Gbagpxk4CduC9/preview,,,emoji_music.006.png,,,6
321,AUTOMATIC PRODUCTION OF ACOUSTIC PIANO TRANSCRIPTION DATA,"A method and apparatus is described for generating and recording acoustic piano performances automatically and at scale.  This technique has generated nearly 500 hours of studio piano recordings, including a complete re-recording of the MAESTRO solo piano dataset. A software utility has been implemented to simultaneously manage MIDI playback on a Yamaha Diskalvier and a synchronized audio capture session. As part of this late-breaking demo, we release the re-recorded studio audio for the MAESTRO Studio dataset and the Python software package piano-capture used for automating data collection. As well as adding to the pool of existing training data for MIR tasks such as transcription and audio-MIDI alignment, this work facilitates the creation of further datasets, for example in other musical styles.",Andrew C Edwards,a.c.edwards@qmul.ac.uk,Andrew C Edwards (QMUL)*; Simon Dixon (Queen Mary University of London); Akira Maezawa (Yamaha Corporation),"Edwards, Andrew C*; Dixon, Simon; Maezawa, Akira",a.c.edwards@qmul.ac.uk*; s.e.dixon@qmul.ac.uk; akira.maezawa@music.yamaha.com,Accept,Yamaha Corporation | Queen Mary University of London | QMUL,Physical,https://drive.google.com/file/d/15_Fnl0FibQ6Jp8XvHCJc8fTbZYl4Btf9/preview,,,emoji_music.007.png,,,7
322,MemoVision: a tool for feature selection and visualization of performance data,"Music performance analysis has traditionally been a peripheral topic in the music information retrieval community. However, it is sometimes included in the goals of music information retrieval studies—from low- and high-level parameter detection and synchronization methods to feature selection and measure transfer. In this contribution, we introduce the MemoVision software, which utilizes retrieval and feature selection methods for comparative music analysis. It is built with JavaScript and Python languages, combining the accessibility of a web-based interface and state-of-the-art feature extraction possibilities. Its features can be used separately or as a visualization tool for music performance case studies.",Matej Istvanek,matej.istvanek@vut.cz,Matej Istvanek (Brno University of Technology)*,"Istvanek, Matej*",matej.istvanek@vut.cz*,Accept,Brno University of Technology,Physical,https://drive.google.com/file/d/10a1fbxBS_fTCGeGQYXX2az_1UQT1pTuf/preview,,,emoji_music.008.png,,,8
323,The Hi-Audio Online Platform for distributed music crowdsourcing database collection,"The present here the recent development of an online platform for musicians, researchers and an open community of  enthusiasts of audio and music with a view to build a public database of music recordings from a wide variety of styles and different cultures. The data generated and collected will primarily be audio data, coming from various sources, including field recordings, existing datasets, and users’ collaboration. The platform aims at gathering a distributed music crowdsourcing database collection where each music piece is built from asynchronous recordings of different tracks at remote sites. The complete tool and databases generated will be openly distributed for research purpose.",Gaël Richard,gael.richard@telecom-paris.fr,"Gaël Richard (Telecom Paris, Institut polytechnique de Paris)*; Jose Manuel Gil Panal (Telecom Paris); Aurélien David (Telecom Paris, Institut polytechnique de Paris)","Richard, Gaël*; Gil Panal, Jose Manuel; David, Aurélien",gael.richard@telecom-paris.fr*; jose.gilpanal@telecom-paris.fr; aurelien.david@telecom-paris.fr,Accept,"Telecom Paris, Institut polytechnique de Paris | Telecom Paris",Physical,https://drive.google.com/file/d/1GgMsbMs1AbakAg4SP47OsszU09hsPQv0/preview,,,emoji_music.009.png,,,9
324,On the Use of Synthesized Datasets and Transformer Adaptors for Musical Instrument Recognition,"This paper investigates training methods for musical instrument recognition (IR). Many studies have tackled IR and improved performance based on limited primary datasets (e.g., the OpenMIC dataset). As such, IR on other datasets, especially small ones, is yet to be studied. We propose to utilize a large synthesized dataset for IR on small datasets with real-world samples. Specifically, we first pre-train a transformer-based model with the Slakh2100 dataset to initialize its weights. We then fine-tune the model by training using the target datasets. We compare our approach with the adaptor approach, widely known as effective in fine-tuning large language models. We also investigate how the IR performance changes when we can access only a limited number of samples from the target dataset. Our experiment shows that the weight initialization performs consistently better than training from scratch and the adaptor approach.",Keitaro Tanaka,phys.keitaro1227@ruri.waseda.jp,Keitaro Tanaka (Waseda University)*; Yin-Jyun Luo (Queen Mary University of London); Kin Wai Cheuk (Singapore University of Technology and Design); Kazuyoshi Yoshii (Kyoto University); Shigeo Morishima (Waseda Research Institute for Science and Engineering); Simon Dixon (Queen Mary University of London),"Tanaka, Keitaro*; Luo, Yin-Jyun; Cheuk, Kin Wai; Yoshii, Kazuyoshi; Morishima, Shigeo; Dixon, Simon",phys.keitaro1227@ruri.waseda.jp*; yin-jyun.luo@qmul.ac.uk; ravencheuk@gmail.com; yoshii@i.kyoto-u.ac.jp; shigeo@waseda.jp; s.e.dixon@qmul.ac.uk,Accept,Waseda University | Kyoto University | Queen Mary University of London | Singapore University of Technology and Design | Waseda Research Institute for Science and Engineering,Physical,https://drive.google.com/file/d/1MBtS7S8wMRgL7aLE_0ySwCaSvaBVO_cF/preview,,,emoji_music.010.png,,,10
325,What can go wrong when conducting beat tracking experiments,"Automated beat tracking is a central task in music information retrieval (MIR) and aims to find time positions that humans would tap a long when listening to music. In a previous contribution, we built upon an existing beat tracking system and evaluated the results on annotated datasets of Western classical music. In our experiments, we obtained poor evaluation results which we first attributed to the complexity of the music. We later discovered that many of the poor results can be traced back to technical issues in the processing pipeline and inaccurate beat annotations. In this contribution, we report on some of these issues and make suggestions regarding visualization and sonification to better understand the data and results.",Ching-Yu Chiu,x2009971@gmail.com,"Ching-Yu Chiu (International Audio Laboratories Erlangen, Germany)*; Meinard Müller (International Audio Laboratories Erlangen)","Chiu, Ching-Yu*; Müller, Meinard",x2009971@gmail.com*; meinard.mueller@audiolabs-erlangen.de,Accept,"International Audio Laboratories Erlangen, Germany | International Audio Laboratories Erlangen",Physical,https://drive.google.com/file/d/16H89T7zTrc8HdLRG1SbHlZ7o6_Tx_IqN/preview,,,emoji_music.011.png,,,11
326,Bridging Audio and Symbolic Piano Data through a Web-Based Annotation Interface,"The improvement of automatic transcription algorithms has led to the accessibility of high-quality symbolic music data aligned with audio, especially for piano music. However, the transcribed MIDI files need extra annotation, such as beat, chord, and structure, to utilize the dataset in other MIR tasks, such as automatic music composition, melody harmonization, and harmonic analysis. Music annotations, especially chord annotations, need high-level music domain knowledge. Although there are a handful of algorithms for chord annotation, the prediction accuracy is limited. Therefore, it needs humans to correct the annotation eventually. We suggest a web-based semi-automated chord annotation tool for piano music to facilitate this validation process conveniently. We integrated the previous piano music transcription, alignment, quantization, and chord annotation research into one system. Given piano audio input, users can modify the predicted chord annotation of converted MIDI. The demo page is available in our website.",Seolhee Lee,seolhee_lee@kaist.ac.kr,Seolhee Lee (KAIST)*; Eunjin Choi (KAIST); Joonhyung Bae (KAIST); Hyerin Kim (Sogang); Eita Nakamura (Kyoto University); Dasaem Jeong (Sogang University); Juhan Nam (KAIST),"Lee, Seolhee*; Choi, Eunjin; Bae, Joonhyung; Kim, Hyerin; Nakamura, Eita; Jeong, Dasaem; Nam, Juhan",seolhee_lee@kaist.ac.kr*; jech@kaist.ac.kr; jh.bae@kaist.ac.kr; kime0225@sogang.ac.kr; eita.nakamura@i.kyoto-u.ac.jp; dasaemj@sogang.ac.kr; juhan.nam@kaist.ac.kr,Accept,Sogang University | Kyoto University | KAIST | Sogang,Physical,https://drive.google.com/file/d/1mGMsClAjyqaM0eExnE0rfVan2vfhG3kK/preview,,,emoji_music.012.png,,,12
327,Towards Differentiable Piano Synthesis Based On Physical Modeling,"We explore the concept of combining physical modeling of the piano with deep learning using methods from differentiable digital signal processing. The core of our proposed approach is a  modal synthesis model for the piano string, which is combined with a linear filter to approximate the acoustic properties of a grand piano. In a preliminary experiment, we train a neural network to estimate an excitation signal for a string in an autoencoder setting and show that the system can match the spectral content of a given target note. Our differentiable piano model could be utilized in a multitude of music processing tasks, including sound matching, signal enhancement, or source separation.",Simon J Schwär,simon.schwaer@audiolabs-erlangen.de,"Hans-Ulrich Berendes (International Audio Laboratories Erlangen); Simon J Schwär (International Audio Laboratories Erlangen)*; Maximilian Schäfer (Digital Communications, Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)); Meinard Müller (International Audio Laboratories Erlangen)","Berendes, Hans-Ulrich; Schwär, Simon J*; Schäfer, Maximilian; Müller, Meinard",uli.berendes@fau.de; simon.schwaer@audiolabs-erlangen.de*; max.schaefer@fau.de; meinard.mueller@audiolabs-erlangen.de,Accept,FAU | International Audio Laboratories Erlangen,Physical,https://drive.google.com/file/d/1YVe1LUEwdjoSbZ3RUQBXCQEUu1mPxeND/preview,,,emoji_music.013.png,,,13
328,DJ StructFreak: Automatic DJ system Built with Music Structure Embeddings,"There have been many attempts to build automatic DJ systems in both industry and academia. However, these often focus solely on track selection and mixing, neglecting the importance of music structure and mix point selection—key elements for determining transition points to the next track. We introduce a web demonstration, DJ StructFreak, which prioritizes mix point selection and music structures. The system identifies the best matching segment for user-selected mix points using music structure embeddings. As a result, it offers a continuous music listening experience and a new level of freedom for users in mix point selection. DJ StructFreak will be accessible via a link provided upon acceptance.",Juhan Nam,juhan.nam@kaist.ac.kr,Taejun Kim (KAIST); Juhan Nam (KAIST)*,"Kim, Taejun; Nam, Juhan*",taejun@kaist.ac.kr; juhan.nam@kaist.ac.kr*,Accept,KAIST,Physical,https://drive.google.com/file/d/1fbwMeFUp2c8lTTsEmsJxhSkp3o5otijk/preview,,,emoji_music.014.png,,,14
329,Generating folk-like music in abc-notation with Masked Language Models,"We present ``abcMLM,'' a masked language model for generating folk-like music that permits a wider range of possibilities compared to autoregressive approaches.  It has similar performance to autoregressive approaches overall,  albeit with a few peculiarities arising from a different training objective.  We demonstrate abcMLM and its different interaction modalities, and discuss its trade-offs in relation to autoregressive modeling.",Luca Casini,casini@kth.se,Luca Casini (KTH Royal Institute of Technology)*; Nicolas Jonason (KTH Royal Institute of Technology ); Bob L. T.  Sturm (KTH Royal Institute of Technology),"Casini, Luca*; Jonason, Nicolas; Sturm, Bob L. T. ",casini@kth.se*; njona@kth.se; bobs@kth.se,Accept,KTH Royal Institute of Technology | KTH Royal Institute of Technology ,Physical,https://drive.google.com/file/d/12pm_LN6s5t85cWbD_iCZTtqk0R3C1NoN/preview,,,emoji_music.015.png,,,15
330,AQUATK: An Audio Quality Assessment Toolkit,"Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs.",Ashvala Vinay,mail@ashvala.net,Ashvala Vinay (Georgia Institute of Technology)*; Alexander Lerch (Georgia Institute of Technology),"Vinay, Ashvala*; Lerch, Alexander",mail@ashvala.net*; alexander.lerch@gatech.edu,Accept,Georgia Institute of Technology,Physical,https://drive.google.com/file/d/1ArXSEip-EwYcaKsvZkm_MYaJAtmTU_Ox/preview,,,emoji_music.016.png,,,16
331,Encoding performance data in MEI with the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT),This paper presents a new method of encoding performance data in MEI using the recently added <extData> element. Performance data was extracted using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT) and encoded as a JSON object within an <extData> element linked to a specific musical note. A set of pop music vocals has been encoded to demonstrate both the range of descriptors that can be encoded in <extData> and how this method can be used for transcriptions in the absence of a fully specified musical score. ,Johanna Devaney,johanna.devaney@brooklyn.cuny.edu,Johanna Devaney (Brooklyn College)*; Cecilia Beauchamp (The Bronx High School of Science),"Devaney, Johanna*; Beauchamp, Cecilia",johanna.devaney@brooklyn.cuny.edu*; cecebeauchamp21@gmail.com,Accept,The Bronx High School of Science | Brooklyn College,Physical,https://drive.google.com/file/d/1Rqn9ueBnKlplJVahEh2y_5zz6RtVxBD_/preview,,,emoji_music.017.png,,,17
332,Retrieval Augmented Generation of Symbolic Music with LLMs,"We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant  examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.",Nicolas Jonason,nicolasjonason@gmail.com,Nicolas Jonason (KTH Royal Institute of Technology)*; Luca Casini (KTH Royal Institute of Technology); Bob L. T.  Sturm (KTH Royal Institute of Technology),"Jonason, Nicolas*; Casini, Luca; Sturm, Bob L. T. ",nicolasjonason@gmail.com*; casini@kth.se; bobs@kth.se,Accept,KTH Royal Institute of Technology,Physical,https://drive.google.com/file/d/1Ch28jptrc8pnYnB4nipgMzsxjPtkpZRz/preview,,,emoji_music.018.png,,,18
334,The Biased Journey of MSD_AUDIO.ZIP,"The equitable distribution of academic data is crucial for ensuring equal research opportunities, and ultimately further progress. Yet, due to the complexity of using the API for audio data that corresponds to the Million Song Dataset along with its misreporting (before 2016) and the discontinuation of this API (after 2016), access to this data has become restricted to those within certain affiliations that are connected peer-to-peer. In this paper, we delve into this issue, drawing insights from the experiences of 22 individuals who either attempted to access the data or played a role in its creation. With this, we hope to initiate more critical dialogue and more thoughtful consideration with regard to access privilege in the MIR community.",Haven Kim,khaven@kaist.ac.kr,"Haven Kim (KAIST)*; Keunwoo Choi (Gaudio Lab, Inc.); Mateusz Modrzejewski (Warsaw University of Technology); Cynthia C. S. Liem (Delft University of Technology)","Kim, Haven*; Choi, Keunwoo; Modrzejewski, Mateusz; Liem, Cynthia C. S.",khaven@kaist.ac.kr*; keunwoo@gaudiolab.com; mateusz.modrzejewski@pw.edu.pl; c.c.s.liem@tudelft.nl,Accept,"Warsaw University of Technology | Gaudio Lab, Inc. | KAIST | Delft University of Technology",Physical,https://drive.google.com/file/d/1sR1AkQMj2P8E551qpZWPlZhkukPCZbhX/preview,,,emoji_music.019.png,,,19
335,FMAK: A DATASET OF KEY AND MODE ANNOTATIONS FOR THE FREE MUSIC ARCHIVE – EXTENDED ABSTRACT,"Despite the importance of musical key detection to computational music understanding, programmatically identifying the tonal center of a musical composition remains a challenging MIR task for modern deep learning systems, resulting in shortcomings in playlist generation and DJ systems. One bottleneck is the lack of availability of reliable tonal center ground truth. Furthermore, deep learning systems that are trained on classical and pop songs exhibit limitations generalizing to other genres. In this paper, we present a new expert-labeled dataset for the evaluation of key detection containing 260 hours (5489 songs) of song-level key and mode annotations, spread across 17 genres. Code for the dataset is made freely available for public use under a Creative Commons license. The dataset’s reusability is enhanced by a bonus script we attach which enables researchers to recreate the dataset as attuned to their individual MIR tasks.",Stella Wong,stellayinwong@gmail.com,Stella Wong (Columbia University)*; Gandalf Hernandez (Columbia University),"Wong, Stella*; Hernandez, Gandalf",stellayinwong@gmail.com*; gandalf.hernandez@gmail.com,Accept,Columbia University,Virtual,https://drive.google.com/file/d/1JGpo5YN93KOsxebPBrD_z4weScslRpE6/preview,,,emoji_music.020.png,,,20
336,Orchestral Texture Classification with Convolution,"We have investigated the classification of different textural elements in orchestral symbolic music data. A simple convolutional neural network (CNN) is utilized to perform the classification task in a track-wise and bar-wise manner. Preliminary results are reported, and different training parameters, including the use of contextual data and the combination of tracks, are also discussed. Code and data are available at: https://github.com/YaHsuanChu/orchestraTextureClassification ",YA-HSUAN CHU,yahsuanchu.ee09@nycu.edu.tw,YA-HSUAN CHU (National Yang-Ming Chiao-Tung University)*; Li Su (Academia Sinica),"CHU, YA-HSUAN*; Su, Li",yahsuanchu.ee09@nycu.edu.tw*; lisu@iis.sinica.edu.tw,Accept,National Yang-Ming Chiao-Tung University | Academia Sinica,Physical,https://drive.google.com/file/d/1J2XFSBWqJubpOiZkdSy-qgt8zmcszu1p/preview,,,emoji_music.021.png,,,21
337,Beat-Aligned Spectrogram-to-Sequence Generation of Rhythm-Game Charts,"In the heart of ""rhythm games"" - games where players must perform actions in sync with a piece of music - are ""charts"", the directives to be given to players. We newly formulate chart generation as a sequence generation problem to train a Transformer using a large dataset. We also introduce tempo-informed preprocessing and training procedures, some of which are suggested to be integral for a successful training. Our model is found to outperform the baselines on a large dataset, and is also found to benefit from pretraining and finetuning.",Jayeon J. Yi,jayeonyi@umich.edu,Jayeon J. Yi (University of Michigan)*; Sungho Lee (Seoul National University); Kyogu Lee (Seoul National University),"Yi, Jayeon J.*; Lee, Sungho; Lee, Kyogu",jayeonyi@umich.edu*; sh-lee@snu.ac.kr; kglee@snu.ac.kr,Accept,University of Michigan | Seoul National University,Virtual,https://drive.google.com/file/d/1wBsPrqQRMOPATngJPbBGyNFGtG0UeWMP/preview,,,emoji_music.022.png,,,22
338,"TOTAL VARIATION IN POPULAR RAP VOCALS FROM 2009-2023: EXTENSION OF THE ANALYSIS BY GEORGIEVA, RIPOLLÉS & MCFEE","Pitch variability in rap vocals is overlooked in favor of the genre’s uniquely dynamic rhythmic properties. We present an analysis of fundamental frequency (F0) variation in rap vocals over the past 14 years, focusing on song examples that represent the state of modern rap music. Our analy- sis aims at identifying meaningful trends over time, and is in turn a continuation of the 2023 analysis by Georgieva, McFee & Ripollés. They found rap to be an outlier with larger F0 variation compared to other genres, but with a declining trend since the genre’s inception. However, they only analyzed data through 2010. Our analysis looks be- yond 2010. We observe again rap’s large vocal F0 varia- tion, but with a decelerated decline over the years.",Kelvin L Walls,kelwalls@gmail.com,Kelvin L Walls (Independant)*; Iran R Roman (NYU); Bea Steers (NYU); Elena Georgieva (NYU),"Walls, Kelvin L*; Roman, Iran R; Steers, Bea; Georgieva, Elena",kelwalls@gmail.com*; roman@nyu.edu; bsteers@nyu.edu; elena@nyu.edu,Accept,NYU | Independant,Physical,https://drive.google.com/file/d/1IpGW0WOOE-Hz_32j2zDXku4-e2ipavoT/preview,,,emoji_music.023.png,,,23
339,TOWARDS AUTOMATED ESTIMATION OF VALUES FROM SONG LYRICS: A DATA COLLECTION PROTOCOL,"This work describes a validated procedure for collecting reference data to estimate personal values from song lyrics. The undertaken work describes our method for sampling songs for lyric annotation, the specific wording and response format of the annotation tool, and estimation of necessary number of annotations per song. We describe two waves of data collection with over 500 respondents each, the first as a method to estimate the necessary number of ratings using 20 songs, and the second to examine the structure of annotations on 360 songs. We show promising similarity in the structure of annotations when compared with the theoretical structure of personal values.",Andrew M. Demetriou,andrew.m.demetriou@gmail.com,Andrew M. Demetriou (Delft University of Technology)*; Jaehun Kim (Pandora / SiriusXM); Sandy Manolios (TU Delft); Cynthia C. S. Liem (Delft University of Technology),"Demetriou, Andrew M.*; Kim, Jaehun; Manolios, Sandy; Liem, Cynthia C. S.",andrew.m.demetriou@gmail.com*; jaehun.kim@siriusxm.com; s.manolios@tudelft.nl; c.c.s.liem@tudelft.nl,Accept,TU Delft | Pandora / SiriusXM | Delft University of Technology,Physical,https://drive.google.com/file/d/1xCIVdJ8bu3WBWNU0rL3NuCGG-JJ7yjvA/preview,,,emoji_music.024.png,,,24
341,Track Role Prediction of Single-Instrumental Sequences,"In the composition process, selecting appropriate single-instrumental music sequences and assigning their track-role is an indispensable task. However, manually determining the track-role for a myriad of music samples can be time-consuming and labor-intensive. This study introduces a deep learning model designed to automatically predict the track-role of single-instrumental music sequences. Our evaluations show a prediction accuracy of 87% in the symbolic domain and 84% in the audio domain. The proposed track-role prediction methods hold promise for future applications in AI music generation and analysis.",ChangHeon Han,datajedi23@hanyang.ac.kr,ChangHeon Han (Hanyang University )*; Suhyun Lee (Hanyang University); Minsam Ko (Hanyang University),"Han, ChangHeon*; Lee, Suhyun; Ko, Minsam",datajedi23@hanyang.ac.kr*; suhyeon00@gmail.com; minsam@hanyang.ac.kr,Accept,Hanyang University  | Hanyang University,Physical,https://drive.google.com/file/d/1EjIDSv-dVRUIOTAq_56eMFo75VDhHKiK/preview,,,emoji_music.025.png,,,25
342,Demo of a smart musical instrument-based real time pattern detection system,,Nishal S Silva,nishal.silva@unitn.it,Nishal S Silva (University of Trento)*; Luca Turchet (University of Trento),"Silva, Nishal S*; Turchet, Luca",nishal.silva@unitn.it*; luca.turchet@unitn.it,Accept,University of Trento,Physical,https://drive.google.com/file/d/1Zth4D96_1gIV4NmR6V9nqHq1UygqDulZ/preview,,,emoji_music.026.png,,,26
343,JamALT: A formatting-aware lyrics transcription benchmark,"In this paper, we discuss the evaluation for the automatic lyric transcription (ALT) task. We argue that existing lyric transcription benchmarks, primarily focusing on word content, may overlook the complex nuances of written lyrics. This leads to potential misalignment between the creative process of musicians and songwriters as well as listeners' experiences. For example, the absence of line breaks can strip the lyrics of their original rhythm, emotional emphasis, and rhyme scheme. To address this issue, we introduce JamALT, a new lyrics transcription benchmark based on the JamendoLyrics dataset. This includes an enhanced version of the data, geared specifically towards ALT evaluation by implementing music industry's transcription and formatting guidelines for lyrics, particularly in terms of punctuation, line breaks, letter case, and non-word vocal sounds. We also propose a suite of evaluation metrics beyond traditional word error rate, which are designed to capture the aforementioned issues. We hope that the proposed benchmark contributes to the ALT task, enabling more precise and reliable assessments of transcription systems and enhancing the user experience in lyrics applications such as subtitle renderings for live captioning or karaoke.",Ondřej Cífka,ondrej@audioshake.ai,Ondřej Cífka (AudioShake)*; Constantinos Dimitriou (AudioShake); Cheng-i Wang (AudioShake); Hendrik Schreiber (AudioShake); Luke Miner (AudioShake); Fabian-Robert Stöter (AudioShake),"Cífka, Ondřej*; Dimitriou, Constantinos; Wang, Cheng-i; Schreiber, Hendrik; Miner, Luke; Stöter, Fabian-Robert",ondrej@audioshake.ai*; constantinos@audioshake.ai; cheng-i@audioshake.ai; hendrik@audioshake.ai; luke@audioshake.ai; fabian@audioshake.ai,Accept,AudioShake,Physical,https://drive.google.com/file/d/1Bc7fG0MrPsnpGiGxW5nXNZVqOVMYwgsq/preview,,,emoji_music.027.png,,,27
344,STraDa: A Singer Traits Dataset,,Yuexuan KONG,ykong@deezer.com,Yuexuan KONG (Deezer)*; Romain Hennequin (Deezer Research); Viet-Anh Tran (Deezer),"KONG, Yuexuan*; Hennequin, Romain; Tran, Viet-Anh",ykong@deezer.com*; rhennequin@deezer.com; vatran@deezer.com,Accept,Deezer | Deezer Research,Physical,https://drive.google.com/file/d/1IGbmPeJlfENKWa9giJemxpmk4eU6r_MH/preview,,,emoji_music.028.png,,,28
345,Interpretable Modular Representation Learning for Full-Band Accompaniment Arrangement,"Style transfer, in the context of music generation, has been primarily enabled through content-style disentanglement. However, a notable limitation of existing disentanglement models is their confinement to short musical clips, typically spanning only a few bars. In this late-breaking demo, we propose and formalize a novel idea of modular style prior modelling to bridge this research gap. Our focus centres on the specific task of accompaniment arrangement, beginning with AccoMontage, a piano arranger that leverages chord-texture disentanglement and a primitive, rule-based style planner to maintain a long-term texture structure. Subsequently, we introduce Q&A-XL, a multi-track orchestrator with a more generic latent style prior model, which characterizes the global structure of orchestration style. From end to end, the complete system is named AccoMontage-3, which is capable of generating full-band accompaniment for whole pieces of music, with cohesive multi-track arrangement and coherent long-term structure.",Jingwei Zhao,jzhao@u.nus.edu,Jingwei Zhao (National University of Singapore)*; Gus Xia (New York University Shanghai); Ye Wang (National University of Singapore),"Zhao, Jingwei*; Xia, Gus; Wang, Ye",jzhao@u.nus.edu*; gxia@nyu.edu; wangye@comp.nus.edu.sg,Accept,National University of Singapore | New York University Shanghai,Physical,https://drive.google.com/file/d/1AsH1nokCxHdh455NdNBC6XPg5AeuKxhj/preview,,,emoji_music.029.png,,,29
346,Improving Embeddings in Harmony Transformer,"Learning the harmonic structure of music is crucial for various music information retrieval (MIR) tasks. Word2vec skip-gram, a well-established technique in natural language processing, has been found to effectively learn harmonic concepts in music. It represents music slices in a vector space, preserving meaningful geometric relationships. These embeddings hold great promise as inputs for MIR tasks, particularly automatic chord recognition (ACR). However, ACR research predominantly focuses on audio data due to the limited availability of well-annotated symbolic music datasets. In this work, we propose an innovative approach utilizing the Harmony Transformer (HT) architecture by Chen and Su. Instead of incorporating input embedding within the network, we leverage skip-gram as an unsupervised embedding technique. Our experiments show that this unsupervised method produces embeddings that adeptly capture harmonic concepts. We also introduce a novel visualization method to assess the fidelity of these embeddings in representing harmonic musical concepts. We perform our experiments on the Lakh MIDI and the BPS-FH dataset.",Maral Ebrahimzadeh,maral.ebrahimzadeh@ovgu.de,Maral Ebrahimzadeh (Otto-von-Guericke-University Magdeburg)*; Valerie Krug (Otto-von-Guericke-University Madgeburg); Sebastian Stober (Otto von Guericke University),"Ebrahimzadeh, Maral*; Krug, Valerie; Stober, Sebastian",maral.ebrahimzadeh@ovgu.de*; valerie.krug@ovgu.de; stober@ovgu.de,Accept,Otto von Guericke University | Otto-von-Guericke-University Magdeburg | Otto-von-Guericke-University Madgeburg,Physical,https://drive.google.com/file/d/1-dAeViSpdzK3TqFylh4EflOL3ep524HA/preview,,,emoji_music.030.png,,,30
347,Singable and Controllable Neural Lyric Translation: a Late-Breaking Showcase,,Longshen Ou,oulongshen@u.nus.edu,Longshen Ou (National University of Singapore)*; Xichu Ma (National University of Singapore); Ye Wang (National University of Singapore),"Ou, Longshen*; Ma, Xichu; Wang, Ye",oulongshen@u.nus.edu*; ma_xichu@u.nus.edu; wangye@comp.nus.edu.sg,Accept,National University of Singapore,Physical,https://drive.google.com/file/d/1dGJ8XTzTxFIPuJDkxJwJ0jdmko5SjSqG/preview,,,emoji_music.031.png,,,31
348,A Noise Augmentation Pipeline for Realistic Query-By-Example Simulation,,Kamil Akesbi,kakesbi@deezer.com,Kamil Akesbi (Deezer Research)*; Benjamin Martin (Deezer); Dorian Desblancs (Deezer),"Akesbi, Kamil*; Martin, Benjamin; Desblancs, Dorian",kakesbi@deezer.com*; bmartin@deezer.com; ddesblancs@deezer.com,Accept,Deezer | Deezer Research,Physical,https://drive.google.com/file/d/1uSa1o0pdPGS9IVLqG8C1GbmWl0uugC2e/preview,,,emoji_music.032.png,,,32
349,LARS: An Open-Source VST3 Plugin for Deep Drums Demixing with Pretrained Models,"LARS is an open-source cross-platform VST3 plugin written in C++ and designed for separating a stereo drums mixture into five individual stems. At the core of LARS lies a novel bank of parallel U-Nets pretrained using a collection of over 1224 hours of isolated drum clips, making LARS the first neural-network-based plugin available for drum source separation. Thanks to a competitive real-time ratio and user-friendly interface, LARS emerges as a versatile tool for music producers and engineers alike. These features makes LARS suitable for tasks such as drum replacement, drum loops decomposition, audio restoration, remixing, and remastering. Furthermore, LARS may find its way into contemporary music production workflows by complementing existing music demixing models.",Alessandro I Mezza,alessandroilic.mezza@polimi.it,Alessandro I Mezza (Politecnico di Milano )*; Riccardo di Palma (Politecnico di Milano); Edoardo Morena (Politecnico di Milano); Alessandro Orsatti (Politecnico di Milano); Riccardo Giampiccolo (Politecnico di Milano); Alberto Bernardini (Politecnico di Milano); Augusto Sarti (Politecnico di Milano),"Mezza, Alessandro I*; di Palma, Riccardo; Morena, Edoardo; Orsatti, Alessandro; Giampiccolo, Riccardo; Bernardini, Alberto; Sarti, Augusto",alessandroilic.mezza@polimi.it*; riccardo.dipalma@mail.polimi.it; edoardo.morena@mail.polimi.it; alessandro.orsatti@mail.polimi.it; riccardo.giampiccolo@polimi.it; alberto.bernardini@polimi.it; augusto.sarti@polimi.it,Accept,Politecnico di Milano | Politecnico di Milano ,Physical,https://drive.google.com/file/d/1mvutl-trgN06__8uloCCeq8nVmdgCJl-/preview,,,emoji_music.033.png,,,33
350,Music Visualization using Chironomie,"The purpose of this study is to debilitate the enjoyment of music for both hearing-impaired and normal-hearing individuals by visually representing music. In order to effectively and distinctively portray the musical rhythm, we focus on Chironomie, a conducting technique used in Gregorian chant. Generally, Chironomie is drawn by a curve that corresponds to the musical score, and this curve is determined by whether a short segment of the score represents one of two classes: Arsis or Thesis. In pursuit of our objective, our endeavors encompass two essential facets: the adaptation of Chironomie for Western tonal music to express intuitively perceivable musical features like tension and relaxation, and the evaluation whether Chironomie can effectively convey music visually. We report an automated method for estimating Arsis and Thesis within composite beats to generate Chironomie. Additionally, it presents evaluation experiments involving normal-hearing to assess the effectiveness of Chironomie.",Kana Tatsumi,tatsumi@lee-lab.org,Kana Tatsumi (Nagoya Institute of Technology)*; Shinji Sako (Nagoya Institute of Technology); Rafael Ramirez (Universitat Pompeu Fabra ),"Tatsumi, Kana*; Sako, Shinji; Ramirez, Rafael",tatsumi@lee-lab.org*; s.sako@nitech.ac.jp; Rafael.ramirez@upf.edu,Accept,Universitat Pompeu Fabra  | Nagoya Institute of Technology,Physical,https://drive.google.com/file/d/1d81e1FNuA4KQ101tmfRZgv_uzcntvnu7/preview,,,emoji_music.034.png,,,34
351,"AUTOMATIC TRANSCRIPTION OF MULTI-INSTRUMENTAL SONGS: INTEGRATING DEMIXING, HARMONIC DILATED CONVOLUTION, AND JOINT BEAT TRACKING","In the rapidly expanding field of music information retrieval (MIR), automatic transcription remains one of the most sought-after capabilities, especially for songs that employ multiple instruments. Musscribe emerges as a state-of-the-art transcription tool that addresses this challenge by integrating three distinct methodologies: demixing, harmonic dilated convolution, and joint beat tracking. Demixing is employed to isolate individual instruments within a song by separating overlapping audio sources, thus ensuring each instrument is transcribed distinctly. Beat tracking is then run as a parallel process to extract the joint beat and downbeat estimations. These processes results in an output midi file, which is then quantized using information derived from the beat tracking. As such, this method paves the way for more accurate and sophisticated analyses, bridging the gap between human and machine understanding of music. Together, these methodologies allow us to produce transcriptions that are not only accurate but also highly representative of the original compositions. Preliminary tests and evaluations showcase the potential in transcribing complex musical pieces with high fidelity, outperforming many contemporary tools in the market. This innovative approach not only has implications for music transcription but also for broader applications in audio analysis, remixing, and digital music production. The model has been instrumental in accelerating the composition process for several Norwegian television shows. Moreover, its efficacy can be observed in the Netflix series ""A Storm for Christmas."" Renowned composer Peter Baden harnessed this tool to enhance his workflow, proving the demand for innovative tools like this in the professional music industry. ",Lars L Monstad,lars.monstad@gmail.com,Lars L Monstad (University of Oslo)*; Olivier Lartillot (University of Oslo),"Monstad, Lars L*; Lartillot, Olivier",lars.monstad@gmail.com*; o.s.g.lartillot@imv.uio.no,Accept,University of Oslo,Physical,https://drive.google.com/file/d/1BtsicT43bP7wFyHIyYIXGV1iK0wbNG0l/preview,,,emoji_music.035.png,,,35
353,Mel-Band RoFormer for Music Source Separation,"Recently, multi-band spectrogram-based approaches such as Band-Split RNN (BSRNN) have demonstrated promising results for music source separation. In our recent work, we introduce the BS-RoFormer model which inherits the idea of band-split scheme in BSRNN at the front-end, and then uses the hierarchical Transformer with Rotary Position Embedding (RoPE) to model the inner-band and inter-band sequences for multi-band mask estimation. This model has achieved state-of-the-art performance, but the band-split scheme is defined empirically, without analytic supports from the literature. In this paper, we propose Mel-RoFormer, which adopts the Mel-band scheme that maps the frequency bins into overlapped subbands according to the mel scale. In contract, the band-split mapping in BSRNN and BS-RoFormer is non-overlapping and designed based on heuristics. Using the MUSDB18HQ dataset for experiments, we demonstrate that Mel-RoFormer outperforms BS-RoFormer in the separation tasks of vocals, drums, and other stems.",Ju-Chiang Wang,ju-chiang.wang@bytedance.com,Ju-Chiang Wang (ByteDance)*; Wei-Tsung Lu (TikTok); Minz Won (ByteDance),"Wang, Ju-Chiang*; Lu, Wei-Tsung; Won, Minz",ju-chiang.wang@bytedance.com*; weitsung.lu@bytedance.com; minzwon@bytedance.com,Accept,TikTok | ByteDance,Physical,https://drive.google.com/file/d/1SeS_P8YnbwbwLM3uWFeSHLMwNcZJ_avA/preview,,,emoji_music.036.png,,,36
354,THE VocalNotes DATASET,"The VocalNotes dataset is a collection of audio and annotations for excerpts of vocal performances from five musical traditions - Japanese Minyo, Chinese Hebei Bangzi opera, Russian traditional singing, Alpine yodel and Jewish Romaniote chant. For each tradition the dataset contains: about 10 minutes of audio; documentation for the songs from which annotated fragments originate; f0, independent onset, offset and note pitch annotations created by two or three experts; The dataset was created as part of the VocalNotes project [1]. It is released under CC-BY-NC-SA license and can be accessed by filling out a request form.",Polina Proutskova,proutskova@googlemail.com,Polina Proutskova (Queen Mary University of London)*; John M McBride (Institute for Basic Science); Yuto Ozaki (Keio University); Gakuto Chiba (Keio University); Yukun Li (Queen Mary University of London); Yu Zhaoxin (Shandong College of Arts); Wei Yue (Shandong College of Arts); Miranda Crowdus (Concordia University); Gabriel Zuckerberg (Brown University); Olga Velichkina (Rench Society for Ethnomusicology); Yulia Nikolaenko (independent); Yannick Wey (Lucerne University of Applied Sciences and Arts); Lawrence Shuster (Cornell University); Patrick E. Savage (Keio University); Elizabeth Phillips (McMaster University); Andrew Killick (University of Sheffield),"Proutskova, Polina*; McBride, John M; Ozaki, Yuto; Chiba, Gakuto; Li, Yukun; Zhaoxin, Yu; Yue, Wei; Crowdus, Miranda; Zuckerberg, Gabriel; Velichkina, Olga; Nikolaenko, Yulia; Wey, Yannick; Shuster, Lawrence; Savage, Patrick E.; Phillips, Elizabeth; Killick, Andrew",proutskova@googlemail.com*; jmmcbride@protonmail.com; yozaki@sfc.keio.ac.jp; gane1222@sfc.keio.ac.jp; yukun.li@qmul.ac.uk; 943469149@qq.com; yuewei1981zgyy@126.cm; miranda.crowdus@concordia.ca; gabriel_zuckerberg@brown.edu; olga.velichkina@gmail.com; mitida173@gmail.com; yannick.wey@hslu.ch; lbs239@cornell.edu; psavage@sfc.keio.ac.jp; phille10@mcmaster.ca; a.killick@sheffield.ac.uk,Accept,Rench Society for Ethnomusicology | Shandong College of Arts | Brown University | Cornell University | Concordia University | independent | Queen Mary University of London | McMaster University | Keio University | University of Sheffield | Lucerne University of Applied Sciences and Arts | Institute for Basic Science,Physical,https://drive.google.com/file/d/1VIcWtjUY0HM94ssTCaZLeAeUCjkH7zUO/preview,,,emoji_music.037.png,,,37
355,FROM TAPS TO DRUMS: AUDIO PERCUSSION STYLE TRANSFER,"A common, and arguably innate, human response when listening to music is to tap one's foot to mark the regular pulse of the beat. A more complex form of interactive synchronization occurs when listeners tap out rhythmic patterns using their fingers, hands, or even some form of improvised drumsticks.  In this late-breaking demo, we explore this interaction by leveraging the style transfer capabilities of a neural audio synthesis model by training it on a drum dataset and feeding it tapped rhythm recordings at inference time. We also provide a concise and high-level overview of the results, which, in our assessment, not only justify further research but also establish an intriguing baseline for future investigations. Finally, we point out several future research directions.",André C. Santos,andresantos@dei.uc.pt,André C. Santos (CISUC - UC)*; F. Amilcar Cardoso (University Coimbra),"Santos, André C.*; Cardoso, F. Amilcar",andresantos@dei.uc.pt*; amilcar@dei.uc.pt,Accept,University Coimbra | CISUC - UC,Physical,https://drive.google.com/file/d/1hzKEZD6DP-WUyWbY6J2PV7_SIc3kZQGb/preview,,,emoji_music.038.png,,,38
356,JAMMIN-GPT: TEXT-BASED IMPROVISATION USING LLMS IN ABLETON LIVE,"We introduce a system that allows users of Ableton Live to create MIDI-clips by naming them with musical descriptions. Users can compose by typing the desired musical content directly in Ableton's clip view, which is then inserted by our integrated system. This allows users to stay in the flow of their creative process while quickly generating musical ideas. The system works by prompting ChatGPT to reply using one of several text-based musical formats, such as ABC notation, chord symbols, or drum tablature. This is an important step in integrating generative AI tools into pre-existing musical workflows, and could be valuable for content makers who prefer to express their creative vision through descriptive language.",Sven Hollowell,sven.hollowell@bristol.ac.uk,Sven Hollowell (University of Bristol)*; Tashi Namgyal (University of Bristol); Paul Marshall (University of Bristol),"Hollowell, Sven*; Namgyal, Tashi; Marshall, Paul",sven.hollowell@bristol.ac.uk*; tashi.namgyal@bristol.ac.uk; p.marshall@bristol.ac.uk,Accept,University of Bristol,Physical,https://drive.google.com/file/d/14xCaG-5JkcBn4a-WUt3eOkjIZwOKRUoZ/preview,,,emoji_music.039.png,,,39
357,Visual Guitar Tab Comparison,"We designed a visual interface for comparing different guitar tablature (tab) versions of the same piece. By automatically aligning the bars of these versions and visually encoding different metrics, our interface helps determine similarity, difficulty, and correctness. During our design, we collected and integrated feedback from musicians and finally conducted a qualitative evaluation with five guitarists. Results confirm that our interface effectively supports comparison and helps musicians choose a version appropriate for their personal skills and tastes.",Frank Heyen,frank.heyen@visus.uni-stuttgart.de,"Frank Heyen (VISUS, University of Stuttgart)*; Alejandro Gabino Diaz Mendoza (VISUS, University of Stuttgart); Quynh Quang Ngo (VISUS, University of Stuttgart); Michael Sedlmair (Uni Stuttgart)","Heyen, Frank*; Diaz Mendoza, Alejandro Gabino; Ngo, Quynh Quang; Sedlmair, Michael",frank.heyen@visus.uni-stuttgart.de*; st180643@stud.uni-stuttgart.de; quynh.ngo@visus.uni-stuttgart.de; Michael.Sedlmair@visus.uni-stuttgart.de,Desk Reject,"Uni Stuttgart | VISUS, University of Stuttgart",Physical,https://drive.google.com/file/d/1KvFlXGEmMjySR_h3lmAz0CDK3thHS3PP/preview,,,emoji_music.040.png,,,40
358,Optimizing the Mridangam Stroke Transcription Pipeline: Addressing Key Challenges,"In this study, we examined different facets of the mridangam stroke transcription pipeline. We employed datasets comprising of mridangam strokes, some artificially generated through simulations using a browser application in which the strokes can be generated by typing the syllables on a text window, and others recorded authentically. We investigate the effectiveness of clustering techniques on the task of stroke classification. Additionally, pre-trained residual neural networks (ResNet) were also examined by fine-tuning them on the task. Our initial findings, supported by existing literature, underscore the necessity of introducing additional steps to address composite strokes (strokes created by combining two basic strokes) and accommodate full-length recordings of the mridangam. We also delved into the task of audio segmentation using onset detection techniques as part of our broader exploration within this context. Our aim was to segment mridangam recordings into individual strokes. Our preliminary finding suggests that utilizing spectral flux for onset detection, coupled with a post-processing step involving a local average computation step, produced promising results. ",Gopika Krishnan,gk1656@nyu.edu,Gopika Krishnan (NYU Abu Dhabi)*; Kaustuv Ganguli (NYU Abu Dhabi); Carlos Guedes (NYU Abu Dhabi),"Krishnan, Gopika*; Ganguli, Kaustuv; Guedes, Carlos",gk1656@nyu.edu*; kaustuvkanti@nyu.edu; carlos.guedes@nyu.edu,Accept,NYU Abu Dhabi,Virtual,https://drive.google.com/file/d/1DhlhRTFYW7AARv3GxmO1J0JZudv3sVa-/preview,,,emoji_music.041.png,,,41
