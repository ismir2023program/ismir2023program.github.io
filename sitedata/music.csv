uid,position,title,abstract,primary_author,primary_email,authors,author_emails,affiliation,bio,web_link,session,yt_id,thumbnail_link,channel_name,channel_url,release_consent
1,1,"""confluyo yo, el ambiente me sigue""","<b>Confluyo yo, el ambiente me sigue</b> is an extrospective study of the relationships between intentional sounds, autonomous natural sounds, and sounds as byproducts of human activity. This piece integrates human improvisation with artificial intelligence, where an 'unlooper’ generates intricate sonic variations of loose, nine-second themes played by the performer called 'seeds.' Through a generative model, the seeds gradually dematerialize and morph into different sound distributions, which fosters an evolving dialogue between the performer and the co-creative AI, resulting in a continuously changing texture for the performer's improvisation. Drawing from the aesthetics of sonic ecology, the piece emphasizes the complex interconnections between intentional, incidental, and autonomous sounds in our acoustic ecosystem.",Hugo Flores Garcia,hugofloresgarcia@u.northwestern.edu,Hugo Flores Garcia,hugofloresgarcia@u.northwestern.edu,Northwestern University,,https://hugo-does-things.notion.site/confluyo-yo-el-ambiente-me-sigue-38a1b1073a1643a5b2ad0902a02daba7,1,https://www.youtube.com/embed/mcjf2iKf8Nk?si=9JxMM2CE6JCjWAE_,,,,
2,2,Conversations with our Digital Selves: the development of an autonomous music improviser,"This paper describes a series of real-time music improvisation systems we have created over the last 15 years. The systems are all based on variable-order Markov models. We explain our iterated design process focused on creating collaborative AI systems to enhance human musical experiences. We have used the systems to play piano, drums and even to control a set of live effects and loopers. The systems have performed at live concerts with professional musicians, with some performances being broadcast on national radio. The system is sufficiently well-developed that professional and amateur musicians can now use it in various improvising contexts without our direct support.",Matthew Yee-King,m.yee-king@gold.ac.uk, Matthew Yee-King | Mark d'Inverno, m.yee-king@gold.ac.uk | dinverno@gold.ac.uk,"Goldsmiths, University of London","<b>Mark d’Inverno</b> is a critically acclaimed jazz pianist (BBC TV & National Radio, BBC Music Magazine, Guardian, Observer, Jazz Review) who has toured nationally and internationally since the 1980s. He is also a full professor at Goldsmiths, University of London, taking on the role in 2006. He is recognised as a leading academic researcher exploring the relationship between Artificial Intelligence and Human creativity, and his invited talks included improvised piano performances with AI systems. He has over 250 peer-reviewed publications, was formerly Director of Research at Goldsmiths, and currently holds honorary research positions at Instituto de Investigación en Inteligencia Artificial (IIIA) Universitat Autònoma de Barcelona and Politecnico di Milano, Italy. ",,1,,,,,
3,3,AI Pianist Performance: Collaboration with Soprano Sumi Jo,"We organized a collaborative vocal performance using interaction with an automated piano. The performance was an experiment to see if MIR technology could be used to minimize the number of performers or operators and create a natural performance. The performance was held on June 27 at the KAIST Sports Complex with soprano Sumi Cho, and &lt;Heidenlöslein (Wild Rose)&gt; was the first of the songs performed. The system consists of 1. an automatic expressive performance generation system virtusonet / 2. a vocal reactive accompaniment system to adjust the timing for virtusonet 3. virtual performer visualization, 4. automatic lyric tracking, and 5. control system.<br/>The piece begins without a piano player, but instead with an automated piano and a visualization of the sides and hands to fill in the presence of the performer. &lt;Heidenlöslein&gt; has three repeated fermata sections in which the performance system waits for the accompaniment in time with the singing and shows interactions at the right time. In addition, a system specialized in tracking lyrics was used to automatically track and display the lyrics, and the entire system was automated by enabling real-time communication. Despite the wet and noisy environment of the performance hall, the system worked successfully in the actual performance, demonstrating the possibility of systematically applying the technology in the actual performance hall instead of the laboratory.<br/>",,,,,,,,1,https://drive.google.com/file/d/1Wx6Kq-PogNSTL6Yv3Tocfvg7ol1FFWZk/view?usp=sharing,,,,
4,4,The Words I Tried to Say ,"This submission introduces ""The Words I Tried to Say,"" a multimedia piece created in Adobe Animate that explores the visual representation of music. The project delves into the emotional intricacies of music by incorporating key frames at significant musical points and complementing them with expressive lines, evoking a sense of directionless wandering in darkness. Each key frame captures the essence of the music at precise moments, forming a profound visual narrative that mirrors the ebb and flow of emotions within the composition. By skillfully interconnecting these key frames using the Adobe Animate transform tool, the project crafts a seamless and immersive experience, enabling viewers to delve deeper into the interconnected relationship between music and visuals. ""The Words I Tried to Say"" offers a captivating and thought-provoking exploration of the expressive potential of music through digital art, paving the way for innovative approaches in the realm of multimedia storytelling.",Angela Weihan Ng,2485186N@student.gla.ac.uk,Angela Weihan Ng,2485186N@student.gla.ac.uk,University of Glasgow,,,1,https://youtu.be/xF_EpFz8SAg,,,,
5,5,Sliogán: a performance composed for the HITar ,"This proposal presents the performance of an original piece inspired by the Irish folk tradition. The HITar, an augmented guitar prototype for percussive fingerstyle, is used here to imitate the sound of the bodhrán, an Irish frame drum, while improvising over the theme of a jig.",Andrea Martelloni,a.martelloni@qmul.ac.uk,Andrea Martelloni | Andrew McPherson | Mathieu Barthet,a.martelloni@qmul.ac.uk | a.mcpherson@qmul.ac.uk | m.barthet@qmul.ac.uk,Queen Mary University of London,"<b>Andrea Martelloni</b> is a guitar player, composer, producer and fourth-year PhD student at the Artificial Intelligence and Music programme at C4DM, Queen . His research includes work on the HITar and the field of gesture recognition applied to expressive digital musical instruments, as well as behavioural methods to evaluate DMIs such as micro-phenomenology. He has being playing guitar for over twenty years, studying at the Centro Professione Musica in Milan during his teenage and carrying on as self-taught afterwards. He is an active session musician in the South East of England. His current main project is Sloth In The City with wife and saxophonist Betty Accorsi. Other projects include solo guitar act Virgult, jazz guitar (Betty Accorsi Quartet, Madz and the Martians), function (Miss and the Demeanors), folk (Monkey See Monkey Do, Hilltop Ceilidh Band).<br><br><b>Mathieu Barthet</b> (PhD) is a senior lecturer at Queen Mary University of London and guitarist/composer. He received MSc degrees in electronics and computer science (Paris VI University, 2003) and acoustics (Aix-Marseille University/Ecole Centrale Marseille, 2004). He was awarded a PhD in acoustics, signal processing, and computer science applied to music (Aix-Marseille University/CNRS-LMA, 2008). He holds a professional certificate in music theory and composition (Berklee Online). He is the director of the UKRI Media & Arts Technology Centre for Doctoral Training (CDT), and co-investigator on the UKRI AI & Music CDT. He co-authored over 130 publications on new interfaces for musical expression, music information retrieval, and music perception.",,1,https://www.youtube.com/watch?v=64JVrCn5ih8,,,,
6,6,Nor Hope,"Nor Hope is a piece for soprano, electronics, and video that I wrote in the summer of 2021. The music was inspired by William Butler Yeats's poem ""Death."" The vocalist sings a melody without words, showcasing the radiant high register of the soprano's voice. Most of the electronic sounds in the piece are generated and processed by programming software. I used the electronics to create a tranquil soundscape that fits the mood of the poem.<br/><br/>In order to present the music at a digital concert during the pandemic, I created a music video that features soprano Stephany Svorinić performing in Salem. The video editing process allowed me to create a visual experience that enhanced the audience's engagement with the music.",Wenbin Lyu,lwb862413031@gmail.com,Wenbin Lyu,lwb862413031@gmail.com,,"<b>Wenbin Lyu</b> is a Chinese composer based in Cincinnati. His compositions blend contemporary Western techniques with ancient Oriental culture, drawing inspiration from nature, science, and video games. His works have been featured at over 60 music festivals, such as Cabrillo, Tanglewood, NYCEMF, IRCAM, SEAMUS, and ICMC. He has collaborated with acclaimed ensembles, including the Buffalo Philharmonic, Albany Symphony, Beijing Symphony, Eighth Blackbird, Akropolis Quintet, and Sandbox Percussion. Lyu has received one ASCAP Young Composer Award and three The American Prize awards. He holds degrees from the China Conservatory, NEC, and CCM.",https://wenbinlyu.com,1,https://drive.google.com/file/d/18JWj-NIC8l2Jc5N2jv5jBvN0GuiAahO4/view,,,,
