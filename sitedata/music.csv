,uid,position,title,abstract,primary_author,primary_email,authors,author_emails,affiliation,bio,web_link,session,yt_id,thumbnail_link,channel_name,channel_url,release_consent
0,1,1,Conversations with our Digital Selves: the development of an autonomous music improviser,"This paper describes a series of real-time music improvisation systems we have created over the last 15 years. The systems are all based on variable-order Markov models. We explain our iterated design process focused on creating collaborative AI systems to enhance human musical experiences. We have used the systems to play piano, drums and even to control a set of live effects and loopers. The systems have performed at live concerts with professional musicians, with some performances being broadcast on national radio. The system is sufficiently well-developed that professional and amateur musicians can now use it in various improvising contexts without our direct support.",Matthew Yee-King,m.yee-king@gold.ac.uk, Matthew Yee-King | Mark d'Inverno, m.yee-king@gold.ac.uk | dinverno@gold.ac.uk,"Goldsmiths, University of London","<b>Mark d’Inverno</b> is a critically acclaimed jazz pianist (BBC TV & National Radio, BBC Music Magazine, Guardian, Observer, Jazz Review) who has toured nationally and internationally since the 1980s. He is also a full professor at Goldsmiths, University of London, taking on the role in 2006. He is recognised as a leading academic researcher exploring the relationship between Artificial Intelligence and Human creativity, and his invited talks included improvised piano performances with AI systems. He has over 250 peer-reviewed publications, was formerly Director of Research at Goldsmiths, and currently holds honorary research positions at Instituto de Investigación en Inteligencia Artificial (IIIA) Universitat Autònoma de Barcelona and Politecnico di Milano, Italy. ",,1,,,session-music-1,https://slack.com/app_redirect?channel=C063Y16QZQR,
1,2,2,"""confluyo yo, el ambiente me sigue""","<b>Confluyo yo, el ambiente me sigue</b> is an extrospective study of the relationships between intentional sounds, autonomous natural sounds, and sounds as byproducts of human activity. This piece integrates human improvisation with artificial intelligence, where an 'unlooper’ generates intricate sonic variations of loose, nine-second themes played by the performer called 'seeds.' Through a generative model, the seeds gradually dematerialize and morph into different sound distributions, which fosters an evolving dialogue between the performer and the co-creative AI, resulting in a continuously changing texture for the performer's improvisation. Drawing from the aesthetics of sonic ecology, the piece emphasizes the complex interconnections between intentional, incidental, and autonomous sounds in our acoustic ecosystem.",Hugo Flores Garcia,hugofloresgarcia@u.northwestern.edu,Hugo Flores Garcia,hugofloresgarcia@u.northwestern.edu,Northwestern University,"<b>Hugo Flores García</b> (he/him) is a computer musician, scientist and guitarist. Hugo is a PhD Candidate at Northwestern University, performing research at the intersection of machine learning, music, and human-computer interaction. His research centers around designing deep learning systems and human-computer interfaces for creative expression, with a focus on the sound arts.  His creative work swirls around the worlds of improvised music, algorithmic composition, electroacoustic music, jazz, latin american music. Hugo was born and raised in Tegucigalpa, Honduras, but now lives in Chicago, IL, where he performs with several groups across the city.",https://hugo-does-things.notion.site/confluyo-yo-el-ambiente-me-sigue-38a1b1073a1643a5b2ad0902a02daba7,1,https://www.youtube.com/embed/mcjf2iKf8Nk?si=9JxMM2CE6JCjWAE_,,session-music-2,https://slack.com/app_redirect?channel=C063Y19U44R,
2,3,3,Sliogán: a performance composed for the HITar ,"This proposal presents the performance of an original piece inspired by the Irish folk tradition. The HITar, an augmented guitar prototype for percussive fingerstyle, is used here to imitate the sound of the bodhrán, an Irish frame drum, while improvising over the theme of a jig.",Andrea Martelloni,a.martelloni@qmul.ac.uk,Andrea Martelloni | Andrew McPherson | Mathieu Barthet,a.martelloni@qmul.ac.uk | a.mcpherson@qmul.ac.uk | m.barthet@qmul.ac.uk,Queen Mary University of London,"<b>Andrea Martelloni</b> is a guitar player, composer, producer and fourth-year PhD student at the Artificial Intelligence and Music programme at C4DM, Queen . His research includes work on the HITar and the field of gesture recognition applied to expressive digital musical instruments, as well as behavioural methods to evaluate DMIs such as micro-phenomenology. He has being playing guitar for over twenty years, studying at the Centro Professione Musica in Milan during his teenage and carrying on as self-taught afterwards. He is an active session musician in the South East of England. His current main project is Sloth In The City with wife and saxophonist Betty Accorsi. Other projects include solo guitar act Virgult, jazz guitar (Betty Accorsi Quartet, Madz and the Martians), function (Miss and the Demeanors), folk (Monkey See Monkey Do, Hilltop Ceilidh Band).<br><br><b>Mathieu Barthet</b> (PhD) is a senior lecturer at Queen Mary University of London and guitarist/composer. He received MSc degrees in electronics and computer science (Paris VI University, 2003) and acoustics (Aix-Marseille University/Ecole Centrale Marseille, 2004). He was awarded a PhD in acoustics, signal processing, and computer science applied to music (Aix-Marseille University/CNRS-LMA, 2008). He holds a professional certificate in music theory and composition (Berklee Online). He is the director of the UKRI Media & Arts Technology Centre for Doctoral Training (CDT), and co-investigator on the UKRI AI & Music CDT. He co-authored over 130 publications on new interfaces for musical expression, music information retrieval, and music perception.",,1,https://www.youtube.com/watch?v=64JVrCn5ih8,,session-music-3,https://slack.com/app_redirect?channel=C063GE9385T,
3,4,4,The Words I Tried to Say ,"This submission introduces ""The Words I Tried to Say,"" a multimedia piece created in Adobe Animate that explores the visual representation of music. The project delves into the emotional intricacies of music by incorporating key frames at significant musical points and complementing them with expressive lines, evoking a sense of directionless wandering in darkness. Each key frame captures the essence of the music at precise moments, forming a profound visual narrative that mirrors the ebb and flow of emotions within the composition. By skillfully interconnecting these key frames using the Adobe Animate transform tool, the project crafts a seamless and immersive experience, enabling viewers to delve deeper into the interconnected relationship between music and visuals. ""The Words I Tried to Say"" offers a captivating and thought-provoking exploration of the expressive potential of music through digital art, paving the way for innovative approaches in the realm of multimedia storytelling.",Angela Weihan Ng,2485186N@student.gla.ac.uk,Angela Weihan Ng,2485186N@student.gla.ac.uk,University of Glasgow,"<b>Ng Weihan Angela</b> (""Angela"") graduated with an MA in Computing Science and Music from the University of Glasgow in 2023, including a transformative period at Seoul National University. Presently, she embraces the role of Assistant Manager (International Relations, Policy & Strategy) at the Infocomm Media Development Authority of Singapore. Beyond her primary disciplines of study and her career, Angela's intellectual curiosities delve into the areas of cognitive science and neuroaesthetics. In Seoul, her academic journey took flight. She undertook research in AI's musical interpretation alongside Prof. Li Wen-Syan and Prof. Park Jonghwa and her seniors at the Graduate School of Data Science which was paralleled by her involvement with the Human Factors Psychology Lab, enriching her multidisciplinary expertise. Returning to Glasgow, Angela's dissertation focused on the intricate comparison between machine and human music interpretation, supervised by Dr. Marwa Mahmoud. Furthermore, under Dr. Drew Hammond's guidance, she explored the realm of music composition. She also wears multiple hats – previously a member of the Student Representative Council, she enjoyed serving the community and is now a youth volunteer leader at Cyber Youth Singapore. Culminating her academic accolades, Angela recently received in-principle-approval for the prestigious National Science Scholarship (PhD), heralding a new chapter in her quest for innovation for good.",,1,https://youtu.be/xF_EpFz8SAg,,session-music-4,https://slack.com/app_redirect?channel=C063CNJL6KY,
4,5,5,Nor Hope,"Nor Hope is a piece for soprano, electronics, and video that I wrote in the summer of 2021. The music was inspired by William Butler Yeats's poem ""Death."" The vocalist sings a melody without words, showcasing the radiant high register of the soprano's voice. Most of the electronic sounds in the piece are generated and processed by programming software. I used the electronics to create a tranquil soundscape that fits the mood of the poem.<br/><br/>In order to present the music at a digital concert during the pandemic, I created a music video that features soprano Stephany Svorinić performing in Salem. The video editing process allowed me to create a visual experience that enhanced the audience's engagement with the music.",Wenbin Lyu,lwb862413031@gmail.com,Wenbin Lyu,lwb862413031@gmail.com,,"<b>Wenbin Lyu</b> is a Chinese composer based in Cincinnati. His compositions blend contemporary Western techniques with ancient Oriental culture, drawing inspiration from nature, science, and video games. His works have been featured at over 60 music festivals, such as Cabrillo, Tanglewood, NYCEMF, IRCAM, SEAMUS, and ICMC. He has collaborated with acclaimed ensembles, including the Buffalo Philharmonic, Albany Symphony, Beijing Symphony, Eighth Blackbird, Akropolis Quintet, and Sandbox Percussion. Lyu has received one ASCAP Young Composer Award and three The American Prize awards. He holds degrees from the China Conservatory, NEC, and CCM.",https://wenbinlyu.com,1,https://drive.google.com/file/d/18JWj-NIC8l2Jc5N2jv5jBvN0GuiAahO4/view,,session-music-5,https://slack.com/app_redirect?channel=C063KC3AFPC,
5,6,6,AI Pianist Performance: Collaboration with Soprano Sumi Jo,"We organized a collaborative vocal performance using interaction with an automated piano. The performance was an experiment to see if MIR technology could be used to minimize the number of performers or operators and create a natural performance. The performance was held on June 27 at the KAIST Sports Complex with soprano Sumi Cho, and &lt;Heidenlöslein (Wild Rose)&gt; was the first of the songs performed. The system consists of 1. an automatic expressive performance generation system virtusonet / 2. a vocal reactive accompaniment system to adjust the timing for virtusonet 3. virtual performer visualization, 4. automatic lyric tracking, and 5. control system.<br/>The piece begins without a piano player, but instead with an automated piano and a visualization of the sides and hands to fill in the presence of the performer. &lt;Heidenlöslein&gt; has three repeated fermata sections in which the performance system waits for the accompaniment in time with the singing and shows interactions at the right time. In addition, a system specialized in tracking lyrics was used to automatically track and display the lyrics, and the entire system was automated by enabling real-time communication. Despite the wet and noisy environment of the performance hall, the system worked successfully in the actual performance, demonstrating the possibility of systematically applying the technology in the actual performance hall instead of the laboratory.<br/>",Taegyun Kwon,ilcobo2@kaist.ac.kr,Taegyun Kwon | Joonhyung Bae| Jiyun Park| Jaeran Choi| Hyeyoon Cho | Yonghyun Kim | Dasaem Jeong| Juhan Nam,ilcobo2@kaist.ac.kr | jh.bae@kaist.ac.kr | june@kaist.ac.kr | jaeran.choi@kaist.ac.kr | hyeyooncho@kaist.ac.kr | yonghyun.kim@kaist.ac.kr | dasaemj@sogang.ac.kr | juhan.nam@kaist.ac.kr,KAIST | Sogang University,"<b>Taegyun Kwon</b> is Ph.D. Candidate at the Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology (KAIST). He researched on piano performance analysis, including realtime piano transcription, alignment and expressive generation.<br><b>Joonhyung Bae</b>, a Korean artist and Ph.D. Candidate at GSCT KAIST. He is currently a Music and Audio Computing Lab member. He is researching sound-based virtual performer visualization for artistic expression using deep learning.<br><b>Jiyun Park</b> is a Ph.D. student in the Music and Audio Computing Lab at GSCT KAIST. Her research focuses on music performance analysis including real-time music alignment and singing voice. In this work, she was responsible for the development and operation of a real-time lyrics tracking system.<br><b>Jaeran Choi</b> is a Master Student at GSCT KAIST. Her research interests include Human-AI musical interaction. In particular, she focuses on multimodal musical cue detection and reactive accompaniment systems.<br><b>Hyeyoon Cho</b> is a second year master student at GSCT KAIST. She received a Bachelor's degree in piano performance at University of Texas, Austin and  Master’s in piano performance at Indiana University. Her research interests include quantization in piano performance and music information retrieval.<br><b>Yonghyun Kim</b> is currently pursuing a master's degree at GSCT KAIST. His research areas of interest are Music, Artificial Intelligence, and HCI. He is currently focusing on research that combines multimedia (esp. audio and vision) and AI to enrich human musical experience and creation.<br><b>Dasaem Jeong</b>, Assistant Professor in the Department of Art & Technology at Sogang University, South Korea. He obtained his Ph.D. in culture technology from KAIST under supervision of Juhan Nam. His research focuses on various music information retrieval tasks, including expressive performance modeling and symbolic music generation.<br><b>Juhan Nam</b> is an Associate Professor in the Graduate School of Culture Technology at the Korea Advanced Institute of Science and Technology (KAIST) in South Korea. He is the director of the Music and Audio Computing Lab. He is interested in various topics at the interaction of music, audio signal processing, machine learning, and human-computer interaction.",,1,https://drive.google.com/file/d/1Wx6Kq-PogNSTL6Yv3Tocfvg7ol1FFWZk/view?usp=sharing,,session-music-6,https://slack.com/app_redirect?channel=C063Y1LNY7K,
