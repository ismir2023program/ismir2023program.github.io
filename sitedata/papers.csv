uid,title,abstract,authors_and_affil,author_emails,Video Link,slides_pdf,poster_pdf,thumbnail,session,pdf_path,day,position,proceedings_id,pdf_name,primary_author,primary_email,primary_subject,secondary_subject,video,AwardNominee,long_presentation,paper_presentation,SpecialTrack,abstract_short,StudentAuthor,channel_url,slack_channel
64,Exploring the correspondence of melodic contour with gesture in raga alap singing ,Musicology research suggests a correspondence between manual gesture and melodic contour in raga performance. Computational tools such as pose estimation from video and time series pattern matching potentially facilitate larger-scale studies of gesture an,Shreyas M Nadkarni (Indian Institute of Technology Bombay); Sujoy Roychowdhury (Indian Institute of Technology Bombay); Preeti Rao (Indian Institute of Technology  Bombay)*; Martin Clayton (Durham University),shreyasnadkarni@ee.iitb.ac.in; 214077004@iitb.ac.in; prao@ee.iitb.ac.in; martin.clayton@durham.ac.uk,https://drive.google.com/file/d/1DvOBLNNh3C8wfID8ClLVXxXqj2suDBzO/view?,https://docs.google.com/presentation/d/1wUQl2ZIp4gBwMCCxHAWl2PAOKeLfN5qs/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1SJX_IvazSTMMezLbKk_2kT-4g3_pSFev/view,https://drive.google.com/file/d/1GlniwEHuaE5n0Zei7oXLyVn_dHGy0bmm/view?usp=sharing,1,https://archives.ismir.net/ismir2023/paper/000001.pdf,2,0,1,000001.pdf,Preeti Rao,prao@ee.iitb.ac.in,Knowledge-driven approaches to MIR -> computational ethnomusicology,MIR fundamentals and methodology -> multimodality,https://drive.google.com/uc?export=view&id=1DvOBLNNh3C8wfID8ClLVXxXqj2suDBzO,True,True,In Person,,,,https://slack.com/app_redirect?channel=C064B57QBB3,p1-01-rao
11,TriAD: Capturing harmonics with 3D Convolutions,"Thanks to advancements in deep learning (DL), automatic music transcription (AMT) systems recently outperformed previous ones fully based on manual feature design. Many of these highly capable DL models, however, are computationally expensive. Researchers",Miguel Perez Fernandez (Universitat Pompeu Fabra; Huawei)*; Holger Kirchhoff (Huawei); Xavier Serra (Universitat Pompeu Fabra ),miguel_on_94@hotmail.com; holger.kirchhoff@huawei.com; xavier.serra@upf.edu,https://drive.google.com/file/d/1yFCCW_X06c_V-Q_xlfOnCbM8JoNzIVtu/view,https://drive.google.com/file/d/1Mmrf4Umxtbn_To-nnkTw8tkcrspuyYCP/view,https://drive.google.com/file/d/14jgsYz5Nb8m3OGaiW4RxQe26yi35tyhL/view,https://drive.google.com/file/d/1juo5XqKdMdu8902Kq6GWZ2BOAE1wCFw4/view,1,https://archives.ismir.net/ismir2023/paper/000002.pdf,2,1,2,000002.pdf,Miguel Perez Fernandez,miguel_on_94@hotmail.com,MIR tasks -> music transcription and annotation,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music,https://drive.google.com/uc?export=view&id=1yFCCW_X06c_V-Q_xlfOnCbM8JoNzIVtu,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J0VR14P,p1-02-fernandez
15,Data Collection in Music Generation Training Sets: A Critical Analysis,"The practices of data collection in training sets for Automatic Music Generation (AMG) tasks are opaque and overlooked. In this paper, we aimed to identify these practices and surface the values they embed. We systematically identified all datasets used t",Fabio Morreale (University of Auckland)*; Megha Sharma (University of Tokyo); I-Chieh Wei (University of Auckland),f.morreale@auckland.ac.nz; meghas@g.ecc.u-tokyo.ac.jp; iwei022@aucklanduni.ac.nz,https://drive.google.com/file/d/1-QENG5zYQSGwLoG9VowxWCQ2p9GqEKf5/view,https://drive.google.com/file/d/1RVWV_3soKGbFOVUz1Q05avA5IMlyw5D-/view?usp=sharing ,https://drive.google.com/file/d/1UqLBTjYlhPcuEpl676ybWk9Nq0kGb9H8/view,https://drive.google.com/file/d/1LO7bnd58OVp9yfX6fId8yV5tJF-rIk68/view,1,https://archives.ismir.net/ismir2023/paper/000003.pdf,2,2,3,000003.pdf,Fabio Morreale,f.morreale@auckland.ac.nz,Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies,MIR tasks -> music generation; Philosophical and ethical discussions -> legal and societal aspects of MIR,https://drive.google.com/uc?export=view&id=1-QENG5zYQSGwLoG9VowxWCQ2p9GqEKf5,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B5Y847K,p1-03-morreale
18,A Review of Validity and its Relationship to Music Information Research,"Validity is the truth of an inference made from evidence and is a central concern in scientific work. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has be",Bob L. T.  Sturm (KTH Royal Institute of Technology); Arthur Flexer (Johannes Kepler University Linz)*,bobs@kth.se; arthur.flexer@jku.at,https://drive.google.com/file/d/1gr803BCvb98rdFiAzIpGivrGkgEtNkt-/view,https://drive.google.com/file/d/1J1lu0SYRpuZW4E7rTZ_a4BL30f8VdaMC/view?usp=share_link,https://drive.google.com/file/d/1J9FH9FKTPCGRiRyCMbB_23NdfKI-7JW8/view,https://drive.google.com/file/d/1LV47ziqlkzs6AtO4SW_7iPcVeUW8BRO6/view,1,https://archives.ismir.net/ismir2023/paper/000004.pdf,2,3,4,000004.pdf,Arthur Flexer,arthur.flexer@jku.at,Philosophical and ethical discussions,"Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation methodology; Philosophical and ethical discussions -> philosophical and methodological foundations",https://drive.google.com/uc?export=view&id=1gr803BCvb98rdFiAzIpGivrGkgEtNkt-,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VJZQLER,p1-04-flexer
19,Segmentation and Analysis of Taniavartanam in Carnatic Music Concerts,"In Carnatic music concerts, taniavartanam is a solo percussion segment that showcases intricate and elaborate extempore rhythmic evolution through a series of homogeneous sections with shared rhythmic characteristics. While taniavartanam segments have bee",Gowriprasad R (IIT Madras)*; Srikrishnan Sridharan (Carnatic Percussionist); R Aravind (Indian Institute of Technology Madras); Hema A Murthy (IIT Madras),ee19d702@smail.iitm.ac.in; srikrishnansridharan@gmail.com; aravind@ee.iitm.ac.in; hema@cse.iitm.ac.in,https://drive.google.com/file/d/1u1UzWleSWCC83KIgrl-JyiHyVkfqVDd6/view,https://drive.google.com/file/d/1kOIYVsnr__hCGj3mTg3EkhAJsrTqzsWy/view,https://drive.google.com/file/d/1_GZqiSYYzUFfLsvr2L8qPjzZYFQF9Iei/view,https://drive.google.com/file/d/1HvjtFZBzPh8_T1ICDvb7yBk3BK6mfBv6/view,1,https://archives.ismir.net/ismir2023/paper/000005.pdf,2,4,5,000005.pdf,Gowriprasad R,ee19d702@smail.iitm.ac.in,"Musical features and properties -> structure, segmentation, and form",Applications -> music retrieval systems,https://drive.google.com/uc?export=view&id=1u1UzWleSWCC83KIgrl-JyiHyVkfqVDd6,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YDWBJ67,p1-05-r
280,Transfer Learning and Bias Correction with Pre-trained Audio Embeddings,Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applicat,"Changhong Wang (Telecom Paris, Institut polytechnique de Paris)*; Gaël Richard (Telecom Paris, Institut polytechnique de Paris); Brian McFee (New York University)",changhong.wang@telecom-paris.fr; gael.richard@telecom-paris.fr; brian.mcfee@nyu.edu,https://drive.google.com/file/d/1QsT16gA_H-9jCpNLm_lV2flwRr7H_NY8/view?usp=share_link,https://docs.google.com/presentation/d/1kUiuzoPCa2uZIVgCoK0aIq1871Lqc8lK/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1xSh8IkEE4LD6SYbg-TwJfMFSNYPX4s1G/view?usp=sharing,https://drive.google.com/file/d/1CX9JLGWqc1qrqyLtjHC5hA85qiBjqvYG/view?usp=share_link,1,https://archives.ismir.net/ismir2023/paper/000006.pdf,2,5,6,000006.pdf,Changhong Wang,changhong.wang@telecom-paris.fr,"Musical features and properties -> timbre, instrumentation, and singing voice",Applications -> music retrieval systems; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification; Philosophical and ethical discussions -> philosophical and methodological foundations,https://drive.google.com/uc?export=view&id=1QsT16gA_H-9jCpNLm_lV2flwRr7H_NY8,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YGS69S6,p1-06-wang
37,Collaborative Song Dataset (CoSoD): An annotated dataset of multi-artist collaborations in popular music,"The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist collaborations from the 2010–2019 Billboard “Hot 100” year-end charts. The corpus is annotated with formal sections, aspects of vocal production (including reverberation, layering, pan",Michèle Duguay (Harvard University)*; Kate Mancey (Harvard University); Johanna Devaney (Brooklyn College),mduguay@fas.harvard.edu; kmancey@g.harvard.edu; johanna.devaney@brooklyn.cuny.edu,https://drive.google.com/file/d/1GFvSckyxGyvPrMzXF7m5K-aq-4ov5sAn/view,https://drive.google.com/file/d/1ad-yWmMHEz--xu2_O9x5tVGeDvnn41tU/view,https://drive.google.com/file/d/1OGexuCofkntBg7wsXCBzB1KLrT30cDXs/view,https://drive.google.com/file/d/15hqeiDppOtjRo2idW-bo4NYiSJMlMXLK/view,1,https://archives.ismir.net/ismir2023/paper/000007.pdf,2,6,7,000007.pdf,Michèle Duguay,mduguay@fas.harvard.edu,"Musical features and properties -> timbre, instrumentation, and singing voice","Evaluation, datasets, and reproducibility -> novel datasets and use cases",https://drive.google.com/uc?export=view&id=1GFvSckyxGyvPrMzXF7m5K-aq-4ov5sAn,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B645ETB,p1-07-duguay
58,Human-AI  Music Creation: Understanding the Perceptions and Experiences of Music Creators for Ethical and Productive Collaboration,"Recently, there has been a surge in Artificial Intelligence (AI) tools that allow creators to develop melodies, harmonies, lyrics, and mixes with the touch of a button. The reception of and discussion on the use of these tools - and more broadly, any AI-b",Michele Newman (University of Washington)*; Lidia J Morris (University of Washington); Jin Ha Lee (University of Washington),mmn13@uw.edu; ljmorris@uw.edu; jinhalee@uw.edu,https://drive.google.com/file/d/12ZEKkCsiIM7gYj_Qkrc5ol0FGDtKb9EX/view,https://drive.google.com/file/d/1WxtwM1fg3o6M-QBJvwQzsK3QRx28TAos/view,https://drive.google.com/file/d/1lTcgeRKggJmoOSX44-Sg1GrrZ2lurfl6/view,https://drive.google.com/file/d/1sOy7sZgpwlBoD_QHqYP9APDTRvUqi9cw/view,1,https://archives.ismir.net/ismir2023/paper/000008.pdf,2,7,8,000008.pdf,Michele Newman,mmn13@uw.edu,Human-centered MIR -> human-computer interaction,"Applications -> music composition, performance, and production; MIR tasks -> music generation; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies",https://drive.google.com/uc?export=view&id=12ZEKkCsiIM7gYj_Qkrc5ol0FGDtKb9EX,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGHGZCG,p1-08-newman
45,Impact of time and note duration tokenizations on deep learning symbolic music modeling,"Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., f",Nathan Fradet (LIP6 - Sorbonne University)*; Nicolas Gutowski (University of Angers); Fabien Chhel (Groupe ESEO); Jean-Pierre Briot (CNRS),nathan.fradet@lip6.fr; nicolas.gutowski@univ-angers.fr; fabien.chhel@eseo.fr; Jean-Pierre.Briot@lip6.fr,https://drive.google.com/file/d/19mvoQ2eSKYbbUn2HbRVjW-8Vimnb6nBh/view,https://docs.google.com/presentation/d/1BtSnnUimfGL5ul_FcvyeIDL0Ah7InzkA/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1t-Odohpt7ZGzGBqbGGJVmXYJtgHfDpY8/view,https://drive.google.com/file/d/1AaS2ikOX0wJ7eMYB15eQnRr8whwSmgA2/view,1,https://archives.ismir.net/ismir2023/paper/000009.pdf,2,8,9,000009.pdf,Nathan Fradet,nathan.fradet@lip6.fr,MIR fundamentals and methodology -> symbolic music processing,"Applications -> music composition, performance, and production; Applications -> music retrieval systems; MIR tasks -> automatic classification; MIR tasks -> music generation; Musical features and properties -> representations of music",https://drive.google.com/uc?export=view&id=19mvoQ2eSKYbbUn2HbRVjW-8Vimnb6nBh,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTCAHQE,p1-09-fradet
93,Musical Micro-Timing for Live Coding,"Micro-timing is an essential part of human music-making, yet it is absent from most computer music systems. Partly to address this gap, we present a novel system for generating music with style-specific micro-timing within the Sonic Pi live coding languag",Max Johnson (University of Cambridge); Mark R H Gotham (Durham)*,mj551@cantab.ac.uk; mark.r.gotham@durham.ac.uk,https://drive.google.com/file/d/13IupDzqB5wZ13ihRCwvW2X4eY_0VKSww/view,https://docs.google.com/presentation/d/1h6_WqB13JtLM0GSw2nZBJq3r_CeSN1ln/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1Ce-lVzeVmR8ZewQV4t9vv0KOzCr-VUDA/view,https://drive.google.com/file/d/1_wE0HU7LTpsi57jFw47FQTcQQjYCxXJb/view,1,https://archives.ismir.net/ismir2023/paper/000010.pdf,2,9,10,000010.pdf,Mark R H Gotham,mark.r.gotham@durham.ac.uk,"Applications -> music composition, performance, and production","Computational musicology -> mathematical music theory; Human-centered MIR -> human-computer interaction; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> rhythm, beat, tempo",https://drive.google.com/uc?export=view&id=13IupDzqB5wZ13ihRCwvW2X4eY_0VKSww,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YDZKV5Z,p1-10-gotham
47,A Few-shot Neural Approach for Layout Analysis of Music Score Images,"Optical Music Recognition (OMR) is a well-established research field focused on the task of reading musical notation from images of music scores. In the standard OMR workflow, layout analysis is a critical component for identifying relevant parts of the i",Francisco J. Castellanos (University of Alicante)*; Antonio Javier Gallego (Universidad de Alicante); Ichiro Fujinaga (McGill University),fcastellanos@dlsi.ua.es; jgallego@dlsi.ua.es; ichiro.fujinaga@mcgill.ca,https://drive.google.com/file/d/1Yk4OZSK1v7reHk99gfC_SD_i-MkAk9DZ/view,https://docs.google.com/presentation/d/1zKoF02N2rR5QIyWXaGT2sF-NnTF_KpqS/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1_bVna_LTgc60q8uVuiVBzoM65a1qtWdg/view,https://drive.google.com/file/d/1n06Hd9Os7bmpYO9lGfmABVz7nz7LEFnK/view,1,https://archives.ismir.net/ismir2023/paper/000011.pdf,2,10,11,000011.pdf,Francisco J. Castellanos,fcastellanos@dlsi.ua.es,MIR tasks -> optical music recognition,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,https://drive.google.com/uc?export=view&id=1Yk4OZSK1v7reHk99gfC_SD_i-MkAk9DZ,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGK281E,p1-11-castellanos
33,TapTamDrum: A Dataset for Dualized Drum Patterns,"Drummers spend extensive time practicing rudiments to develop technique, speed, coordination, and phrasing. These rudiments are often practiced on ""silent"" practice pads using only the hands. Additionally, many percussive instruments across cultures are p",Behzad Haki (Universitat Pompeu Fabra)*; B_a_ej Kotowski (MTG); Cheuk Lun Isaac Lee (Universitat Pompeu Fabra ); Sergi Jordà (Universitat Pompeu Fabra),behzad.haki@upf.edu; kotowski.blazej@gmail.com; clilee@connect.ust.hk; sergi.jorda@upf.edu,https://drive.google.com/file/d/1XuSgxSZWStQ-ojpQ88Zs_ZJ1ZqQbtAf7/view,https://drive.google.com/file/d/1VzVmgua7PcBoZzsVC2ZRNbg4Icj3kK9Q/view,https://drive.google.com/file/d/1riEMHjWzUeSl6UDSchFFDwMUa5vLn55O/view,https://drive.google.com/file/d/1HrcaIZ5sVn1gMcPy2ZLP1QRF0wD1a9A2/view,1,https://archives.ismir.net/ismir2023/paper/000012.pdf,2,11,12,000012.pdf,Behzad Haki,behzad.haki@upf.edu,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Musical features and properties -> representations of music; Musical features and properties -> rhythm, beat, tempo",https://drive.google.com/uc?export=view&id=1XuSgxSZWStQ-ojpQ88Zs_ZJ1ZqQbtAf7,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTCLBE2,p1-12-haki
48,Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar,"Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar bo",Andrea Martelloni (Queen Mary University of London)*; Andrew McPherson (QMUL); Mathieu Barthet (Queen Mary University of London),a.martelloni@qmul.ac.uk; a.mcpherson@qmul.ac.uk; m.barthet@qmul.ac.uk,https://drive.google.com/file/d/1nr-HTFvdiIC2X5TbKLGL3ri8vZr_npdO/view?usp=sharing ,https://drive.google.com/file/d/1hOx2myUhk_H6u9r9CP4WHomRkEj3EtV2/view?usp=sharing ,https://drive.google.com/file/d/1Ergs4Ua6t4qmQvxxNk3auyatJkzI4HEn/view?usp=sharing ,https://drive.google.com/file/d/1tvccEB0fozHYq3Mk_sf9jfX99sNeAH3o/view?usp=sharing ,1,https://archives.ismir.net/ismir2023/paper/000013.pdf,2,12,13,000013.pdf,Andrea Martelloni,a.martelloni@qmul.ac.uk,"Applications -> music composition, performance, and production",Human-centered MIR -> human-computer interaction; Human-centered MIR -> music interfaces and services; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification,https://drive.google.com/uc?export=view&id=1nr-HTFvdiIC2X5TbKLGL3ri8vZr_npdO,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410NQRK6,p1-13-martelloni
80,IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models,"Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generat",Hiromu Yakura (University of Tsukuba)*; Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)),hiromu1996@gmail.com; m.goto@aist.go.jp,https://drive.google.com/file/d/1cHkxzOuNBuD0Oc2L2lCZ_wy-gvfdp9hH/view,https://docs.google.com/presentation/d/1y3RfHFBj8ikT1eGcI-Fk40Hhw-Z09Rnf/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/14DnHS_JotLJtFvUB75FB358BSX02a6kf/view,https://drive.google.com/file/d/1Gxv25ON-lumZ4g95_AA97sD4RESUovos/view,1,https://archives.ismir.net/ismir2023/paper/000014.pdf,2,13,14,000014.pdf,Hiromu Yakura,hiromu1996@gmail.com,Human-centered MIR -> human-computer interaction,"Applications -> music composition, performance, and production; Human-centered MIR -> music interfaces and services; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",https://drive.google.com/uc?export=view&id=1cHkxzOuNBuD0Oc2L2lCZ_wy-gvfdp9hH,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B66AMJM,p1-14-yakura
174,Similarity evaluation of violin directivity patterns for musical instrument retrieval,"The directivity of a musical instrument is a function that describes the spatial characteristics of its sound radiation. The majority of the available literature focuses on measuring directivity patterns, with analysis mainly limited to visual inspections",Mirco Pezzoli (Politecnicno di Milano)*; Raffaele Malvermi (Politecnico di Milano); Fabio Antonacci (Politecnico di Milano); Augusto Sarti (Politecnico di Milano),mirco.pezzoli@polimi.it; raffaele.malvermi@polimi.it; fabio.antonacci@polimi.it; augusto.sarti@polimi.it,https://drive.google.com/file/d/1DzQBvv4ftbSQDl1R0T7jL6U53ghxGgax/view?usp=share_link,https://drive.google.com/file/d/1PR2B1z0Po2w0caRcljD0QwDimTJQ_Nzg/view,https://drive.google.com/file/d/1ky9p0XdOSPFVcj4bJBtp-KfoHcYF7g01/view?usp=share_link,https://drive.google.com/file/d/1H_OImxj9fVzhHXVarGyY8QDqvp-HbeU_/view,1,https://archives.ismir.net/ismir2023/paper/000015.pdf,2,14,15,000015.pdf,Mirco Pezzoli,mirco.pezzoli@polimi.it,MIR and machine learning for musical acoustics,MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis; MIR tasks -> pattern matching and detection; MIR tasks -> similarity metrics,https://drive.google.com/uc?export=view&id=1DzQBvv4ftbSQDl1R0T7jL6U53ghxGgax,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YGUL2E6,p1-15-pezzoli
82,Polyrhythmic modelling of non-isochronous and microtiming patterns,"Computational models and analyses of musical rhythms are predominantly based on the subdivision of durations down to a common isochronous pulse, which plays a fundamental structural role in the organization of their durational patterns. Meter, the most wi",George Sioros (University of Plymouth)*,georgios.sioros@plymouth.ac.uk,,,,,1,https://archives.ismir.net/ismir2023/paper/000016.pdf,2,15,16,000016.pdf,George Sioros,georgios.sioros@plymouth.ac.uk,"Musical features and properties -> rhythm, beat, tempo",Computational musicology; Computational musicology -> mathematical music theory; MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation,,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B66GBKK,p1-16-sioros
91,CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval,"We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we coll",Shangda Wu (Central Conservatory of Music); Dingyao Yu (Peking University); Xu Tan (Microsoft Research Asia); Maosong Sun (Tsinghua University)*,shangda@mail.ccom.edu.cn; yudingyao@pku.edu.cn; xuta@microsoft.com; sms@tsinghua.edu.cn,https://drive.google.com/file/d/1wxx0BrnXEJSS5fFApirGRz1ZjZ4jFfNG/view,https://docs.google.com/presentation/d/1_utPVBoTcFC6oAIl02bIwMDQjo5on6vu/edit?usp=sharing&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1splCZQofFRIUpAVjijY7oNyIwKrzk8Ao/view?usp=sharing,https://drive.google.com/file/d/1EK8OjR8ufm8Kte0qn8N2mgZShn-3ln7X/view?usp=sharing,2,https://archives.ismir.net/ismir2023/paper/000017.pdf,2,0,17,000017.pdf,Maosong Sun,sms@tsinghua.edu.cn,Applications -> music retrieval systems,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> automatic classification; MIR tasks -> indexing and querying",https://drive.google.com/uc?export=view&id=1wxx0BrnXEJSS5fFApirGRz1ZjZ4jFfNG,True,True,In Person,,,,https://slack.com/app_redirect?channel=C06410P6R28,p2-01-sun
238,GENDER-CODED SOUND: ANALYSING THE GENDERING OF MUSIC IN TOY COMMERCIALS VIA MULTI-TASK LEARNING,"Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from exam",Luca Marinelli (Queen Mary University of London)*; George Fazekas (QMUL); Charalampos Saitis (Queen Mary University of London),l.marinelli@qmul.ac.uk; george.fazekas@qmul.ac.uk; c.saitis@qmul.ac.uk,https://drive.google.com/file/d/14XBWlSHxm8Z6cDKguzHtyqobvyS5xmTc/view,https://docs.google.com/presentation/d/1W0kjovJhcsT5tCfxr8mvI0mM0RLmg5C0/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/14oC82QCZKHcnJYoG7X0PicHaE_iNhrSv/view,https://drive.google.com/file/d/1t0t9U2ApBUsYYGC_vorbh7E5bpWql_jk/view,2,https://archives.ismir.net/ismir2023/paper/000018.pdf,2,1,18,000018.pdf,Luca Marinelli,l.marinelli@qmul.ac.uk,Knowledge-driven approaches to MIR -> computational music theory and musicology,"Applications -> business and marketing; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> automatic classification; Musical features and properties -> musical affect, emotion and mood",https://drive.google.com/uc?export=view&id=14XBWlSHxm8Z6cDKguzHtyqobvyS5xmTc,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTD8D5L,p2-02-marinelli
56,A dataset and Baselines for Measuring and Predicting the Music Piece Memorability,"Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspi",Li-Yang Tseng (National Yang Ming Chiao Tung University); Tzu-Ling Lin (National Yang Ming Chiao Tung University); Hong-Han Shuai (National Yang Ming Chiao Tung University)*; JEN-WEI HUANG (NYCU); Wen-Whei Chang (National Yang Ming Chiao Tung University),liyangtseng.ee10@nycu.edu.tw; jolinkiion5629@gmail.com; hhshuai@nctu.edu.tw; admsd.ee10@nycu.edu.tw; wwchang@nctu.edu.tw,https://drive.google.com/file/d/1sx0VLxNPVR2yhVcF7oafzlE4dwxFYw11/view?usp=sharing,https://drive.google.com/file/d/1itkdJM23n1dktth-X1wQE_umX3YzhRE2/view?usp=share_link,https://drive.google.com/file/d/1_57q-qGsRO9druNfRi3YHndX4LqLll4r/view,https://drive.google.com/file/d/1zbxwJflt2qQSiUWTzgr1kb08DcU36rRK/view,2,https://archives.ismir.net/ismir2023/paper/000019.pdf,2,2,19,000019.pdf,Hong-Han Shuai,hhshuai@nctu.edu.tw,MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics,"Evaluation, datasets, and reproducibility -> novel datasets and use cases",https://drive.google.com/uc?export=view&id=1sx0VLxNPVR2yhVcF7oafzlE4dwxFYw11,False,False,Virtually,,,,https://slack.com/app_redirect?channel=C063RTDB6PQ,p2-03-shuai
38,Efficient Notation Assembly in Optical Music Recognition,"Optical Music Recognition (OMR) is the field of research that studies how to computationally read music notation from written documents. Thanks to recent advances in computer vision and deep learning, there are successful approaches that can locate the mu",Carlos Penarrubia (University of Alicante); Carlos Garrido-Munoz (University of Alicante); Jose J. Valero-Mas (Universitat Pompeu Fabra); Jorge Calvo-Zaragoza (University of Alicante)*,carlos.penarrubia@ua.es; carlos.garrido@ua.es; jjvalero@dlsi.ua.es; jcalvo@dlsi.ua.es,https://drive.google.com/file/d/11CfMM9NabhgZnbd6yK3IOaivpeEA_5mE/view,https://drive.google.com/file/d/1rQC4xpjRIsFtJh7k8YZpU2c7Ma3AKY1L/view,https://drive.google.com/file/d/1iqZnqyOnUMX_j5Dr0peH4QVEYHhnODbd/view,https://drive.google.com/file/d/1sxHVg-cit6-6hmsMKt8gi07jQPb01nXH/view,2,https://archives.ismir.net/ismir2023/paper/000020.pdf,2,3,20,000020.pdf,Jorge Calvo-Zaragoza,jcalvo@dlsi.ua.es,MIR tasks -> optical music recognition,,https://drive.google.com/uc?export=view&id=11CfMM9NabhgZnbd6yK3IOaivpeEA_5mE,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGKUE1E,p2-04-calvo-zaragoza
59,White Box Search over Audio Synthesizer Parameters,"Synthesizer parameter inference searches for a set of patch connections and parameters to generate audio that best matches a given target sound. Such optimization tasks benefit from access to accurate gradients. However, typical audio synths incorporate c",Yuting Yang (Princeton University)*; Zeyu Jin (Adobe Research); Adam Finkelstein (Princeton University); Connelly Barnes (Adobe Research),yutingyang.wh@gmail.com; zejin@adobe.com; af@princeton.edu; connellybarnes@gmail.com,https://drive.google.com/file/d/1037kUfS31td5mzFzQ8M_gvlnrQpxUHLH/view,https://docs.google.com/presentation/d/1VPcJ2KJOWeNnM4Jx3EXbAcimTvOtB9Mz/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1fao-b0_BcWYeBoCyAM5SPrdromGaVKlV/view,https://drive.google.com/file/d/1ZR2fsJ7Mh2YO0Q-mu-rHHV6VexG4inz8/view,2,https://archives.ismir.net/ismir2023/paper/000021.pdf,2,4,21,000021.pdf,Yuting Yang,yutingyang.wh@gmail.com,MIR and machine learning for musical acoustics,"Applications -> music composition, performance, and production; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR and machine learning for musical acoustics -> applications of musical acoustics to signal synthesis; MIR fundamentals and methodology -> music signal processing",https://drive.google.com/uc?export=view&id=1037kUfS31td5mzFzQ8M_gvlnrQpxUHLH,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YGV7KRQ,p2-05-yang
63,"Decoding drums, instrumentals, vocals, and mixed sources in music using human brain activity with fMRI","Brain decoding allows the read-out of stimulus and mental content from neural activity, and has been utilised in various neural-driven classification tasks related to the music information retrieval community. However, even the relatively simple task of i","Vincent K.M. Cheung (Sony Computer Science Laboratories, Inc.)*; Lana Okuma (RIKEN); Kazuhisa Shibata (RIKEN); Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST)); Masataka Goto (National Institute of Advanced Industr",vkmcheung@gmail.com; lana.okuma@riken.jp; kazuhisa.shibata@riken.jp; k.tsukuda@aist.go.jp; m.goto@aist.go.jp; furuya@csl.sony.co.jp,https://drive.google.com/file/d/1xhJ5KjTYnpdTQB3oFl5CYxaLjUd1DVl3/view,https://docs.google.com/presentation/d/1JPaaTUEg8eS2EbAodC9bQH1iBeKAgWs_/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1UndCVJ8d5hWou47TfEBwovykn9yq7sQc/view,https://drive.google.com/file/d/1YV45v4F9uMKuPcNsJDJ3FiK2c6eJS2et/view,2,https://archives.ismir.net/ismir2023/paper/000022.pdf,2,5,22,000022.pdf,Vincent K.M. Cheung,vkmcheung@gmail.com,Human-centered MIR -> user-centered evaluation,"Human-centered MIR -> human-computer interaction; Human-centered MIR -> user behavior analysis and mining, user modeling; Knowledge-driven approaches to MIR -> cognitive MIR",https://drive.google.com/uc?export=view&id=1xhJ5KjTYnpdTQB3oFl5CYxaLjUd1DVl3,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTDKFFY,p2-06-cheung
68,Dual Attention-based Multi-scale Feature Fusion Approach for Dynamic Music Emotion Recognition,"Music Emotion Recognition (MER) refers to automatically extracting emotional information from music and predicting its perceived emotions, and it has social and psychological applications. This paper proposes a Dual Attention-based Multi-scale Feature Fus",Liyue Zhang ( Xi’an Jiaotong University)*; Xinyu Yang (Xi'an Jiaotong University); Yichi Zhang (Xi'an Jiaotong University); Jing Luo (Xi'an Jiaotong University),3121358019@stu.xjtu.edu.cn; yxyphd@mail.xjtu.edu.cn; datasonezyc@stu.xjtu.edu.cn; luojingl@stu.xjtu.edu.cn,https://drive.google.com/file/d/1mC18LEQOFkb3GyHxXFDdUf5puYgjv_f8/view ,https://docs.google.com/presentation/d/1Hka2DKYZV4xEwIxCoCrMnV2NppVZRlOD/edit?usp=sharing&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1HgMEA6LvG9R0TJFZAuBAFK7uCemTWkJf/view,https://drive.google.com/file/d/1xPhQIFxNHwkfqy90YJFTvgrIUIlAQ_wh/view,2,https://archives.ismir.net/ismir2023/paper/000023.pdf,2,6,23,000023.pdf,Liyue Zhang,3121358019@stu.xjtu.edu.cn,MIR tasks -> automatic classification,"MIR fundamentals and methodology -> music signal processing; Musical features and properties -> musical affect, emotion and mood",https://drive.google.com/uc?export=view&id=1mC18LEQOFkb3GyHxXFDdUf5puYgjv_f8,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGL7E7J,p2-07-zhang
72,Automatic Piano Transcription with Hierarchical Frequency-Time Transformer,"Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription.
This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content.
In this case, we may r",Keisuke Toyama (Sony Group Corporation)*; Taketo Akama (Sony CSL); Yukara Ikemiya (Sony Research); Yuhta Takida (Sony Group Corporation); WeiHsiang Liao (Sony Group Corporation); Yuki Mitsufuji (Sony Group Corporation),keisuke.toyama@sony.com; Taketo.Akama@sony.com; yukara.ikemiya@sony.com; yuta.takida@sony.com; weihsiang.liao@sony.com; Yuhki.Mitsufuji@sony.com,https://drive.google.com/file/d/1Flog5weyWDCqIryWhj9XDpDo2kmLajwq/view,https://drive.google.com/file/d/1SXdfz2nXUf78UXfIWt_1o6KpuJ34XNi9/view,https://drive.google.com/file/d/1VGhS9fshR57vmR2125mYzNEGbYGFmr49/view,https://drive.google.com/file/d/1eJKauSWyuJxlw920Kba_NSnEPg6hb_wE/view,2,https://archives.ismir.net/ismir2023/paper/000024.pdf,2,7,24,000024.pdf,Keisuke Toyama,keisuke.toyama@sony.com,MIR tasks -> music transcription and annotation,,https://drive.google.com/uc?export=view&id=1Flog5weyWDCqIryWhj9XDpDo2kmLajwq,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTDRA5C,p2-08-toyama
223,High-Resolution Violin Transcription using Weak Labels,"A descriptive transcription of a violin performance requires detecting not only the notes but also the fine-grained pitch variations, such as vibrato. Most existing deep learning methods for music transcription do not capture these variations and often ne",Nazif Can Tamer (Universitat Pompeu Fabra)*; Yigitcan Özer (International Audio Laboratories Erlangen); Meinard Müller (International Audio Laboratories Erlangen); Xavier Serra (Universitat Pompeu Fabra ),nazifcan.tamer@upf.edu; yigitcan.oezer@audiolabs-erlangen.de; meinard.mueller@audiolabs-erlangen.de; xavier.serra@upf.edu,https://drive.google.com/file/d/19PEwJtC-35XcbPaJy5ukN0uBoIzi3d1f/view,https://docs.google.com/presentation/d/1lXBcDlLiXWaqWgJsuYfGu0I4QfNjVUeB/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1KRLWGk5HcUFNG9bYsywrSugL_a8PVh3J/view,https://drive.google.com/file/d/1iBNLcgwzHEbulDBoDNAkAicD-WAITY4N/view,2,https://archives.ismir.net/ismir2023/paper/000025.pdf,2,8,25,000025.pdf,Nazif Can Tamer,nazifcan.tamer@upf.edu,MIR tasks -> music transcription and annotation,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> alignment, synchronization, and score following; Musical features and properties -> expression and performative aspects of music; Musical features and properties -> representations of music",https://drive.google.com/uc?export=view&id=19PEwJtC-35XcbPaJy5ukN0uBoIzi3d1f,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B67CE1F,p2-09-tamer
51,Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls,"We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. ",Lejun Min (Shanghai Jiao Tong University)*; Junyan Jiang (New York University Shanghai); Gus Xia (New York University Shanghai); Jingwei Zhao (National University of Singapore),aik2mlj@gmail.com; at2jjy@gmail.com; gxia@nyu.edu; jzhao@u.nus.edu,https://drive.google.com/file/d/1dGIexp74B_hpMGlqHfo5sfAk-9kHC3vP/view,https://docs.google.com/presentation/d/1iX7xho9P6HIQGKCWBIGty7tfpLav3JXx/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1LtBs1P-Uf63J208pU7ziT8o5PpmkHfz_/view,https://drive.google.com/file/d/1LpF3DpX_weJWjoczb27TLVUtPHTzrgbL/view,2,https://archives.ismir.net/ismir2023/paper/000026.pdf,2,9,26,000026.pdf,Lejun Min,aik2mlj@gmail.com,MIR tasks -> music generation,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing,https://drive.google.com/uc?export=view&id=1dGIexp74B_hpMGlqHfo5sfAk-9kHC3vP,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YGVMYCS,p2-10-min
104,The Coordinated Corpus of Popular Musics (CoCoPops): A Meta-Dataset of Melodic and Harmonic Transcriptions,"This paper introduces a new corpus, CoCoPops: The Coordinated Corpus of Popular Musics. The corpus can be considered a “meta corpus” in that it both extends and combines two existing corpora—the widely-used McGill Bill-
board corpus the and RS200 corpus. ",Claire Arthur (Georgia Institute of Technology)*; Nathaniel Condit-Schultz (Georgia Institute of Technology),claire.arthur@gatech.edu; natcs@gatech.edu,https://drive.google.com/file/d/12W7e61WVW3bqyfuKWISRz9M2D1UQ_xNb/view,https://drive.google.com/file/d/1KkDcqY3RPolBMEjfWE-7URSlQ3S1KqB2/view,https://drive.google.com/file/d/1Z0vMqYVaW7Hx_nSVl9ZtmYNbiUfJmKd2/view,https://drive.google.com/file/d/1BpgecJ0c7FvybDh1WR11NSPBqYqgOoHT/view,2,https://archives.ismir.net/ismir2023/paper/000027.pdf,2,10,27,000027.pdf,Claire Arthur,claire.arthur@gatech.edu,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Computational musicology -> digital musicology; Computational musicology -> systematic musicology; Knowledge-driven approaches to MIR -> cognitive MIR; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> melody and motives",https://drive.google.com/uc?export=view&id=12W7e61WVW3bqyfuKWISRz9M2D1UQ_xNb,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B67JV2M,p2-11-arthur
153,Towards computational music analysis for music therapy,The research field of music therapy has witnessed a rising interest in recent years to develop and employ computational methods to support therapists in their daily practice. While Music Information Retrieval (MIR) research has identified the area of heal,Anja Volk (Utrecht University)*; Tinka Veldhuis (Utrecht University); Katrien Foubert (LUCA School of Arts); Jos De Backer (LUCA School of Arts),A.Volk@uu.nl; T.Veldhuis@students.uu.nl; katrien.foubert@luca-arts.be; jos.debacker@luca-arts.be,https://drive.google.com/file/d/1Y2c00qrsceqIemNOanPAnBklWPEowTds/view,https://drive.google.com/file/d/1jsAeGXPpZqUKDMWd3QFAFPKxPeV8L5OQ/view?usp=share_link,https://drive.google.com/file/d/1PglOD8-VretOEUQOKD9uBkEvbW_NDwOg/view,https://drive.google.com/file/d/1oRXNzZemPVUNgX7VN0k6H4bnZY3aiJet/view?usp=sharing,2,https://archives.ismir.net/ismir2023/paper/000028.pdf,2,11,28,000028.pdf,Anja Volk,A.Volk@uu.nl,Applications,"Applications -> music and health, well-being and therapy",https://drive.google.com/uc?export=view&id=1Y2c00qrsceqIemNOanPAnBklWPEowTds,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B67NGNM,p2-12-volk
197,Timbre Transfer using Image-to-Image Denoising Diffusion Implicit Models,"Timbre transfer techniques aim at converting the sound of a musical piece generated by one instrument into the same one as if it was played by another instrument, while maintaining as much as possible the content in terms of musical characteristics such a",Luca Comanducci (Politecnico di Milano)*; Fabio Antonacci (Politecnico di Milano); Augusto Sarti (Politecnico di Milano),luca.comanducci@polimi.it; fabio.antonacci@polimi.it; augusto.sarti@polimi.it,https://drive.google.com/file/d/1qU345rZrSCXeeK_k3aylFUWrvOTuQFSW/view,https://drive.google.com/file/d/1he6IRIRvPzuDjXgwbvkLmQFxJT1lDH6W/view,https://drive.google.com/file/d/1h2BzyHiG6_5k1XCckQO4CnkWleWcmaCI/view,https://drive.google.com/file/d/10Y2cPu2ro6LmAIkvgdXX44MRmrSpSCRP/view,2,https://archives.ismir.net/ismir2023/paper/000029.pdf,2,12,29,000029.pdf,Luca Comanducci,luca.comanducci@polimi.it,"Musical features and properties -> timbre, instrumentation, and singing voice",MIR tasks -> music synthesis and transformation,https://drive.google.com/uc?export=view&id=1qU345rZrSCXeeK_k3aylFUWrvOTuQFSW,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410QDPUL,p2-13-comanducci
259,Correlation of EEG responses reflects structural similarity of choruses in popular music,Music structure analysis is a core topic in Music Information Retrieval and could be advanced through the inclusion of new data modalities. In this study we consider neural correlates of music structure processing using popular music - specifically chorus,Neha Rajagopalan (Stanford University)*; Blair Kaneshiro (Stanford University),neharaj@stanford.edu; blairbo@ccrma.stanford.edu,https://drive.google.com/file/d/1IEs2vHDAZxE76QV10AHpIX9h39tol-M_/view,https://drive.google.com/file/d/1eOFJuiWD0suR0TYxi9yCC01w-hcK0Isp/view,https://drive.google.com/file/d/1UjDa6qx3fJHgfKkmK5PSzSUfz0Q_90B9/view,https://drive.google.com/file/d/1xn8-GkvSdeBcQZve5kgEwb5eyUfkmO5t/view,2,https://archives.ismir.net/ismir2023/paper/000030.pdf,2,13,30,000030.pdf,Neha Rajagopalan,neharaj@stanford.edu,Knowledge-driven approaches to MIR -> cognitive MIR,"Human-centered MIR; MIR fundamentals and methodology -> multimodality; Musical features and properties -> structure, segmentation, and form",https://drive.google.com/uc?export=view&id=1IEs2vHDAZxE76QV10AHpIX9h39tol-M_,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGM0UJU,p2-14-rajagopalan
46,Chromatic Chords in Theory and Practice,"“Chromatic harmony” is seen as a fundamental part of (extended) tonal music in the Western classical tradition (c.1700–1900). It routinely features in core curricula. Yet even in this globalised and data-driven age, 1) there are significant gaps between h",Mark R H Gotham (Durham)*,mark.r.gotham@durham.ac.uk,https://drive.google.com/file/d/1vrmYxGG7OMYAndo77lzDq7XWkQ3IMGZm/view ,https://drive.google.com/file/d/1zAwTQceFmR3DYtAq6JVkF5pmRV25Reuu/view ,https://drive.google.com/file/d/16cyvIY6D-V582ZckDmzh1A1ZJZY1UioP/view,https://drive.google.com/file/d/1zsZG_DIAhhbZa4qyebWH91uv98M7Qmpf/view?usp=sharing,2,https://archives.ismir.net/ismir2023/paper/000031.pdf,2,14,31,000031.pdf,Mark R H Gotham,mark.r.gotham@durham.ac.uk,Knowledge-driven approaches to MIR -> computational music theory and musicology,"Computational musicology -> digital musicology; Computational musicology -> mathematical music theory; Computational musicology -> systematic musicology; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> rhythm, beat, tempo",https://drive.google.com/uc?export=view&id=1vrmYxGG7OMYAndo77lzDq7XWkQ3IMGZm,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J16E095,p2-15-gotham
145,BPS-Motif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic Music,"Intra-opus repeated pattern discovery in polyphonic symbolic music data has  challenges in both algorithm design and data annotation. To solve these challenges, we propose BPS-motif, a new symbolic music dataset containing the note-level annotation of mot",YO-WEI HSIAO (Academia Sinica); TZU-YUN Hung (National Taiwan Normal University); Tsung-Ping Chen (Academia Sinica); Li Su (Academia Sinica)*,willyhsiao@iis.sinica.edu.tw; hong.megan@gmail.com; tearfulcanon@yahoo.com.tw; lisu@iis.sinica.edu.tw,https://drive.google.com/file/d/1yVFoMUKa7i5jE94EA1b_tDCqQ-AhfRT2/view?usp=share_link,https://drive.google.com/file/d/1ylIClYOpST2lAkFnxhxy_bxQ_ZrCE2Sy/view,https://drive.google.com/file/d/1CIPZH9MrmtumY-M9GTEaTc9lNeyX2IGq/view,https://drive.google.com/file/d/1K2sK-hgZPRH-72jMbxbd1hk23j0FUISY/view?usp=share_link,3,https://archives.ismir.net/ismir2023/paper/000032.pdf,3,0,32,000032.pdf,Li Su,lisu@iis.sinica.edu.tw,MIR fundamentals and methodology -> symbolic music processing,MIR tasks -> pattern matching and detection,https://drive.google.com/uc?export=view&id=1yVFoMUKa7i5jE94EA1b_tDCqQ-AhfRT2,True,True,In Person,,,,https://slack.com/app_redirect?channel=C064MGM78JC,p3-01-su
81,Weakly Supervised Multi-Pitch Estimation Using Cross-Version Alignment,"Multi-pitch estimation (MPE), the task of detecting active pitches within a polyphonic music recording, has garnered significant research interest in recent years. Most state-of-the-art approaches for MPE are based on deep networks trained using pitch ann",Michael Krause (International Audio Laboratories Erlangen)*; Sebastian Strahl (Friedrich-Alexander-Universität Erlangen-Nürnberg); Meinard Müller (International Audio Laboratories Erlangen),michael.krause@audiolabs-erlangen.de; sebastian.strahl@fau.de; meinard.mueller@audiolabs-erlangen.de,https://drive.google.com/file/d/1ta1EFfmQaRw6K2M6CwRyOo-YkNPZ-LU0/view,https://drive.google.com/file/d/1so1PVXiBTU37EBsnO07FIP0CwmEN5imu/view,https://drive.google.com/file/d/1lGmk9yrSduurAmTnRTTu-POFNk1QwZC2/view,https://drive.google.com/file/d/16Dox9AzY9TJDz8mORKIF7bJ5t57DFtyn/view,3,https://archives.ismir.net/ismir2023/paper/000033.pdf,3,1,33,000033.pdf,Michael Krause,michael.krause@audiolabs-erlangen.de,MIR tasks -> music transcription and annotation,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> alignment, synchronization, and score following",https://drive.google.com/uc?export=view&id=1ta1EFfmQaRw6K2M6CwRyOo-YkNPZ-LU0,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE227EF,p3-02-krause
92,The Batik-plays-Mozart Corpus:  Linking Performance to Score to Musicological Annotations,"We present the Batik plays Mozart Corpus, a piano performance dataset
combining professional Mozart piano sonata performances with expert-labelled scores at a note-precise level. The performances originate from a recording by Viennese pianist Roland Batik",Patricia Hu (Johannes Kepler University)*; Gerhard Widmer (Johannes Kepler University),patricia.hu@jku.at; gerhard.widmer@jku.at,https://drive.google.com/file/d/1f4QHPE7FMDQO3v3qF2beiqsaujGwSPVw/view,https://drive.google.com/file/d/1DdBgLcfUCRaYq4ZWTXrxfJCyFeX6SXHb/view,https://drive.google.com/file/d/13CtnTbgJlUT0Wz_DtR5x0o4EHEo3DJmk/view,https://drive.google.com/file/d/1ExVUQUbUIa0FRn_fsD_BJrw0C0l6lu8q/view,3,https://archives.ismir.net/ismir2023/paper/000034.pdf,3,2,34,000034.pdf,Patricia Hu,patricia.hu@jku.at,"Evaluation, datasets, and reproducibility","Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> reproducibility; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> expression and performative aspects of music",https://drive.google.com/uc?export=view&id=1f4QHPE7FMDQO3v3qF2beiqsaujGwSPVw,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J16Q4LX,p3-03-hu
100,Mono-to-stereo through parametric stereo generation,"Generating a stereophonic presentation from a monophonic audio signal is a challenging open task, especially if the goal is to obtain a realistic spatial imaging with a specific panning of sound elements. In this work, we propose to convert mono to stereo",Joan Serra (Dolby Laboratories)*; Davide Scaini (Dolby Laboratories); Santiago Pascual (Dolby Laboratories); Daniel Arteaga (Dolby Laboratories); Jordi Pons (Dolby Laboratories); Jeroen Breebaart (Dolby Laboratories); Giulio Cengarle (Dolby Laboratories),joan.serra@dolby.com; davide.scaini@dolby.com; santiago.pascual@dolby.com; daniel.arteaga@dolby.com; idrojsnop@gmail.com; jeroen.breebaart@dolby.com; giulio.cengarle@dolby.com,https://drive.google.com/file/d/1qQz-iD-vnFqK9jd-WOnGhFUbsiWQ_oi5/view,https://drive.google.com/file/d/1i7Piz7qfMK495zyorkxV7626LITEohCr/view,https://drive.google.com/file/d/1jV46u1KFaKVfBtpPoG-FTR1gAckO7j-c/view,https://drive.google.com/file/d/10yWhpi-LUE35EOagxWXY4NNKLbGsqB4K/view,3,https://archives.ismir.net/ismir2023/paper/000035.pdf,3,3,35,000035.pdf,Joan Serra,joan.serra@dolby.com,MIR tasks -> music synthesis and transformation,MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR fundamentals and methodology -> music signal processing; MIR tasks -> music generation,https://drive.google.com/uc?export=view&id=1qQz-iD-vnFqK9jd-WOnGhFUbsiWQ_oi5,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J16TLFR,p3-04-serra
101,From West to East: Who can understand the music of the others better?,"Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related style","Charilaos Papaioannou (School of ECE, National Technical University of Athens)*; Emmanouil Benetos (Queen Mary University of London); Alexandros Potamianos (National Technical University of Athens)",cpapaioan@mail.ntua.gr; emmanouil.benetos@qmul.ac.uk; potam@central.ntua.gr,https://drive.google.com/file/d/1Q5im0b7JCRsIYxzLriy1pW-zu_vqpUOt/view,https://drive.google.com/file/d/1zm48__646MtyzEy_MpQoevBHw3QOXu0Y/view,https://drive.google.com/file/d/1ALR54HMKN2zggi7c8UaT7U0rxIb26i-E/view,https://drive.google.com/file/d/1vs-muvttYYR82INpXW2rz08gmXI4w1eu/view,3,https://archives.ismir.net/ismir2023/paper/000036.pdf,3,4,36,000036.pdf,Charilaos Papaioannou,cpapaioan@mail.ntua.gr,Knowledge-driven approaches to MIR -> computational ethnomusicology,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification,https://drive.google.com/uc?export=view&id=1Q5im0b7JCRsIYxzLriy1pW-zu_vqpUOt,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK62693,p3-05-papaioannou
85,On the Performance of Optical Music Recognition in the Absence of Specific Training Data,"Optical Music Recognition (OMR) has become a popular technology to retrieve information present in musical scores in conjunction with the increasing improvement of Deep Learning techniques, which represent the state-of-the-art in the field. However, its e",Juan Carlos Martinez-Sevilla (University of Alicante)*; Adrián Roselló (Universidad de Alicante); David Rizo (Universidad de Alicante); Jorge Calvo-Zaragoza (University of Alicante),jcmartinez@dlsi.ua.es; adrian.rosello@ua.es; drizo@dlsi.ua.es; jcalvo@dlsi.ua.es,https://drive.google.com/file/d/1w9jaxLD0ZFatSOtQIh_czdrrtShP9pfH/view,https://drive.google.com/file/d/1uxBA5g4qOZKuGlxPPZtUPsWY19_GDr3y/view,https://drive.google.com/file/d/1eZPZUrFE9AUGQd_bba8ao7-KkZheW6HE/view,https://drive.google.com/file/d/1oXTE-pEwab37bpwkZLtolcKUZCfhrmcc/view,3,https://archives.ismir.net/ismir2023/paper/000037.pdf,3,5,37,000037.pdf,Juan Carlos Martinez-Sevilla,jcmartinez@dlsi.ua.es,MIR tasks -> optical music recognition,"Applications -> music retrieval systems; Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> evaluation methodology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music",https://drive.google.com/uc?export=view&id=1w9jaxLD0ZFatSOtQIh_czdrrtShP9pfH,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B68PN73,p3-06-martinez-sevilla
113,Composer's Assistant: An Interactive Transformer for Multi-Track MIDI Infilling,"We introduce Composer’s Assistant, a system for interactive human-computer composition in the REAPER digital audio workstation. We consider the task of multi-track MIDI infilling when arbitrary track-measures have been deleted from a contiguous slice of m",Martin E Malandro (Sam Houston State University)*,malandro@shsu.edu,https://drive.google.com/file/d/1_MuEV4JpOSaYscTf3Vm3f10g07XRwyU7/view,https://docs.google.com/presentation/d/1dH-663bQnZnxtrM-OwIvkEYbcE6XfoS8/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1R7ShCQyyIiT004viq1WSHOZGENpn3MdV/view,https://drive.google.com/file/d/1PFs40Nj02piatufF5cFyj_rGrnxr51UQ/view,3,https://archives.ismir.net/ismir2023/paper/000038.pdf,3,6,38,000038.pdf,Martin E Malandro,malandro@shsu.edu,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,Human-centered MIR -> human-computer interaction; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music generation,https://drive.google.com/uc?export=view&id=1_MuEV4JpOSaYscTf3Vm3f10g07XRwyU7,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTFDAG6,p3-07-malandro
114,"The FAV Corpus: An audio dataset of favorite pieces and excerpts, with formal analyses and music theory descriptors","We introduce a novel audio corpus, the FAV Corpus, of over 400 favorite musical excerpts and pieces, formal analyses, and free-response comments. In a survey, 140 American university students (mostly music majors) were asked to provide three of their favo",Ethan Lustig (Ethan Lustig)*; David Temperley (Eastman School of Music),ethan.s.lustig@gmail.com; dtemperley@esm.rochester.edu,https://drive.google.com/file/d/1nbsc_C380nLwUlgZQJRDJ7Pme8oIapqI/view,https://docs.google.com/presentation/d/1SPxtuSacSrpEatQ66HzdDlZijOIqav2G/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1bdzme3wv93SkvLyotfnMp72f8tt2jE-s/view,https://drive.google.com/file/d/1r5OfnK5jaNHBKre4Wk9uEkbgGr5ZmASG/view,3,https://archives.ismir.net/ismir2023/paper/000039.pdf,3,7,39,000039.pdf,Ethan Lustig,ethan.s.lustig@gmail.com,Knowledge-driven approaches to MIR -> cognitive MIR,"Human-centered MIR -> personalization; Human-centered MIR -> user-centered evaluation; Knowledge-driven approaches to MIR -> representations of music; Musical features and properties -> musical affect, emotion and mood",https://drive.google.com/uc?export=view&id=1nbsc_C380nLwUlgZQJRDJ7Pme8oIapqI,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE2NEQK,p3-08-lustig
117,LyricWhiz: Robust Multilingual Lyrics Transcription by Whispering to ChatGPT,"We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-fre",Le Zhuo (Beihang University); Ruibin Yuan (CMU)*; Jiahao Pan (HKBU); Yinghao MA (Queen Mary University of London); Yizhi Li (The University  of Sheffield); Ge Zhang (University of Michigan); Si Liu (Beihang University); Roger B. Dannenberg (School of Comp,zhuole1025@gmail.com; a43992899@gmail.com; fengshicherish@gmail.com; yinghao.ma@qmul.ac.uk; yizhi.li@sheffield.ac.uk; gezhang@umich.edu; liusi@buaa.edu.cn; rbd@cs.cmu.edu; fujie@baai.ac.cn; c.lin@sheffield.ac.uk; emmanouil.benetos@qmul.ac.uk; wenhuchen@uw,https://drive.google.com/file/d/1h5vskyrQIKUo9cmat3NHqZ1g_a6Uo0-9/view,https://drive.google.com/file/d/1Wqdk_zo2b95Yt5xxWadc4UP9qRPNSOmB/view,https://drive.google.com/file/d/1rx9VXZ4WlFb24S7awWdrD9cStrRUqLZs/view,https://drive.google.com/file/d/1jKPwGZ-emmV1pjh9bqaN0rIq3OrVZeUy/view,3,https://archives.ismir.net/ismir2023/paper/000040.pdf,3,8,40,000040.pdf,Ruibin Yuan,a43992899@gmail.com,MIR fundamentals and methodology -> lyrics and other textual data,"Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> novel datasets and use cases",https://drive.google.com/uc?export=view&id=1h5vskyrQIKUo9cmat3NHqZ1g_a6Uo0-9,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410RM5TN,p3-09-yuan
118,SOUNDS OUT OF PLÄCE? SCORE INDEPENDENT DETECTION OF CONSPICUOUS MISTAKE REGIONS IN MIDI PIANO PERFORMANCES,"In piano performance, some mistakes stand out to listeners, whereas others may go unnoticed. Former research concluded that the salience of mistakes depended on factors including their contextual appropriateness and a listener’s degree of familiarity to w",Alia Morsi (Universitat Pompeu Fabra)*; Kana Tatsumi (Nagoya Institute of Technology); Akira Maezawa (Yamaha Corporation); Takuya Fujishima (Yamaha Corporation); Xavier Serra (Universitat Pompeu Fabra ),alia.morsi@upf.edu; tatsumi@lee-lab.org; akira.maezawa@music.yamaha.com; takuya.fujishima@music.yamaha.com; xavier.serra@upf.edu,https://drive.google.com/file/d/1Cx0nL2HWni7ocLN0GUXBM3g-IZ0_oOAi/view?usp=share_link,https://drive.google.com/file/d/1CXo42JTbXW3fRGsSJBkP2cdSRMsykEpY/view,https://drive.google.com/file/d/1FNEeaNq4xjANe8Ui888VjxYaFHT3RO5M/view?usp=sharing,https://drive.google.com/file/d/15KebKdcDpidnwnwH_OwEdVeHXDBNZOnq/view,3,https://archives.ismir.net/ismir2023/paper/000041.pdf,3,9,41,000041.pdf,Alia Morsi,alia.morsi@upf.edu,Applications -> music training and education,"Evaluation, datasets, and reproducibility -> annotation protocols; Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> automatic classification; Musical features and properties -> expression and performative aspects of music",https://drive.google.com/uc?export=view&id=1Cx0nL2HWni7ocLN0GUXBM3g-IZ0_oOAi,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J17J2DV,p3-10-morsi
125,VampNet: Music Generation via Masked Acoustic Token Modeling,"We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. 
We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety o",Hugo F  Flores Garcia (Northwestern University)*; Prem Seetharaman (Northwestern University); Rithesh Kumar (Descript); Bryan Pardo (Northwestern University),hugofloresgarcia@u.northwestern.edu; prem@u.northwestern.edu; rithesh@descript.com; pardo@northwestern.edu,https://drive.google.com/file/d/1x7EP-4GCeM9fHEmrMHPozzRj9cldvYty/view,https://docs.google.com/presentation/d/1EsHnwqeJyJ0mZ1k7kSU5XMP644mQNzaZ/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1Bt4nhwqEbz0rlNkKwwv_77wA7ThABDDM/view,https://drive.google.com/file/d/1AYBS6z3-ZfjmS0YXALUbZfOAEj1m9cZ1/view,3,https://archives.ismir.net/ismir2023/paper/000042.pdf,3,10,42,000042.pdf,Hugo F  Flores Garcia,hugofloresgarcia@u.northwestern.edu,MIR tasks -> music generation,"Applications -> music composition, performance, and production; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> music synthesis and transformation",https://drive.google.com/uc?export=view&id=1x7EP-4GCeM9fHEmrMHPozzRj9cldvYty,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE3326P,p3-11-garcia
129,Expert and Novice Evaluations of Piano Performances: Criteria for Computer-Aided Feedback,"Learning an instrument can be rewarding, but is unavoidably a huge undertaking. Receiving constructive feedback on one’s playing is crucial for improvement. However, personal feedback from an expert instructor is seldom available on demand. The goal motiv",Yucong Jiang (University of Richmond)*,yjiang3@richmond.edu,https://drive.google.com/file/d/1uHn0_0BtDAzHyx2lewrp2xuAMO6BWhWM/view,https://drive.google.com/file/d/1J2WyHy2fLUKeJ9Qotj9DPPTMSW0FncDf/view,https://drive.google.com/file/d/117YbHrFSD6J4L5i0bxrzfF02jqLJNkMv/view,https://drive.google.com/file/d/1oPnn8Zl3n7s544Q3m1yvVAsfAfOQGx12/view,3,https://archives.ismir.net/ismir2023/paper/000043.pdf,3,11,43,000043.pdf,Yucong Jiang,yjiang3@richmond.edu,Applications -> music training and education,"Human-centered MIR -> music interfaces and services; MIR fundamentals and methodology -> lyrics and other textual data; MIR fundamentals and methodology -> music signal processing; MIR tasks -> alignment, synchronization, and score following; Musical features and properties -> expression and performative aspects of music",https://drive.google.com/uc?export=view&id=1uHn0_0BtDAzHyx2lewrp2xuAMO6BWhWM,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK6RDJ9,p3-12-jiang
147,Contrastive Learning for Cross-modal Artist Retrieval,"Music retrieval and recommendation applications often rely on content features encoded as embeddings, which provide vector representations of items in a music dataset. Numerous complementary embeddings can be derived from processing items originally repre",Andres Ferraro (Pandora/SiriusXM)*; Jaehun Kim (Pandora / SiriusXM); Andreas Ehmann (Pandora); Sergio Oramas (Pandora/SiriusXM); Fabien Gouyon (Pandora/SiriusXM),andresferraro@acm.org; jaehun.kim@siriusxm.com; aehmann@pandora.com; soramas@pandora.com; fgouyon@pandora.com,https://drive.google.com/file/d/1glaZPkG4r3hjat4frky06CPI2dJQu7Gv/view,https://drive.google.com/file/d/1pqW0gudcaYJBfm-fRgswBUW-GU0yFhyD/view,https://drive.google.com/file/d/1uiop7gVPLVJrYbGF-v71nbYQBKSMEta2/view,https://drive.google.com/file/d/13JN57EZ3ekgQYB9qN-lq9JQSsOIIoRJP/view,3,https://archives.ismir.net/ismir2023/paper/000044.pdf,3,12,44,000044.pdf,Andres Ferraro,andresferraro@acm.org,Applications -> music retrieval systems,Applications -> music recommendation and playlist generation; MIR fundamentals and methodology -> multimodality; MIR tasks -> similarity metrics; Musical features and properties -> representations of music,https://drive.google.com/uc?export=view&id=1glaZPkG4r3hjat4frky06CPI2dJQu7Gv,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK6UPHT,p3-13-ferraro
139,Repetition-Structure Inference with Formal Prototypes,"The concept of form in music encompasses a wide range of musical aspects, such as phrases and (hierarchical) segmentation, formal functions, cadences and voice-leading schemata, form templates, and repetition structure. In an effort towards a unified mode",Christoph Finkensiep (EPFL)*; Matthieu Haeberle (EPFL); Friedrich Eisenbrand (EPFL); Markus Neuwirth (Anton Bruckner Privatuniversität Linz); Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne),c.finkensiep@uva.nl; matthieu.haeberle@epfl.ch; friedrich.eisenbrand@epfl.ch; markus.neuwirth@bruckneruni.at; martin.rohrmeier@epfl.ch,https://drive.google.com/file/d/1_NLe1v6sPdZHtLwyG9EPrTKMr4NYPRxr/view,https://drive.google.com/file/d/1OIMWWwli7XnTp4FwGPhKI7CRkFU6eu4I/view,https://drive.google.com/file/d/1g6z6blYSx82jkFjDSXELbDiatLGdIWnC/view,https://drive.google.com/file/d/1Ss8ym3rjM_ntIrTuqqOmIz0uGkcHnFBA/view,3,https://archives.ismir.net/ismir2023/paper/000045.pdf,3,13,45,000045.pdf,Christoph Finkensiep,c.finkensiep@uva.nl,Computational musicology,"Computational musicology -> mathematical music theory; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> structure, segmentation, and form",https://drive.google.com/uc?export=view&id=1_NLe1v6sPdZHtLwyG9EPrTKMr4NYPRxr,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410S8TBN,p3-14-finkensiep
140,Algorithmic Harmonization of Tonal Melodies using Weighted Pitch Context Vectors,"Most melodies from the Western common practice period have a harmonic background, i.e., a succession of chords that fit the melody. In this paper we provide a novel approach to infer this harmonic background from the score notation of a melody. We first c",Peter Van Kranenburg (Utrecht University; Meertens Institute)*; Eoin J Kearns (Meertens Instituut),peter.van.kranenburg@meertens.knaw.nl; eoin.kearns@meertens.knaw.nl,https://drive.google.com/file/d/1z4Ty51ZGrqT_IOXD08dqGK99PE3pHy-a/view,https://drive.google.com/file/d/1lHkyLI3JsvIiMtgupLxP5PuRJ4HgwMNB/view,https://drive.google.com/file/d/1fti4fr4-IrECum8xM05AGxKQbzfPEd5H/view,https://drive.google.com/file/d/1JZegifNNWqpB8I8YSPIMA7B3XB6gssy8/view,3,https://archives.ismir.net/ismir2023/paper/000046.pdf,3,14,46,000046.pdf,Peter Van Kranenburg,peter.van.kranenburg@meertens.knaw.nl,Musical features and properties -> melody and motives,"Computational musicology -> digital musicology; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR tasks -> music generation; Musical features and properties -> harmony, chords and tonality",https://drive.google.com/uc?export=view&id=1z4Ty51ZGrqT_IOXD08dqGK99PE3pHy-a,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J185MD5,p3-15-kranenburg
142,Text-to-lyrics generation with image-based semantics and reduced risk of plagiarism,"This paper proposes a text-to-lyrics generation method, aiming to provide lyric writing support by suggesting the generated lyrics to users who struggle to find the right words to convey their message. Previous studies on lyrics generation have focused on",Kento Watanabe (National Institute of Advanced Industrial Science and Technology (AIST))*; Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)),kento.watanabe@aist.go.jp; m.goto@aist.go.jp,https://drive.google.com/file/d/1DWTDGJ_tm3_Bcyy15p7Nd30HtcOWXSd7/view,https://docs.google.com/presentation/d/1684DsuLAJCz4RzZE8tC_E9dMJ6ID_2Ri/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1Jg01Ghr0AaE6RPHudgG1cW6zqYC9J_hj/view,https://drive.google.com/file/d/1le4B70DjdhgRRPBBy7prQnoos-dVUOXB/view,3,https://archives.ismir.net/ismir2023/paper/000047.pdf,3,15,47,000047.pdf,Kento Watanabe,kento.watanabe@aist.go.jp,MIR fundamentals and methodology -> lyrics and other textual data,"MIR fundamentals and methodology -> multimodality; MIR fundamentals and methodology -> web mining, and natural language processing",https://drive.google.com/uc?export=view&id=1DWTDGJ_tm3_Bcyy15p7Nd30HtcOWXSd7,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTGDX1U,p3-16-watanabe
219,LP-MusicCaps: LLM-Based Pseudo Music Captioning,"Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face chall","Seungheon Doh (KAIST)*; Keunwoo Choi (Gaudio Lab, Inc.); Jongpil Lee (Neutune); Juhan Nam (KAIST)",seungheondoh@kaist.ac.kr; keunwoo@gaudiolab.com; jongpillee@neutune.com; juhan.nam@kaist.ac.kr,https://drive.google.com/file/d/1PeK7SflyMaoIyGKdQQ5xnEVKslx39qqp/view?usp=sharing,https://drive.google.com/file/d/1Po-oyxbz_jj2Q_GI7Z6iaq33io8uWetz/view,https://drive.google.com/file/d/14lE01yNAmpBWGastr-Ve09Cz1YSCiRRH/view?usp=share_link,https://drive.google.com/file/d/1COeBiC463MUcyj5fiL2FCvYrt7-DROaX/view?usp=share_link,4,https://archives.ismir.net/ismir2023/paper/000048.pdf,3,0,48,000048.pdf,Seungheon Doh,seungheondoh@kaist.ac.kr,"MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",MIR fundamentals and methodology -> multimodality; MIR tasks -> automatic classification,https://drive.google.com/uc?export=view&id=1PeK7SflyMaoIyGKdQQ5xnEVKslx39qqp,True,True,In Person,,,,https://slack.com/app_redirect?channel=C063VK7DBSR,p4-01-doh
146,A Repetition-based Triplet Mining Approach for Music Segmentation,"Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and ther",Morgan Buisson (Telecom-Paris)*; Brian McFee (New York University); Slim Essid (Telecom Paris - Institut Polytechnique de Paris); Helene-Camille Crayencour (CNRS),morgan.buisson76@gmail.com; brian.mcfee@nyu.edu; slim.essid@telecom-paristech.fr; helene.camille.crayencour@gmail.com,https://drive.google.com/file/d/1R7OXYDdiBnHw0MOzYYS22E5I0BzAsudD/view,https://drive.google.com/file/d/1iJMqKCQ62h0s0zs0K_zAWeJF6A9stqMa/view,https://drive.google.com/file/d/1u-UNDElMDqS_kKycvYlk-SzGI-pOCQHW/view,https://drive.google.com/file/d/17MzD1-iUXo7XGH97XV0-A-ntlQ2eQ6_j/view,4,https://archives.ismir.net/ismir2023/paper/000049.pdf,3,1,49,000049.pdf,Morgan Buisson,morgan.buisson76@gmail.com,"Musical features and properties -> structure, segmentation, and form",Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> pattern matching and detection; Musical features and properties -> representations of music,https://drive.google.com/uc?export=view&id=1R7OXYDdiBnHw0MOzYYS22E5I0BzAsudD,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTGLTK8,p4-02-buisson
87,Predicting Music Hierarchies with a Graph-Based Neural Decoder,"This paper describes a data-driven framework to parse musical sequences into dependency trees, which are hierarchical structures used in music cognition research and music analysis. The parsing involves two steps. First, the input sequence is passed throu",Francesco Foscarin (Johannes Kepler University Linz)*; Daniel Harasim (École Polytechnique Fédérale de Lausanne); Gerhard Widmer (Johannes Kepler University),francesco.foscarin@jku.at; daniel.harasim@mail.de; gerhard.widmer@jku.at,https://drive.google.com/file/d/1RDdU2rt3vymAbrxAePGUMcqIdsb9alVR/view,https://drive.google.com/file/d/1MLCi_M9oXIRII9bKkLCNzDqTgaC-4VMx/view,https://drive.google.com/file/d/1nMXueZAbPiQ7mt4ULaRE4rKCIOc2nqb3/view,https://drive.google.com/file/d/1L5mXxa9Uqnaw5ccYK7Yc-lhKO5DKiRa6/view,4,https://archives.ismir.net/ismir2023/paper/000050.pdf,3,2,50,000050.pdf,Francesco Foscarin,francesco.foscarin@jku.at,MIR fundamentals and methodology -> symbolic music processing,"Computational musicology -> digital musicology; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> melody and motives; Musical features and properties -> structure, segmentation, and form",https://drive.google.com/uc?export=view&id=1RDdU2rt3vymAbrxAePGUMcqIdsb9alVR,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTGQA5C,p4-03-foscarin
133,Stabilizing Training with Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation with Weakly Aligned Targets,"Soft dynamic time warping (SDTW) is a differentiable loss function that allows for training neural networks from weakly aligned data. Typically, SDTW is used to iteratively compute and refine soft alignments that compensate for temporal deviations between",Johannes Zeitler (International Audio Laboratories Erlangen)*; Simon Deniffel (International Audio Laboratories Erlangen); Michael Krause (International Audio Laboratories Erlangen); Meinard Müller (International Audio Laboratories Erlangen),johannes.zeitler@audiolabs-erlangen.de; simon.deniffel@fau.de; michael.krause@audiolabs-erlangen.de; meinard.mueller@audiolabs-erlangen.de,https://drive.google.com/file/d/1-78moijWNgGdVggNJ7Y9-m0Zf-urbrCc/view,https://drive.google.com/file/d/1KgOMTRro4Y0yzKAU9qPk7tdfTIS7kmQg/view,https://drive.google.com/file/d/1CJoFlJz2D5RNrZGTpc38z21k2hmv8GNw/view,https://drive.google.com/file/d/1MLqN1GfLKbUXUNEh7GwFKx8cZ8N1hV3E/view,4,https://archives.ismir.net/ismir2023/paper/000051.pdf,3,3,51,000051.pdf,Johannes Zeitler,johannes.zeitler@audiolabs-erlangen.de,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,"MIR tasks -> alignment, synchronization, and score following; MIR tasks -> music transcription and annotation",https://drive.google.com/uc?export=view&id=1-78moijWNgGdVggNJ7Y9-m0Zf-urbrCc,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J18NFHD,p4-04-zeitler
149,Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song,"In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, th",Danbinaerin Han (Sogang Univ.); Rafael Caro Repetto (Kunstuniversität Graz); Dasaem Jeong (Sogang University)*,naerin71@sogang.ac.kr; rafael.caro-repetto@kug.ac.at; dasaemj@sogang.ac.kr,https://drive.google.com/file/d/15pD_MIwtse6iGJYq87yB3clpOQF1yXUb/view,https://drive.google.com/file/d/1Tx5Bx2ucf2-M9_Mc8wqFxEss8TtLuo_6/view?usp=share_link,https://drive.google.com/file/d/17T2UdKBXGrBPEkuGEqXvNkIK7ZzyRdtC/view,https://drive.google.com/file/d/1quJ5o6fdJ_S2CUXY8j9UJUsQOMdc2wGb/view,4,https://archives.ismir.net/ismir2023/paper/000052.pdf,3,4,52,000052.pdf,Dasaem Jeong,dasaemj@sogang.ac.kr,Knowledge-driven approaches to MIR -> computational ethnomusicology,Applications -> digital libraries and archives; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Musical features and properties -> melody and motives,https://drive.google.com/uc?export=view&id=15pD_MIwtse6iGJYq87yB3clpOQF1yXUb,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGPFXDE,p4-05-jeong
204,Singer Identity Representation Learning using Self-Supervised Techniques,"
Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoder","Bernardo Torres (Telecom Paris, Institut polytechnique de Paris)*; Stefan Lattner (Sony CSL); Gaël Richard (Telecom Paris, Institut polytechnique de Paris)",bernardo.torres@telecom-paris.fr; stefan.lattner@sony.com; gael.richard@telecom-paris.fr,https://drive.google.com/file/d/1r37TEwXvewpSGLyZrd4H5H5x3qy3zoQG/view,https://drive.google.com/file/d/1HFBDu5KoNXU-EOArg0vTfKmpN0XzKmd1/view?usp=share_link,https://drive.google.com/file/d/1B3Wp9yCZoTjOkhRhx831DlTksG2ErIM4/view,https://drive.google.com/file/d/1o44VOuMP9sJ60tGerTcJSSM-JyvkUmk2/view,4,https://archives.ismir.net/ismir2023/paper/000053.pdf,3,5,53,000053.pdf,Bernardo Torres,bernardo.torres@telecom-paris.fr,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,"Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> indexing and querying; MIR tasks -> music synthesis and transformation; MIR tasks -> similarity metrics; Musical features and properties -> timbre, instrumentation, and singing voice",https://drive.google.com/uc?export=view&id=1r37TEwXvewpSGLyZrd4H5H5x3qy3zoQG,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK80NP7,p4-06-torres
154,ON THE EFFECTIVENESS OF SPEECH SELF-SUPERVISED LEARNING FOR MUSIC,"Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained o",Yinghao MA (Queen Mary University of London)*; Ruibin Yuan (CMU); Yizhi Li (The University  of Sheffield); Ge Zhang (University of Michigan); Chenghua Lin (University of Sheffield); Xingran Chen (University of Michigan); Anton Ragni (University of Sheffie,yinghao.ma@qmul.ac.uk; a43992899@gmail.com; yizhi.li@sheffield.ac.uk; gezhang@umich.edu; c.lin@sheffield.ac.uk; chenxran@umich.edu; a.ragni@sheffield.ac.uk; hanzhiy@andrew.cmu.edu; emmanouil.benetos@qmul.ac.uk; n.g.gyenge@sheffield.ac.uk; Ruibo.Liu.GR@dar,https://drive.google.com/file/d/10mpCdKtSk5Yoru88f_Xb9G9ws8hWff3Z/view,https://drive.google.com/file/d/1HH2SDoSjd8NKioAlgVG-nJ5q2Q1TyGDH/view?usp=sharing ,https://drive.google.com/file/d/1_LgOUP9QGXuJMS61lIa-LVW3U_SRovOP/view,https://drive.google.com/file/d/1RqHXshWTsX5c4giQIs3sdUJXeG1Rn17I/view,4,https://archives.ismir.net/ismir2023/paper/000054.pdf,3,6,54,000054.pdf,Yinghao MA,yinghao.ma@qmul.ac.uk,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,Knowledge-driven approaches to MIR -> representations of music; Musical features and properties -> representations of music,https://drive.google.com/uc?export=view&id=10mpCdKtSk5Yoru88f_Xb9G9ws8hWff3Z,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6AMLG1,p4-07-ma
150,Transformer-based beat tracking with low-resolution encoder and high-resolution decoder,"In this paper, we address the beat tracking task which is to predict beat times corresponding to the input audio. Due to the long sequential inputs, it is still challenging to model the global structure efficiently and to deal with the data imbalance betw",Tian Cheng (National Institute of Advanced Industrial Science and Technology (AIST))*; Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)),tian.cheng@aist.go.jp; m.goto@aist.go.jp,https://drive.google.com/file/d/1Ui91-lbVqm_xEI-oEa0ZmS5DOznR5hdi/view,https://drive.google.com/file/d/1LykqJhYimwUyVR94CJBEZE06_gAwFnOv/view,https://drive.google.com/file/d/1hPkF6xVjUmG1Y_yaqI3bZHbbSRSbS6C2/view,https://drive.google.com/file/d/159NbL0joMwLVn2icPdD_E8jgBtBWstvk/view,4,https://archives.ismir.net/ismir2023/paper/000055.pdf,3,7,55,000055.pdf,Tian Cheng,tian.cheng@aist.go.jp,"Musical features and properties -> rhythm, beat, tempo",,https://drive.google.com/uc?export=view&id=1Ui91-lbVqm_xEI-oEa0ZmS5DOznR5hdi,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGPR77S,p4-08-cheng
156,Adding Descriptors to Melodies Improves Pattern Matching: A Study on Slovenian Folk Songs,The objective of pattern-matching topics is to gain insights into repetitive patterns within or across various music genres and cultures. This approach aims to shed light on the recurring instances present in diverse musical traditions. The paper presents,"Vanessa Nina Borsan (Université de Lille)*; Mathieu Giraud (CNRS, Université de Lille); Richard Groult (Université de Rouen Normandie); Thierry Lecroq (Université de Rouen Normandie )",vanessanina.borsan@univ-lille.fr; mathieu@algomus.fr; richard.groult@univ-rouen.fr; thierry.lecroq@univ-rouen.fr,https://drive.google.com/file/d/126Ahz0VboSi0Q92hJ9CKcnW9xLZW0AKQ/view,https://drive.google.com/file/d/1bcK02B0QvzxDqayHYDCPRFk9gm0dctro/view,https://drive.google.com/file/d/1KnhvnhEOK0CZFBallZ1QasV1vJyWU3-p/view,https://drive.google.com/file/d/1ICMtuG-1wFvmkPbjCbwh0HQLGOoXM-BR/view,4,https://archives.ismir.net/ismir2023/paper/000056.pdf,3,8,56,000056.pdf,Vanessa Nina Borsan,vanessanina.borsan@univ-lille.fr,Knowledge-driven approaches to MIR -> computational ethnomusicology,"Computational musicology; Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> pattern matching and detection; Musical features and properties -> melody and motives",https://drive.google.com/uc?export=view&id=126Ahz0VboSi0Q92hJ9CKcnW9xLZW0AKQ,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410TEZHA,p4-09-borsan
159,How Control and Transparency for Users Could Improve Artist Fairness in Music Recommender Systems,"As streaming services have become a main channel for music consumption, they significantly impact various stakeholders: users, artists who provide music, and other professionals working in the music industry. Therefore, it is essential to consider all sta",Karlijn Dinnissen (Utrecht University)*; Christine Bauer (Paris Lodron University Salzburg),k.dinnissen@uu.nl; christine.bauer@plus.ac.at,https://drive.google.com/file/d/1TFaGAyaFQK70F5rd7eXXeu-AlrKcsrzc/view,https://drive.google.com/file/d/12rWDR-MwXe02O4QL4g4Jfx2w-Zee1ue4/view,https://drive.google.com/file/d/1eL3Z3KjS4tNX58UGKSDJBmNCJVt3aRrr/view,https://drive.google.com/file/d/1DU_DRny34LO8VqB7JAsZVh-2XCzbr4h5/view,4,https://archives.ismir.net/ismir2023/paper/000057.pdf,3,9,57,000057.pdf,Karlijn Dinnissen,k.dinnissen@uu.nl,Human-centered MIR,Human-centered MIR -> human-computer interaction; Human-centered MIR -> music interfaces and services; Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR tools and technologies,https://drive.google.com/uc?export=view&id=1TFaGAyaFQK70F5rd7eXXeu-AlrKcsrzc,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTHGFHC,p4-10-dinnissen
165,Towards a New Interface for Music Listening: A User Experience Study on YouTube,"In light of the enduring success of music streaming services, it is noteworthy that an increasing number of users are positively gravitating toward YouTube as their preferred platform for listening to music. YouTube differs from traditional music streamin",Ahyeon Choi (Seoul National University)*; Eunsik Shin (Seoul National University); Haesun Joung (Seoul National University); Joongseek Lee (Seoul National University); Kyogu Lee (Seoul National University),chah0623@snu.ac.kr; esshin@snu.ac.kr; gotjs3841@snu.ac.kr; joonlee8@snu.ac.kr; kglee@snu.ac.kr,https://drive.google.com/file/d/1iJq_hG7isweMKvYvZluLrr9eTSThcwf2/view,https://drive.google.com/file/d/1wwl6z_9B-Gn4dZwKI18n_bgtzZikXjyk/view?usp=sharing,https://drive.google.com/file/d/1oHJYmdzFIa4qVWUCPl3RqvFNDsIZ2UCC/view,https://drive.google.com/file/d/1OhjCgBZQFMCRUtI_knEe9dDjBhJwldma/view,4,https://archives.ismir.net/ismir2023/paper/000058.pdf,3,10,58,000058.pdf,Ahyeon Choi,chah0623@snu.ac.kr,Human-centered MIR -> music interfaces and services,"Applications -> music videos, multimodal music systems; Human-centered MIR -> human-computer interaction; Human-centered MIR -> user behavior analysis and mining, user modeling; MIR fundamentals and methodology -> multimodality",https://drive.google.com/uc?export=view&id=1iJq_hG7isweMKvYvZluLrr9eTSThcwf2,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410TLVDJ,p4-11-choi
236,FiloBass: A Dataset and Corpus Based Study of Jazz Basslines,"We present FiloBass: a novel corpus of music scores and annotations which focuses on the important but often overlooked role of the double bass in jazz accompaniment. Inspired by recent works that shed light on the role of the soloist, we offer a collecti",Xavier Riley (C4DM)*; Simon Dixon (Queen Mary University of London),j.x.riley@qmul.ac.uk; s.e.dixon@qmul.ac.uk,https://drive.google.com/file/d/1YpN38YW55mYs9E4A7jGfwGCr0vyC-Xug/view,https://drive.google.com/file/d/1V_r9FwmTwgE9822y5QIqeiKPx6VIg0vf/view,https://drive.google.com/file/d/1waVg_oTy_OJxKRZ-2X_9ifOnLd2Kk4m9/view,https://drive.google.com/file/d/1WCEw7f4sB5C7c6Sd0xIecmNcGYVW2-an/view,4,https://archives.ismir.net/ismir2023/paper/000059.pdf,3,11,59,000059.pdf,Xavier Riley,j.x.riley@qmul.ac.uk,Computational musicology -> digital musicology,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> music transcription and annotation",https://drive.google.com/uc?export=view&id=1YpN38YW55mYs9E4A7jGfwGCr0vyC-Xug,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGQ7GMN,p4-12-riley
170,Comparing Texture in Piano Scores,"In this paper, we propose four different approaches to quantify similarities of compositional texture in symbolically encoded piano music. A melodic contour or harmonic progression can be shaped into a wide variety of different rhythms, densities, or comb","Louis Couturier (MIS, Université de Picardie Jules Verne)*; Louis Bigo (Université de Lille); Florence Leve (Université de Picardie Jules Verne - Lab. MIS - Algomus)",louis.couturier@u-picardie.fr; louis.bigo@univ-lille.fr; Florence.Leve@u-picardie.fr,https://drive.google.com/file/d/1hk8eiCacdB8iR0ZwJu-c5UjEEZ96MJ-w/view,https://drive.google.com/file/d/1e5nisQSGH6XQfI1ZqVnOT4GnuO48W4Tb/view,https://drive.google.com/file/d/1Ve5XrBWIIW976IRWGxgCD04HwEDX1MXM/view,https://drive.google.com/file/d/1osooaWw-wYuQdjnUAxqY_R1JsaEi3Av3/view,4,https://archives.ismir.net/ismir2023/paper/000060.pdf,3,12,60,000060.pdf,Louis Couturier,louis.couturier@u-picardie.fr,MIR tasks -> similarity metrics,Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties,https://drive.google.com/uc?export=view&id=1hk8eiCacdB8iR0ZwJu-c5UjEEZ96MJ-w,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGQABAL,p4-13-couturier
52,Introducing Anonymous to leverage the dataframe for processing and analyzing notated music on a very large scale,"As corpora of digital musical scores continue to grow, the need for research tools capable of manipulating such data efficiently, with an intuitive interface, and support for a diversity of file formats, becomes increasingly pressing. In response, this pa",Johannes Hentschel (École Polytechnique Fédérale de Lausanne)*; Andrew McLeod (Fraunhofer IDMT); Yannis Rammos (EPFL); Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne),johannes.hentschel@epfl.ch; andrew.mcleod@idmt.fraunhofer.de; yannis.rammos@epfl.ch; martin.rohrmeier@epfl.ch,https://drive.google.com/file/d/1h9TxRIy3JM9Xq-zU8aDLzL-SykIampwO/view,https://docs.google.com/presentation/d/1wJTdo_3MTTpm5mSSvFkkwjpx8DhJY_PM/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/18IuVXr59wiqy3Qou-zlpV0bHhNQdSLzo/view,https://drive.google.com/file/d/1byrcTFqiyyGpMDUoB_vyZYYNYTZNd2-1/view,4,https://archives.ismir.net/ismir2023/paper/000061.pdf,3,13,61,000061.pdf,Johannes Hentschel,johannes.hentschel@epfl.ch,Computational musicology -> digital musicology,"Applications -> digital libraries and archives; Evaluation, datasets, and reproducibility -> reproducibility; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> alignment, synchronization, and score following",https://drive.google.com/uc?export=view&id=1h9TxRIy3JM9Xq-zU8aDLzL-SykIampwO,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6BCKLH,p4-14-hentschel
161,Sequence-to-Sequence Network Training Methods for Automatic Guitar Transcription with Tokenized Outputs,We propose multiple methods for effectively training a sequence-to-sequence automatic guitar transcription model which uses tokenized music representation as an output. Our proposed method mainly consists of 1) a hybrid CTC-Attention model for sequence-to,Sehun Kim (Nagoya University)*; Kazuya Takeda (Nagoya University); Tomoki Toda (Nagoya University),kim.sehun@g.sp.m.is.nagoya-u.ac.jp; kazuya.takeda@nagoya-u.jp; tomoki@icts.nagoya-u.ac.jp,https://drive.google.com/file/d/1H-rOfrOD9_sWFJxM5q8-tM5ybEotVi6_/view,https://drive.google.com/file/d/1CDmkdE_W_CPDQc31KxK-bdfIqZ_-pnd7/view,https://drive.google.com/file/d/1nfV8jdToJLq9AjSHgV9299rCC7OtF3YC/view,https://drive.google.com/file/d/1L3Kgc0udyncXQhq8m61wF-U9RYA28pp-/view?usp=share_link,4,https://archives.ismir.net/ismir2023/paper/000062.pdf,3,14,62,000062.pdf,Sehun Kim,kim.sehun@g.sp.m.is.nagoya-u.ac.jp,MIR tasks -> music transcription and annotation,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; MIR fundamentals and methodology -> symbolic music processing,https://drive.google.com/uc?export=view&id=1H-rOfrOD9_sWFJxM5q8-tM5ybEotVi6_,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410U35QC,p4-15-kim
205,PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective,"In this paper, we address the problem of pitch estimation using self-supervised learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being ","Alain Riou (Télécom Paris, IP Paris, Sony CSL)*; Stefan Lattner (Sony CSL); Gaëtan Hadjeres (Sony CSL); Geoffroy Peeters (LTCI - Télécom Paris, IP Paris)",alain.riou@telecom-paris.fr; stefan.lattner@sony.com; gaetan.hadjeres@sony.com; geoffroy.peeters@telecom-paris.fr,https://drive.google.com/file/d/1IyGmxASQUh9-Vvokvlyh28TPkuWkhHTL/view,https://docs.google.com/presentation/d/12IZkbSdiISEfU0Gl9tevHd5Va_JLK_Tp/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1E38ECBFAzVt-Z1hvSyPoZb4TIShie6Yy/view,https://drive.google.com/file/d/1-cJpDm-wpwO4_qHAzU5GJlZCDrkxYltb/view,5,https://archives.ismir.net/ismir2023/paper/000063.pdf,4,0,63,000063.pdf,Alain Riou,alain.riou@telecom-paris.fr,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,MIR fundamentals and methodology -> music signal processing; MIR tasks -> music transcription and annotation,https://drive.google.com/uc?export=view&id=1IyGmxASQUh9-Vvokvlyh28TPkuWkhHTL,True,True,In Person,,,,https://slack.com/app_redirect?channel=C064MGQJY00,p5-01-riou
158,The Games We Play: Exploring The Impact of ISMIR on Musicology,"Throughout history, a consistent temporal and spatial gap has persisted between the inception of novel knowledge and technology and their subsequent adoption for extensive practical utilization. The article explores the dynamic interaction and exchange of","Vanessa Nina Borsan (Université de Lille)*; Mathieu Giraud (CNRS, Université de Lille); Richard Groult (Université de Rouen Normandie)",vanessanina.borsan@univ-lille.fr; mathieu@algomus.fr; richard.groult@univ-rouen.fr,https://drive.google.com/file/d/1qalUTMUdUqsIFLg8CNj01uDXllrInwEu/view,https://drive.google.com/file/d/1UD6VHMPMxPyTG18o1bPhFpNE81wkl5EJ/view,https://drive.google.com/file/d/1LeZsFax_jlsd1IS8-wmjaOhkaYhr9KlC/view,https://drive.google.com/file/d/1QBK2dKq6-Sz-UYq1lNEkOfkXlTQuwCQ4/view,5,https://archives.ismir.net/ismir2023/paper/000064.pdf,4,1,64,000064.pdf,Vanessa Nina Borsan,vanessanina.borsan@univ-lille.fr,Philosophical and ethical discussions,Computational musicology; Human-centered MIR -> human-computer interaction; Human-centered MIR -> user-centered evaluation,https://drive.google.com/uc?export=view&id=1qalUTMUdUqsIFLg8CNj01uDXllrInwEu,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6BM74Z,p5-02-borsan
176,Carnatic Singing Voice Separation Using Cold Diffusion on Training Data with Bleeding,"Supervised music source separation systems using deep learning are trained by minimizing a loss function between pairs of predicted separations and ground-truth isolated sources. However, open datasets comprising isolated sources are few, small, and restr",Genís Plaja-Roglans (Music Technology Group)*; Marius Miron (Universitat Pompeu Fabra); Adithi Shankar (Universitat Pompeu Fabra); Xavier Serra (Universitat Pompeu Fabra ),genis.plaja@upf.edu; miron.marius@gmail.com; adithishankar2406@gmail.com; xavier.serra@upf.edu,https://drive.google.com/file/d/1Yqe3wjveVzIkww9Oo4ljGlszdTPvRqkD/view,https://drive.google.com/file/d/1Er44BwZDEQolLDoWF_KXJw3nIw4g9mPf/view,https://drive.google.com/file/d/1_ANxPnfRtsoefXwSSdezWmwiY54AQx0P/view,https://drive.google.com/file/d/1g2JQFuTdYhuXCy3WOs9CG90ko3Yt4cj-/view,5,https://archives.ismir.net/ismir2023/paper/000065.pdf,4,2,65,000065.pdf,Genís Plaja-Roglans,genis.plaja@upf.edu,MIR tasks -> sound source separation,"Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Musical features and properties -> timbre, instrumentation, and singing voice",https://drive.google.com/uc?export=view&id=1Yqe3wjveVzIkww9Oo4ljGlszdTPvRqkD,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410UC97E,p5-03-plaja-roglans
179,Unveiling the Impact of Musical Factors in Judging a Song on First Listen: Insights from a User Survey,"When a user listens to a song for the first time, what musical factors (e.g., melody, tempo, and lyrics) influence the user's decision to like or dislike the song? An answer to this question would enable researchers to more deeply understand how people in",Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))*; Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST)); Masahiro Hamasaki (National Institute of Advanced Industrial Science and,k.tsukuda@aist.go.jp; t.nakano@aist.go.jp; masahiro.hamasaki@aist.go.jp; m.goto@aist.go.jp,https://drive.google.com/file/d/1k3TdguuXpFUM-8BYGn6q_stKRRaIbvua/view,https://drive.google.com/file/d/1dqR2PW5Tqb5BDpQxWAM9hugq7u6CHq53/view,https://drive.google.com/file/d/1B9kmqixemKtVnFleUFKGTboodh8t2FFr/view,https://drive.google.com/file/d/1NvvZYM7IVTorRFkOerJ_SMLAmO7tfeEC/view,5,https://archives.ismir.net/ismir2023/paper/000066.pdf,4,3,66,000066.pdf,Kosetsu Tsukuda,k.tsukuda@aist.go.jp,Human-centered MIR,,https://drive.google.com/uc?export=view&id=1k3TdguuXpFUM-8BYGn6q_stKRRaIbvua,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK9AMHT,p5-04-tsukuda
180,Towards Building a Phylogeny of Gregorian Chant Melodies,"The historical development of medieval plainchant melodies is an intriguing musicological topic that invites computational approaches to study it at scale. Plainchant melodies can be represented as strings from a limited alphabet, hence making it technica","Jan Haji_, jr. (Charles University)*; Gustavo Ballen (dos Reis research group, School of Biological and Behavioural Sciences, Queen Mary University of London); Klára Mühlová (Institute of Musicology, Faculty of Arts, Masaryk University); Hana Vlhová-Wörne",hajicj@ufal.mff.cuni.cz; gaballench@qmul.ac.uk; muhlova@mail.cuni.cz; vlhova@mua.cas.cz,https://drive.google.com/file/d/1A3OyouqQ6G7G0bXYC5zUaX0-67Kc6WNn/view,https://drive.google.com/file/d/1lg_PIDjHx8xSWPmiJOV-K7pIuMi8mXH6/view,https://drive.google.com/file/d/1njr07WsSeq2iWcgYfvi2NwhY5aE-RDkt/view,https://drive.google.com/file/d/1xmnYaY-xg1BuYEeW22XyKuO_h7mx1IwF/view,5,https://archives.ismir.net/ismir2023/paper/000067.pdf,4,4,67,000067.pdf,"Jan Hajič, jr.",hajicj@ufal.mff.cuni.cz,Computational musicology -> digital musicology,Applications -> digital libraries and archives; Knowledge-driven approaches to MIR -> computational ethnomusicology; Knowledge-driven approaches to MIR -> computational music theory and musicology,https://drive.google.com/uc?export=view&id=1A3OyouqQ6G7G0bXYC5zUaX0-67Kc6WNn,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGR54TA,p5-05-hajič
182,Audio Embeddings as Teachers for Music Classification,"Music classification has been one of the most popular tasks in the field of music information retrieval. With the development of deep learning models, the last decade has seen impressive improvements in a wide range of classification tasks. However, the i",Yiwei Ding (Georgia Institute of Technology)*; Alexander Lerch (Georgia Institute of Technology),yding402@gatech.edu; alexander.lerch@gatech.edu,https://drive.google.com/file/d/1hh6L-FIc_3Ptt_MrZHeFD8-uQw88a4tm/view,https://drive.google.com/file/d/1gbLLZ2Do2-u0GRmlG5HMxUDseG0rPOjV/view,https://drive.google.com/file/d/1jDmXNpEtDnYx13zWdgBWZxFv07--79Xw/view,https://drive.google.com/file/d/1yHE7wVW21ORQmtobdGp71QvM9lfIrxS6/view,5,https://archives.ismir.net/ismir2023/paper/000068.pdf,4,5,68,000068.pdf,Yiwei Ding,yding402@gatech.edu,Knowledge-driven approaches to MIR,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> automatic classification; Musical features and properties -> representations of music,https://drive.google.com/uc?export=view&id=1hh6L-FIc_3Ptt_MrZHeFD8-uQw88a4tm,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK9M545,p5-06-ding
183,ScorePerformer: Expressive Piano Performance Rendering with Fine-Grained Control,"We present ScorePerformer, an encoder-decoder transformer with hierarchical style encoding heads for controllable rendering of expressive piano music performances. We design a tokenized representation of symbolic score and performance music, the Score Per",Ilya Borovik (Skolkovo Institute of Science and Technology)*; Vladimir Viro (Peachnote),Ilya.Borovik@skoltech.ru; cmt3@viro.name,https://drive.google.com/file/d/1vbIybJB4N66A0xQ7yAWzKkquZYAmJtTm/view,https://drive.google.com/file/d/1UPfJULh0vG7aKP5C6LjqFMay-KYTbXdx/view,https://drive.google.com/file/d/13j1Zkl2_WFOTKRA-WgzOy_2G3UII5ipa/view,https://drive.google.com/file/d/1nk1g0AAilKaTQaBbUu3WUkZFyruTUNRl/view,5,https://archives.ismir.net/ismir2023/paper/000069.pdf,4,6,69,000069.pdf,Ilya Borovik,Ilya.Borovik@skoltech.ru,Musical features and properties -> expression and performative aspects of music,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> alignment, synchronization, and score following; Musical features and properties -> representations of music",https://drive.google.com/uc?export=view&id=1vbIybJB4N66A0xQ7yAWzKkquZYAmJtTm,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VK9TH8V,p5-07-borovik
89,Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features,"Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. 
This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an inter",Emmanouil Karystinaios (Johannes Kepler University)*; Gerhard Widmer (Johannes Kepler University),emmanouil.karystinaios@jku.at; gerhard.widmer@jku.at,https://drive.google.com/file/d/11FgFY287Aemntt0pXL5tCnLJ-WtRRXVA/view,https://drive.google.com/file/d/1_54nqe69FU_v39lSQMy8Et1-w_tm_KCy/view,https://drive.google.com/file/d/16X_gLqPUC64Oi48kfzaSCe0MErT9aY5h/view,https://drive.google.com/file/d/13jGWJNT1KC01cosmgMgCJ9haPcGJOVUz/view,5,https://archives.ismir.net/ismir2023/paper/000070.pdf,4,7,70,000070.pdf,Emmanouil Karystinaios,emmanouil.karystinaios@jku.at,"Musical features and properties -> harmony, chords and tonality",Knowledge-driven approaches to MIR -> computational music theory and musicology; MIR fundamentals and methodology -> symbolic music processing,https://drive.google.com/uc?export=view&id=11FgFY287Aemntt0pXL5tCnLJ-WtRRXVA,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YH110BC,p5-08-karystinaios
199,Semi-Automated Music Catalog Curation Using Audio and Metadata,"We present a system to assist Subject Matter Experts (SMEs) to curate large online music catalogs. The system detects releases that are incorrectly attributed to an artist discography (misattribution), when the discography of a single artist is incorrectl",Brian Regan (Spotify)*; Desislava Hristova (Spotify); Mariano Beguerisse-Díaz (Spotify),brianr@spotify.com; desih@spotify.com; marianob@spotify.com,https://drive.google.com/file/d/16_7tMqQrmgDBP_AGtaBTo9DEsTD1XgV0/view,https://drive.google.com/file/d/10tnkLU92yPqP-HA8sU9480-C3toBPPxM/view,https://drive.google.com/file/d/11bJaG1XGVRnokhKwaR47howB-BdQakK_/view,https://drive.google.com/file/d/1yTPpHfuqjLvRoKFjVoXHZlfHWnzqp_SN/view,5,https://archives.ismir.net/ismir2023/paper/000071.pdf,4,8,71,000071.pdf,Brian Regan,brianr@spotify.com,Applications -> digital libraries and archives,"Human-centered MIR -> user-centered evaluation; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; MIR fundamentals and methodology -> multimodality",https://drive.google.com/uc?export=view&id=16_7tMqQrmgDBP_AGtaBTo9DEsTD1XgV0,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTKGCAJ,p5-09-regan
202,Crowd's Performance on Temporal  Activity Detection of Musical Instruments in Polyphonic Music,"Musical instrument recognition enables applications such as instrument-based music search and audio manipulation, which are highly sought-after processes in everyday music consumption and production. Despite continuous progresses, advances in automatic mu",Ioannis Petros Samiotis (Delft University of Technology)*; Alessandro  Bozzon (Delft University of Technology); Christoph Lofi (TU Delft),i.p.samiotis@tudelft.nl; a.bozzon@tudelft.nl; c.lofi@tudelft.nl,https://drive.google.com/file/d/19KmwqtzdT_U7Hmx7Is-_WjRsm7qDP7Qv/view,https://drive.google.com/file/d/1UnOhoLaOwkjl8pG7-UemEH2zcSXSkFKH/view,https://drive.google.com/file/d/1FzPdsSh7yjCQQ6JPZHd0ZSlK_ue6tfjb/view,https://drive.google.com/file/d/1LwNRWoWsIIYeECYsJx7cE_v1yoswxU8h/view,5,https://archives.ismir.net/ismir2023/paper/000072.pdf,4,9,72,000072.pdf,Ioannis Petros Samiotis,i.p.samiotis@tudelft.nl,Human-centered MIR,"Human-centered MIR -> human-computer interaction; Human-centered MIR -> music interfaces and services; MIR tasks -> music transcription and annotation; MIR tasks -> sound source separation; Musical features and properties -> timbre, instrumentation, and singing voice",https://drive.google.com/uc?export=view&id=19KmwqtzdT_U7Hmx7Is-_WjRsm7qDP7Qv,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J1BH2T1,p5-10-samiotis
160,MoisesDB: A Dataset For Source Separation Beyond 4 Stems,"In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. 
For each song, we provide its individual audio sources, organized in a two-level hierarchical taxon",Igor G. Pereira (Moises.AI)*; Felipe Araujo (Moises.AI); Filip Korzeniowski (Moises.AI); Richard Vogl (moises.ai),igor@moises.ai; felipe.araujo@moises.ai; filip.korzeniowski@moises.ai; richard.vogl@moises.ai,https://drive.google.com/file/d/1tpXSRd_xGFPNBldII75K1wZsAsoQ068d/view,https://drive.google.com/file/d/1sTOnnJJLI9pUjLeCpKS6e5IFqxFu5MDp/view,https://drive.google.com/file/d/1x1nTGSfK6jUcphVYGlgKgXXVZAmsKItX/view,https://drive.google.com/file/d/1V1bBCq2K2HBY0ULXBjuFENZg4q30Y7oy/view,5,https://archives.ismir.net/ismir2023/paper/000073.pdf,4,10,73,000073.pdf,Igor G. Pereira,igor@moises.ai,"Evaluation, datasets, and reproducibility","Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> sound source separation",https://drive.google.com/uc?export=view&id=1tpXSRd_xGFPNBldII75K1wZsAsoQ068d,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YH1JYEN,p5-11-pereira
206,Music as flow: a formal representation of hierarchical processes in music,"Modeling the temporal unfolding of musical events and its interpretation in terms of hierarchical relations is a common theme in music theory, cognition, and composition. To faithfully encode such relations, we need an elegant way to represent both the se",Zeng Ren (EPFL)*; Wulfram Gerstner (EPFL); Martin A Rohrmeier (Ecole Polytechnique Fédérale de Lausanne),zeng.ren@epfl.ch; wulfram.gerstner@epfl.ch; martin.rohrmeier@epfl.ch,https://drive.google.com/file/d/19qDR-nUFtQUAxJE0km1U2Kzvm-HQvO7o/view,https://drive.google.com/file/d/1PSWl8tbBiqMPqK08h120HQTJ-6Po7_FL/view,https://drive.google.com/file/d/1MnOQegGQp3oL4vayaPQrNvkGMMfmjf7e/view,https://drive.google.com/file/d/1eO7LLD33O0ycx-w_RRKBtASn3kshtHz0/view,5,https://archives.ismir.net/ismir2023/paper/000074.pdf,4,11,74,000074.pdf,Zeng Ren,zeng.ren@epfl.ch,Knowledge-driven approaches to MIR -> representations of music,"Computational musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Musical features and properties -> harmony, chords and tonality; Musical features and properties -> structure, segmentation, and form",https://drive.google.com/uc?export=view&id=19qDR-nUFtQUAxJE0km1U2Kzvm-HQvO7o,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE7B1CK,p5-12-ren
208,Online Symbolic Music Alignment with Offline Reinforcement Learning,"Symbolic Music Alignment is the process of matching
performed MIDI notes to corresponding score notes. In
this paper, we introduce a reinforcement learning (RL)-
based online symbolic music alignment technique. The
RL agent — an attention-based neural net",Silvan Peter (JKU)*,silvan.peter@jku.at,https://drive.google.com/file/d/1ZV51EZdGgtBusGXIBV4fGjiTy9lexm-I/view,https://drive.google.com/file/d/1OrnnHfjrazGOuey-mIpeXrapOu9mmrZ_/view,https://drive.google.com/file/d/1g3T20qaG1EU4Py-guad-sUdCr4Tkd14s/view,https://drive.google.com/file/d/1_obDs8CHpqCJJyW1j0WiJ7wNNciCcT_5/view,5,https://archives.ismir.net/ismir2023/paper/000075.pdf,4,12,75,000075.pdf,Silvan Peter,silvan.peter@jku.at,"MIR tasks -> alignment, synchronization, and score following",MIR fundamentals and methodology -> symbolic music processing,https://drive.google.com/uc?export=view&id=1ZV51EZdGgtBusGXIBV4fGjiTy9lexm-I,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VKB7H7F,p5-13-peter
209,INVERSYNTH II: SOUND MATCHING VIA SELF-SUPERVISED SYNTHESIZER-PROXY AND INFERENCE-TIME FINETUNING,"Synthesizers are widely used electronic musical instruments. Given an input sound, inferring the underlying synthesizer's parameters to reproduce it is a difficult task known as sound-matching. In this work, we tackle the problem of automatic sound matchi",Oren Barkan (Microsoft); Shlomi Shvartzamn (Tel Aviv University ); Noy Uzrad  (Tel Aviv University ); Moshe Laufer  (Tel Aviv University); Almog Elharar (Tel Aviv University); Noam Koenigstein (Tel Aviv University)*,barkanoren1@gmail.com; sshlomi6@gmail.com; noyuzrad@mail.tau.ac.il; moshe13269@gmail.com; alicranck@gmail.com; noamk@tauex.tau.ac.il,,,,,5,https://archives.ismir.net/ismir2023/paper/000076.pdf,4,13,76,000076.pdf,Noam Koenigstein,noamk@tauex.tau.ac.il,MIR tasks -> music synthesis and transformation,MIR tasks -> music generation,,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGT3CHW,p5-14-koenigstein
210,A Semi-Supervised Deep Learning Approach to Dataset Collection for Query-by-Humming Task,"Query-by-Humming (QbH) is a task that involves finding the most relevant song based on a hummed or sung fragment. Despite recent successful commercial solutions, implementing QbH systems remains challenging due to the lack of high-quality datasets for tra","Amantur Amatov (Higher School of Economics)*; Dmitry Lamanov (Huawei Noah's Ark Lab); Maksim Titov (Huawei Noah's Ark Lab); Ivan Vovk (Huawei Noah's Ark Lab); Ilya Makarov (AI Center, NUST MISiS); Mikhail Kudinov (Huawei Noah's Ark Lab)",amatoamant@gmail.com; lamanov.dmitry@huawei.com; maksim.titov@huawei.com; ivan.vovk@huawei.com; iamakarov@misis.ru; kudinov.mikhail@huawei.com,https://drive.google.com/file/d/10IuEp3DDBqV0F0WM4u3nodR54J9fjajt/view,https://drive.google.com/file/d/1uSA4UuV6xLGU70470OraRCOE2sP38IWf/view,https://drive.google.com/file/d/10HbIvg_hiwfZmJB1HVd-v8MI-yOLmF-2/view,https://drive.google.com/file/d/1PEvsaujw2Rjs6gVj3n09ox3I_Puiszh2/view,5,https://archives.ismir.net/ismir2023/paper/000077.pdf,4,14,77,000077.pdf,Amantur Amatov,amatoamant@gmail.com,Applications -> music retrieval systems,"Evaluation, datasets, and reproducibility -> novel datasets and use cases; MIR tasks -> fingerprinting; MIR tasks -> indexing and querying",https://drive.google.com/uc?export=view&id=10IuEp3DDBqV0F0WM4u3nodR54J9fjajt,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063J1CKS6B,p5-15-amatov
212,Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction,"In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First",Keren Shao (UCSD)*; Ke Chen (University of California San Diego); Taylor Berg-Kirkpatrick (UCSD); Shlomo Dubnov (UC San Diego),k5shao@ucsd.edu; knutchen@ucsd.edu; tberg@ucsd.edu; sdubnov@ucsd.edu,https://drive.google.com/file/d/1rPO9dd5FjAqQ76wSk7iYdjBQC7srhTMm/view,https://drive.google.com/file/d/1ZS0w-QK8R-VtmSFhWl1cnbQ_rWmYmazP/view,https://drive.google.com/file/d/15HMp76uOrrL_9hjVj0PTJpPPbNL6r8QI/view,https://drive.google.com/file/d/1V9iJvFXFF38FppM-7nhlYT1e_CSYX8mF/view,5,https://archives.ismir.net/ismir2023/paper/000078.pdf,4,15,78,000078.pdf,Keren Shao,k5shao@ucsd.edu,Musical features and properties -> melody and motives,MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification,https://drive.google.com/uc?export=view&id=1rPO9dd5FjAqQ76wSk7iYdjBQC7srhTMm,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE86HDZ,p5-16-shao
220,SINGING VOICE SYNTHESIS USING DIFFERENTIABLE LPC AND GLOTTAL-FLOW-INSPIRED WAVETABLES,"This paper introduces GlOttal-flow LPC Filter (GOLF), a novel method for singing voice synthesis (SVS) that exploits the physical characteristics of the human voice using differentiable digital signal processing. GOLF employs a glottal model as the harmon",Chin-Yun Yu (Queen Mary University of London)*; George Fazekas (QMUL),chin-yun.yu@qmul.ac.uk; george.fazekas@qmul.ac.uk,https://drive.google.com/file/d/1i46u1z9bGukUxzCAxTxrIgyix-uKQ0X4/view,https://drive.google.com/file/d/1VNQc3u1tr_5yfqQBiEZ-IX73UfOuqWiv/view,https://drive.google.com/file/d/1gZIPuuU2QzKivJaIGpM2Uh2mgBND_ia6/view,https://drive.google.com/file/d/1esbWBjCD4bL-1b1R334yin24FFIZZVwo/view,6,https://archives.ismir.net/ismir2023/paper/000079.pdf,4,0,79,000079.pdf,Chin-Yun Yu,chin-yun.yu@qmul.ac.uk,MIR tasks -> music synthesis and transformation,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> music signal processing; Musical features and properties -> timbre, instrumentation, and singing voice",https://drive.google.com/uc?export=view&id=1i46u1z9bGukUxzCAxTxrIgyix-uKQ0X4,True,True,In Person,,,,https://slack.com/app_redirect?channel=C063J1CV40P,p6-01-yu
264,Harmonic Analysis with Neural Semi-CRF,"Automatic harmonic analysis of symbolic music is an important
and useful task for both composers and listeners.
The task consists of two components: recognizing harmony
labels and finding their time boundaries. Most of the
previous attempts focused on the",Qiaoyu Yang (University of Rochester)*; Frank Cwitkowitz (University of Rochester); Zhiyao Duan (Unversity of Rochester),qyang15@ur.rochester.edu; fcwitkow@ur.rochester.edu; zhiyao.duan@rochester.edu,https://drive.google.com/file/d/1nmlSKLZSPGNvc7LmNLfNGa5RkDpdZI1G/view,https://drive.google.com/file/d/1-aMmN-KI1fD0HaifkFwIDZqiaDZmzpDV/view,https://drive.google.com/file/d/1o-KmlLu-lsRgMxrSIRF-Hf-zaY8299hY/view?usp=sharing,https://drive.google.com/file/d/1w-loLMW-_BmPFrdJcciLHoltDTHCKPVS/view,6,https://archives.ismir.net/ismir2023/paper/000080.pdf,4,1,80,000080.pdf,Qiaoyu Yang,qyang15@ur.rochester.edu,Musical features and properties,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Musical features and properties -> harmony, chords and tonality",https://drive.google.com/uc?export=view&id=1nmlSKLZSPGNvc7LmNLfNGa5RkDpdZI1G,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YH2V4US,p6-02-yang
213,A Dataset and Baseline for Automated Assessment of Timbre Quality in Trumpet Sound,"Music Performance Analysis is based on the evaluation of performance parameters such as pitch, dynamics, timbre, tempo and timing. While timbre is the least specific parameter among these and is often only implicitly understood, prominent brass pedagogues",Ninad Puranik (McGill University ); Alberto Acquilino (McGill University)*; Ichiro Fujinaga (McGill University); Gary Scavone (McGill University),ninad.puranik@mail.mcgill.ca; alberto.acquilino@mail.mcgill.ca; ichiro.fujinaga@mcgill.ca; gary.scavone@mcgill.ca,https://drive.google.com/file/d/1wd_U2mEHtz_d_6P8HsDPcxC-voNPH4TJ/view,https://drive.google.com/file/d/1u0LgGMqp0_BNni8LK3A3MpyenzDkci2j/view,https://drive.google.com/file/d/1yO2gg3CZlQDiddv2qQVZQLPXz2S9bDSm/view,https://drive.google.com/file/d/178jwgniWfuX8pqTIDgNAKf1zs3NnbG48/view,6,https://archives.ismir.net/ismir2023/paper/000081.pdf,4,2,81,000081.pdf,Alberto Acquilino,alberto.acquilino@mail.mcgill.ca,"Musical features and properties -> timbre, instrumentation, and singing voice","Evaluation, datasets, and reproducibility -> novel datasets and use cases; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR and machine learning for musical acoustics -> applications of machine learning to musical acoustics; MIR fundamentals and methodology -> music signal processing; MIR tasks -> automatic classification",https://drive.google.com/uc?export=view&id=1wd_U2mEHtz_d_6P8HsDPcxC-voNPH4TJ,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE8K0UB,p6-03-acquilino
216,Visual Overviews for Sheet Music Structure,"We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall simi","Frank Heyen (VISUS, University of Stuttgart)*; Quynh Quang Ngo (VISUS, University of Stuttgart); Michael Sedlmair (Uni Stuttgart)",frank.heyen@visus.uni-stuttgart.de; quynh.ngo@visus.uni-stuttgart.de; Michael.Sedlmair@visus.uni-stuttgart.de,https://drive.google.com/file/d/1QqrLFOACnxQebUM1qdQT2AmCTegxTCmz/view,https://docs.google.com/presentation/d/1nyDdRA9G_GKqxqVvd_VAjyZZ6vNasbcyGI5aZX-5Q4A/edit#slide=id.g24dde5fbebb_2_259,https://drive.google.com/file/d/1w8dCKsqJcsnqocFs_85czyjyKre-gtkJ/view,https://drive.google.com/file/d/1eP7DTcz1w1CQzdCdZkDkFWNSRLn8Ppil/view,6,https://archives.ismir.net/ismir2023/paper/000082.pdf,4,3,82,000082.pdf,Frank Heyen,frank.heyen@visus.uni-stuttgart.de,"Musical features and properties -> structure, segmentation, and form",MIR tasks -> pattern matching and detection; MIR tasks -> similarity metrics,https://drive.google.com/uc?export=view&id=1QqrLFOACnxQebUM1qdQT2AmCTegxTCmz,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGU2H08,p6-04-heyen
217,Passage Summarization with recurrent models for Audio – Sheet Music Retrieval,"Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippe",Luis Carvalho (Johannes Kepler University)*; Gerhard Widmer (Johannes Kepler University),luis.carvalho@jku.at; gerhard.widmer@jku.at,https://drive.google.com/file/d/15qRSYgL-w431GrhVbmHAjeBfhJE2FK2k/view,https://drive.google.com/file/d/11QvPMTpdKHgomBbt8AbYQSbKHydqwWZr/view?usp=share_link,https://drive.google.com/file/d/16aq0qsq19INHfi84Sin02FuzEjB-Xmtg/view,https://drive.google.com/file/d/1RAPcV6AWFbK_fcH4Nhtxo_cCjFeFRmuE/view,6,https://archives.ismir.net/ismir2023/paper/000083.pdf,4,4,83,000083.pdf,Luis Carvalho,luis.carvalho@jku.at,Applications -> music retrieval systems,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> multimodality; MIR tasks -> indexing and querying; Musical features and properties -> representations of music,https://drive.google.com/uc?export=view&id=15qRSYgL-w431GrhVbmHAjeBfhJE2FK2k,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YH3DABC,p6-05-carvalho
218,Predicting performance difficulty from piano sheet music images,"Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the music information retrieval community has recently shown interest in this task, existing ",Pedro Ramoneda (Universitat Pompeu Fabra)*; Dasaem Jeong (Sogang University); Jose J. Valero-Mas (Universitat Pompeu Fabra); Xavier Serra (Universitat Pompeu Fabra ),pedro.ramoneda@upf.edu; dasaemj@sogang.ac.kr; josejavier.valero@upf.edu; xavier.serra@upf.edu,https://drive.google.com/file/d/1QBmb6yaCFABAGc5IwYkOjqB7PKkCWBLY/view,https://docs.google.com/presentation/d/1ggbjTinWnjXKdAUtl18jk_x6ZiwNbxp8/edit#slide=id.g1e92513d498_0_7,https://drive.google.com/file/d/1H8IC8Nh5hP1n4RMFKRI_J9sE9-oe50QB/view,https://drive.google.com/file/d/1pyIv2gmhW7ESVie5XjZhA9e4lypN4Idc/view,6,https://archives.ismir.net/ismir2023/paper/000084.pdf,4,5,84,000084.pdf,Pedro Ramoneda,pedro.ramoneda@upf.edu,Applications,Applications -> digital libraries and archives; Applications -> music training and education,https://drive.google.com/uc?export=view&id=1QBmb6yaCFABAGc5IwYkOjqB7PKkCWBLY,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE93MGT,p6-06-ramoneda
288,Self-Refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data,"Music source separation (MSS) faces challenges due to limited availability and potential noise in correctly labeled individual instrument tracks. In this paper, we propose an automated approach for refining mislabeled instrument tracks in a partially nois",Junghyun Koo (Seoul National University); Yunkee Chae (Seoul National University)*; Chang-Bin Jeon (Seoul National University); Kyogu Lee (Seoul National University),dg22302@snu.ac.kr; yunkimo95@snu.ac.kr; vinyne@snu.ac.kr; kglee@snu.ac.kr,https://drive.google.com/file/d/1y4ZGkF2Jlhi-sn4Ci2OnoFTHEm_lshRA/view,https://drive.google.com/file/d/1kL8I5joAX4lUI488J8rrf5ei-5jFXuin/view,https://drive.google.com/file/d/139i9K8i6jx5hs0D6oPwD9gUUuN4XEzpz/view,https://drive.google.com/file/d/1wz5TKBDBZF5HXfuJiHr_ouDVLW2xQ9RU/view,6,https://archives.ismir.net/ismir2023/paper/000085.pdf,4,6,85,000085.pdf,Yunkee Chae,yunkimo95@snu.ac.kr,MIR tasks -> automatic classification,"Evaluation, datasets, and reproducibility -> annotation protocols; MIR tasks -> sound source separation",https://drive.google.com/uc?export=view&id=1y4ZGkF2Jlhi-sn4Ci2OnoFTHEm_lshRA,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YH3NK1Q,p6-07-chae
225,Quantifying the Ease of Playing Song Chords on the Guitar,"Quantifying the difficulty of playing songs has recently gained traction in the MIR community. While previous work has mostly focused on piano, this paper concentrates on rhythm guitar, which is especially popular with amateur musicians and has a broad sk","Marcel A Vélez Vásquez (University of Amsterdam)*; Mariëlle  Baelemans (University of Amsterdam); Jonathan Driedger (Chordify); Willem Zuidema (ILLC, UvA); John Ashley Burgoyne (University of Amsterdam)",m.a.velezvasquez@uva.nl; m.c.e.baelemans@uva.nl; jonathan@chordify.net; w.h.zuidema@uva.nl; j.a.burgoyne@uva.nl,https://drive.google.com/file/d/18wnkJGsvsQoydKJCUe1OFNoaK_fS0W35/view,https://drive.google.com/file/d/1zuttpAiI6ylNAqCUCZcCb4QMBOVaPL74/view,https://drive.google.com/file/d/1sesOh351ItbTXHGgEfFTCQY5U7P4oNg7/view?usp=sharing ,https://drive.google.com/file/d/1cDyf7XFifNl7ojPkqYTlZhr63H5LuovV/view,6,https://archives.ismir.net/ismir2023/paper/000086.pdf,4,7,86,000086.pdf,Marcel A Vélez Vásquez,m.a.velezvasquez@uva.nl,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Applications -> music training and education; Evaluation, datasets, and reproducibility -> annotation protocols; Human-centered MIR -> user-centered evaluation; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Musical features and properties -> harmony, chords and tonality",https://drive.google.com/uc?export=view&id=18wnkJGsvsQoydKJCUe1OFNoaK_fS0W35,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6FLBDF,p6-08-vásquez
235,FlexDTW: Dynamic Time Warping With Flexible Boundary Conditions,"Alignment algorithms like DTW and subsequence DTW assume specific boundary conditions on where an alignment path can begin and end in the cost matrix.  In practice, the boundary conditions may not be known a priori or may not satisfy such strict assumptio",Irmak Bukey (Pomona College); Jason Zhang (University of Michigan); Timothy Tsai (Harvey Mudd College)*,ibab2018@mymail.pomona.edu; zhangjt@umich.edu; ttsai@g.hmc.edu,https://drive.google.com/file/d/1X4iAg6IrkXmFmOODNbJSbq71M-aMlR7H/view,https://drive.google.com/file/d/13QXrP9TfIRAlWXiVj1YfHZJJtOowEInc/view,https://drive.google.com/file/d/1qNpxCnv167R5lwPIetpbOveI_VK3QKsh/view,https://drive.google.com/file/d/1BN51rFgI2RdzQOar5_fjQ-Ll3Rvqnwwt/view,6,https://archives.ismir.net/ismir2023/paper/000087.pdf,4,8,87,000087.pdf,Timothy Tsai,ttsai@g.hmc.edu,"MIR tasks -> alignment, synchronization, and score following",MIR fundamentals and methodology -> music signal processing,https://drive.google.com/uc?export=view&id=1X4iAg6IrkXmFmOODNbJSbq71M-aMlR7H,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410YBZT6,p6-09-tsai
166,Modeling Bends in Popular Music Guitar Tablatures,"Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specifi",Alexandre D'Hooge (Université de Lille)*; Louis Bigo (Université de Lille); Ken Déguernel (CNRS),alexandre.dhooge@univ-lille.fr; louis.bigo@univ-lille.fr; ken.deguernel@univ-lille.fr,https://drive.google.com/file/d/1XXZNSayceHF8YHyjGkUCzeF3CHWeNJIs/view,https://docs.google.com/presentation/d/1UGmVpe7pBZ-5D-psux8IYuJoa-c8JeNN/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,https://drive.google.com/file/d/1K3KRVVWiVIuj59AeMspoHofr2mAQUCFa/view,https://drive.google.com/file/d/1CRKc3eJrIAxxfuM8dnUPP8uWTQC9XibN/view,6,https://archives.ismir.net/ismir2023/paper/000088.pdf,4,9,88,000088.pdf,Alexandre D'Hooge,alexandre.dhooge@univ-lille.fr,Knowledge-driven approaches to MIR,"Applications -> music composition, performance, and production; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties -> expression and performative aspects of music",https://drive.google.com/uc?export=view&id=1XXZNSayceHF8YHyjGkUCzeF3CHWeNJIs,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B7CPKA5,p6-10-dhooge
279,Self-Similarity-Based and Novelty-based loss for music structure analysis,"Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. 
In this paper we propose a supervised approach for the task of music boundary detection. In ou","Geoffroy Peeters (LTCI - Télécom Paris, IP Paris)*",geoffroy.peeters@telecom-paris.fr,https://drive.google.com/file/d/1xqLMnWUj3hT_sjnqvwheaofzbZXjVOzJ/view,https://drive.google.com/file/d/1C_GxzKeRASGzV66NXQbG5Z6Vrzk3bQXG/view,https://drive.google.com/file/d/121wH3eycz4zZ9mNNgiw8YeOnAsGmPnAe/view,https://drive.google.com/file/d/1PzC9ayjSkxw8Bjpfo5WumSCw27LCT4Qe/view,6,https://archives.ismir.net/ismir2023/paper/000089.pdf,4,10,89,000089.pdf,Geoffroy Peeters,geoffroy.peeters@telecom-paris.fr,"Musical features and properties -> structure, segmentation, and form",MIR fundamentals and methodology -> music signal processing,https://drive.google.com/uc?export=view&id=1xqLMnWUj3hT_sjnqvwheaofzbZXjVOzJ,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410YG1JQ,p6-11-peeters
239,Modeling Harmonic Similarity for Jazz Using Co-occurrence Vectors and the Membrane Area,"In jazz, measuring harmonic similarity is complicated by the common practice of reharmonization -- the altering or substitution of chords without fundamentally changing the piece's harmonic identity. This is analogous to natural language processing tasks ","Carey Bunks (City, University London)*; Simon Dixon (Queen Mary University of London); Tillman Weyde (City, University of London); Bruno Di Giorgi (Apple)",c.bunks@qmul.ac.uk; s.e.dixon@qmul.ac.uk; t.e.weyde@city.ac.uk; bdigiorgi@apple.com,https://drive.google.com/file/d/1lNhHbatsqrN-i2u-FddjD2ziHofHHhCg/view,https://drive.google.com/file/d/1_jthEKwcX8fB0q662jv0SMGg_jj5wIB8/view,https://drive.google.com/file/d/1ciE5bkv_nwm_kw_WaV6GLaqgN3gPT1JG/view,https://drive.google.com/file/d/1DrWsujLUQNd6LlP-WwBq1SMYX-lvV7qd/view,6,https://archives.ismir.net/ismir2023/paper/000090.pdf,4,11,90,000090.pdf,Carey Bunks,c.bunks@qmul.ac.uk,Musical features and properties -> representations of music,"MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> similarity metrics; Musical features and properties -> harmony, chords and tonality",https://drive.google.com/uc?export=view&id=1lNhHbatsqrN-i2u-FddjD2ziHofHHhCg,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064MGV379N,p6-12-bunks
25,SingStyle111: A Multilingual Singing Dataset With Style Transfer,"There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multip","Shuqi Dai (Carnegie Mellon University)*; Siqi Chen (University of South California); Yuxuan Wu (Carnegie Mellon University); Roy Huang (Carnegie Mellon University); Roger B. Dannenberg (School of Computer Science, Carnegie Mellon University)",shuqid@cs.cmu.edu; schen307@usc.edu; yuxuanw2@andrew.cmu.edu; rghuang@andrew.cmu.edu; rbd@cs.cmu.edu,https://drive.google.com/file/d/1zUOje_v-rxU2buZJAE78g0UeOoZu8aex/view,https://docs.google.com/presentation/d/1tdEBfkCBUr5rp47yxtelZuxUtwdY0aJC/edit?usp=share_link&ouid=116278840554580538889&rtpof=true&sd=true,,https://drive.google.com/file/d/162X-YscQ5gJDBkLRCsHHhfmXBoxs9c_y/view,6,https://archives.ismir.net/ismir2023/paper/000091.pdf,4,12,91,000091.pdf,Shuqi Dai,shuqid@cs.cmu.edu,"Evaluation, datasets, and reproducibility","Evaluation, datasets, and reproducibility -> novel datasets and use cases",https://drive.google.com/uc?export=view&id=1zUOje_v-rxU2buZJAE78g0UeOoZu8aex,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YE9TT7D,p6-13-dai
255,A Computational Evaluation Framework for Singable Lyric Translation,"Lyric translation plays a pivotal role in amplifying the global resonance of music, bridging cultural divides, and fostering universal connections. Translating lyrics, unlike conventional translation tasks, requires a delicate balance between singability ",Haven Kim (KAIST)*; Kento Watanabe (National Institute of Advanced Industrial Science and Technology (AIST)); Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)); Juhan Nam (KAIST),khaven@kaist.ac.kr; kento.watanabe@aist.go.jp; m.goto@aist.go.jp; juhan.nam@kaist.ac.kr,https://drive.google.com/file/d/1u2Zp6skZxD3j5rwmlFHi_D1JkQKcDifA/view,https://drive.google.com/file/d/1Q4KYujFVZN08NpdPsjNVLDNk5wp5sXJ_/view,https://drive.google.com/file/d/13EbuMQL3bXnAMgI9Rjm37m0_u4Npg8VC/view,https://drive.google.com/file/d/1bsNIzDKIKvA3XDvcrXjhLr8UiYlhuATe/view,6,https://archives.ismir.net/ismir2023/paper/000092.pdf,4,13,92,000092.pdf,Haven Kim,khaven@kaist.ac.kr,MIR fundamentals and methodology -> lyrics and other textual data,"Computational musicology; Evaluation, datasets, and reproducibility; Evaluation, datasets, and reproducibility -> evaluation methodology; Evaluation, datasets, and reproducibility -> evaluation metrics; MIR fundamentals and methodology -> web mining, and natural language processing",https://drive.google.com/uc?export=view&id=1u2Zp6skZxD3j5rwmlFHi_D1JkQKcDifA,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VKDLWR3,p6-14-kim
257,Chorus-Playlist: Exploring the Impact of Listening to Only Choruses in a Playlist,"When people listen to playlists on a music streaming service, they typically listen to each song from start to end in order. However, what if it were possible to use a function to listen to only the choruses of each song in a playlist one after another? I",Kosetsu Tsukuda (National Institute of Advanced Industrial Science and Technology (AIST))*; Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST)); Masataka Goto (National Institute of Advanced Industrial Science and T,k.tsukuda@aist.go.jp; masahiro.hamasaki@aist.go.jp; m.goto@aist.go.jp,https://drive.google.com/file/d/1Yg9P6EWBZlriwOMFyjsR268Y7X5waAGn/view,https://drive.google.com/file/d/1gYyQWnAkIZXf9mnDPVY43dTd0jyF-RhD/view,https://drive.google.com/file/d/1D_S3hsdk6Gd_ypaU1QNEIZUYZ5J1F-nn/view,https://drive.google.com/file/d/1a7HsyxwthWwKit7Ukc0ZFxF2onE1pY_9/view,6,https://archives.ismir.net/ismir2023/paper/000093.pdf,4,14,93,000093.pdf,Kosetsu Tsukuda,k.tsukuda@aist.go.jp,Human-centered MIR -> human-computer interaction,Human-centered MIR -> music interfaces and services,https://drive.google.com/uc?export=view&id=1Yg9P6EWBZlriwOMFyjsR268Y7X5waAGn,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6GD36V,p6-15-tsukuda
172,Supporting musicological investigations with information retrieval tools: an iterative approach to data collection,"Digital musicology research often proceeds by extending and enriching its evidence base as it progresses, rather than starting with a complete corpus of data and metadata, as a consequence of an emergent research need.

In this paper, we consider a resear",David Lewis (University of Oxford eResearch Centre)*; Elisabete Shibata (Beethoven-Haus Bonn); Andrew Hankinson (RISM Digital); Johannes Kepper (Paderborn University); Kevin R Page (University of Oxford); Lisa Rosendahl (Paderborn University); Mark Saccom,david.lewis@oerc.ox.ac.uk; shibata@beethoven.de; andrew.hankinson@rism.digital; kepper@edirom.de; kevin.page@oerc.ox.ac.uk; lisarosendahl@gmx.com; mark.saccomano@uni-paderborn.de; siegert@beethoven.de,https://drive.google.com/file/d/1hhHR2gBnuA_U-ur1la-5g_Gdjigaj-IA/view,https://drive.google.com/file/d/1L695BRLMbAvPgZ2diOXLIpC10fzt42Ar/view,https://drive.google.com/file/d/1SfhuAl2LxICb6Tqh9VHB4MhrkLYy7LXZ/view,https://drive.google.com/file/d/1TJAiDOT_z9XkW0iAvJiyOyO6otFfN__i/view,7,https://archives.ismir.net/ismir2023/paper/000094.pdf,5,0,94,000094.pdf,David Lewis,david.lewis@oerc.ox.ac.uk,Computational musicology -> digital musicology,"Applications -> digital libraries and archives; Evaluation, datasets, and reproducibility -> annotation protocols; Human-centered MIR -> music interfaces and services; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",https://drive.google.com/uc?export=view&id=1hhHR2gBnuA_U-ur1la-5g_Gdjigaj-IA,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTP2S3Y,p7-01-lewis
96,Optimizing Feature Extraction for Symbolic Music,"This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we",Federico Simonetta (Instituto Complutense de Ciencias Musicales)*; Ana Llorens (Universidad Complutense de Madrid); Martín Serrano (Instituto Complutense de Ciencias Musicales); Eduardo García-Portugués (Universidad Carlos III de Madrid); Álvaro Torrente ,fsimonetta@iccmu.es; allorens@ucm.es; mserrano@iccmu.es; edgarcia@est-econ.uc3m.es; atorrente@iccmu.es,https://drive.google.com/file/d/1O6IL5DwvLC1w389b5agKpwZG-c8kTz8N/view,https://drive.google.com/file/d/1_w78MiJSAvgTVFKZu0bbGxDpaaMWzpA5/view,https://drive.google.com/file/d/1pMp8Jvp4vA8rIN2SrAmF8-9sXIpOnGiO/view,https://drive.google.com/file/d/1opJZYW_qoczWG31bwyX_A-xMhutI0V8g/view,7,https://archives.ismir.net/ismir2023/paper/000095.pdf,5,1,95,000095.pdf,Federico Simonetta,fsimonetta@iccmu.es,Computational musicology,Computational musicology -> systematic musicology; Knowledge-driven approaches to MIR -> computational music theory and musicology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR fundamentals and methodology -> symbolic music processing; Musical features and properties,https://drive.google.com/uc?export=view&id=1O6IL5DwvLC1w389b5agKpwZG-c8kTz8N,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6GKVND,p7-02-simonetta
274,Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model,"Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling ",Mathias Rose Bjare (Johannes Kepler University Linz)*; Stefan Lattner (Sony CSL); Gerhard Widmer (Johannes Kepler University),muthissar@gmail.com; stefan.lattner@sony.com; gerhard.widmer@jku.at,https://drive.google.com/file/d/1F_A6d1BGTWa4n-v1osN5z9gROll7r5Lj/view,https://drive.google.com/file/d/1-aIfAI9BtIWCBerw08HPK9qU3OZqGoy2/view,https://drive.google.com/file/d/1_ZolJIkB1I7G0S5K_-DsqR7_-9MnWtfa/view,https://drive.google.com/file/d/1rbDRvHNLeKUN7NvnFjCaU6HU6b3HRFZz/view,7,https://archives.ismir.net/ismir2023/paper/000096.pdf,5,2,96,000096.pdf,Mathias Rose Bjare,muthissar@gmail.com,MIR tasks -> music generation,"Applications -> music composition, performance, and production; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> music synthesis and transformation; Musical features and properties -> melody and motives; Musical features and properties -> structure, segmentation, and form",https://drive.google.com/uc?export=view&id=1F_A6d1BGTWa4n-v1osN5z9gROll7r5Lj,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6GPRG9,p7-03-bjare
276,Measuring the Eurovision Song Contest: A Living Dataset for Real-World MIR,"Every year, several dozen, primarily European, countries, send performers to compete on live television at the Eurovision Song Contest, with the goal of entertaining an international audience of more than 150 million viewers. Each participating country is",John Ashley Burgoyne (University of Amsterdam)*; Janne Spijkervet (University of Amsterdam); David J Baker (University of Amsterdam),j.a.burgoyne@uva.nl; janne.spijkervet@gmail.com; d.j.baker@uva.nl,,,,,7,https://archives.ismir.net/ismir2023/paper/000097.pdf,5,3,97,000097.pdf,John Ashley Burgoyne,j.a.burgoyne@uva.nl,"Evaluation, datasets, and reproducibility -> novel datasets and use cases","Evaluation, datasets, and reproducibility -> evaluation metrics; Human-centered MIR -> user behavior analysis and mining, user modeling; Knowledge-driven approaches to MIR -> computational ethnomusicology; MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web; Musical features and properties -> expression and performative aspects of music",,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063YEAKF7V,p7-04-burgoyne
248,Efficient Supervised Training of Audio Transformers for Music Representation Learning,"In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In",Pablo Alonso-Jiménez (Universitat Pompeu Fabra)*; Xavier Serra (Universitat Pompeu Fabra ); Dmitry Bogdanov (Universitat Pompeu Fabra),pablo.alonso@upf.edu; xavier.serra@upf.edu; dmitry.bogdanov@upf.edu,https://drive.google.com/file/d/1pYJ_xgEA1kqqYQ9WvqzVPHnRewmz-hzJ/view,https://drive.google.com/file/d/1RI6H4LsGvtfPitec9A_tbIU3F1CPzMjY/view,https://drive.google.com/file/d/1RSGNN1xzfXuzyjHy1ZIkWmUWF8qOS-Ml/view,https://drive.google.com/file/d/1OfoD4hN9VyyTdAfWv0mFF8gIZfUteVNU/view,7,https://archives.ismir.net/ismir2023/paper/000098.pdf,5,4,98,000098.pdf,Pablo Alonso-Jiménez,pablo.alonso@upf.edu,Musical features and properties -> representations of music,"Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; MIR tasks -> automatic classification; Musical features and properties -> musical affect, emotion and mood; Musical features and properties -> musical style and genre; Musical features and properties -> timbre, instrumentation, and singing voice",https://drive.google.com/uc?export=view&id=1pYJ_xgEA1kqqYQ9WvqzVPHnRewmz-hzJ,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063VKECFV3,p7-05-alonso-jiménez
79,A Cross-Version Approach to Audio Representation Learning for Orchestral Music,"Deep learning systems have become popular for tackling a variety of music information retrieval tasks. However, these systems often require large amounts of labeled data for supervised training, which can be very costly to obtain. To alleviate this proble",Michael Krause (International Audio Laboratories Erlangen)*; Christof Weiß (University of Würzburg); Meinard Müller (International Audio Laboratories Erlangen),michael.krause@audiolabs-erlangen.de; christof.weiss@uni-wuerzburg.de; meinard.mueller@audiolabs-erlangen.de,https://drive.google.com/file/d/1m4gubvlJ3XO25aLLlSbhY3_O7c960PXe/view,https://drive.google.com/file/d/1abOekT3Z5IB-9UpyKZLm9iZj7NuxHwxD/view,https://drive.google.com/file/d/116v-81IMZNYlMz551ahs-D7xmnKdCvkn/view,https://drive.google.com/file/d/17lmKrbUhR_PxD6RwdUpKmqRLcSvWShDt/view,7,https://archives.ismir.net/ismir2023/paper/000099.pdf,5,5,99,000099.pdf,Michael Krause,michael.krause@audiolabs-erlangen.de,Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music,"MIR tasks -> similarity metrics; Musical features and properties -> representations of music; Musical features and properties -> timbre, instrumentation, and singing voice",https://drive.google.com/uc?export=view&id=1m4gubvlJ3XO25aLLlSbhY3_O7c960PXe,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTPKUNA,p7-06-krause
278,"Music source separation with MLP mixing of time, frequency, and channel","This paper proposes a new music source separation (MSS) model based on an architecture with MLP-Mixer that leverages multilayer perceptrons (MLPs). Most of the recent MSS techniques are based on architectures with CNNs, RNNs, and attention-based transform",Tomoyasu Nakano (National Institute of Advanced Industrial Science and Technology (AIST))*; Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST)),t.nakano@aist.go.jp; m.goto@aist.go.jp,https://drive.google.com/file/d/1OdDDCEu0TnAWs0wi2vAtTGRm5pAodRN4/view,https://drive.google.com/file/d/1mUCXimBtROHSKFznac8FlM727Uf-Kb5g/view,https://drive.google.com/file/d/1pVSndiIoUYLscD295HxipgYLpldiJ56i/view,https://drive.google.com/file/d/1x1nOic95F0W6ASQHRd0avta3_U209gEY/view,7,https://archives.ismir.net/ismir2023/paper/000100.pdf,5,6,100,000100.pdf,Tomoyasu Nakano,t.nakano@aist.go.jp,MIR tasks -> sound source separation,MIR fundamentals and methodology -> music signal processing,https://drive.google.com/uc?export=view&id=1OdDDCEu0TnAWs0wi2vAtTGRm5pAodRN4,False,False,In Person,,,,https://slack.com/app_redirect?channel=C06410ZQ5FE,p7-07-nakano
54,Symbolic Music Representations for Classification Tasks: A Systematic Evaluation,"Music Information Retrieval (MIR) has seen a recent surge in deep learning-based approaches, which often involve encoding symbolic music (i.e., music represented in terms of discrete note events) in an image-like or language-like fashion. However, symboli",Huan Zhang (Queen Mary University of London)*; Emmanouil Karystinaios (Johannes Kepler University); Simon Dixon (Queen Mary University of London); Gerhard Widmer (Johannes Kepler University); Carlos Eduardo Cancino-Chacón (Johannes Kepler University Linz),huan.zhang@qmul.ac.uk; emmanouil.karystinaios@jku.at; s.e.dixon@qmul.ac.uk; gerhard.widmer@jku.at; carlos_eduardo.cancino_chacon@jku.at,https://drive.google.com/file/d/1uMhKpUkVAiNaaUTFoFzAhxOQZBsHnu4Z/view,https://drive.google.com/file/d/1Xfrdwc4P-oI5bzz9g_SrJchCwqu09dvo/view,https://drive.google.com/file/d/1_57q-qGsRO9druNfRi3YHndX4LqLll4r/view,https://drive.google.com/file/d/1Drbq0ENSx89p_s9JtB9LDo7ITkDV8hkT/view,7,https://archives.ismir.net/ismir2023/paper/000101.pdf,5,7,101,000101.pdf,Huan Zhang,huan.zhang@qmul.ac.uk,MIR fundamentals and methodology -> symbolic music processing,"Evaluation, datasets, and reproducibility -> evaluation methodology; Knowledge-driven approaches to MIR -> machine learning/artificial intelligence for music; Knowledge-driven approaches to MIR -> representations of music; MIR tasks -> automatic classification; Musical features and properties -> representations of music",https://drive.google.com/uc?export=view&id=1uMhKpUkVAiNaaUTFoFzAhxOQZBsHnu4Z,False,False,Virtually,,,,https://slack.com/app_redirect?channel=C063YEB286P,p7-08-zhang
283,The Music Meta Ontology: a flexible semantic model for the interoperability of music metadata,"The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity o",Valentina Carriero (University of Bologna); Jacopo de Berardinis (King's College London); Albert Meroño-Peñuela (King's College London); Andrea Poltronieri (University of Bologna)*; Valentina Presutti (University of Bologna),valentina.carriero3@unibo.it; jacopo.deberardinis@kcl.ac.uk; albert.merono@kcl.ac.uk; andrea.poltronieri2@unibo.it; valentina.presutti@unibo.it,https://drive.google.com/file/d/1sWmDSmjUDTcma5qxam4u_qSe1tQZXY6K/view,https://drive.google.com/file/d/1fIylHtuJBQX1X1wnIvR0oInoxBE0lvpq/view,https://drive.google.com/file/d/1lJSkNd_OzlU73QswUv1Ca93O_UY4BA-c/view,https://drive.google.com/file/d/1BjDSWYba5EKYc3zxqWTFJdMroDD7OWVm/view,7,https://archives.ismir.net/ismir2023/paper/000102.pdf,5,8,102,000102.pdf,Andrea Poltronieri,andrea.poltronieri2@unibo.it,"MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",Applications -> digital libraries and archives; Knowledge-driven approaches to MIR -> representations of music,https://drive.google.com/uc?export=view&id=1sWmDSmjUDTcma5qxam4u_qSe1tQZXY6K,False,False,In Person,,,,https://slack.com/app_redirect?channel=C064B6HD32M,p7-09-poltronieri
294,Polar Manhattan Displacement: measuring tonal distances between chords based on intervallic content,"Large-scale studies of musical harmony are often hampered by lack of suitably labelled data. It would be highly advantageous if an algorithm were able to autonomously describe chords, scales, etc. in a consistent and musically informative way. In this pap",Jeffrey K Miller (Queen Mary University of London)*; Johan Pauwels (Queen Mary University of London); Mark B Sandler (Queen Mary University of London),j.k.miller@qmul.ac.uk; j.pauwels@qmul.ac.uk; mark.sandler@qmul.ac.uk,https://drive.google.com/file/d/1wRsgtl_jBL20Cm317Yp1879UHKs7-d5b/view,https://drive.google.com/file/d/1gSLS95kXdT7yw8oneBF16E_FOduR36h5/view,https://drive.google.com/file/d/1BdVT7RIJJIrCPVhUBL5JoFYJHoUjzZWd/view,https://drive.google.com/file/d/1zJyH-rDZuVL4AdIP8p25WRbLLtizMkEf/view,7,https://archives.ismir.net/ismir2023/paper/000103.pdf,5,9,103,000103.pdf,Jeffrey K Miller,j.k.miller@qmul.ac.uk,Knowledge-driven approaches to MIR -> computational music theory and musicology,"Computational musicology -> mathematical music theory; Knowledge-driven approaches to MIR -> representations of music; MIR fundamentals and methodology -> symbolic music processing; MIR tasks -> similarity metrics; Musical features and properties -> harmony, chords and tonality",https://drive.google.com/uc?export=view&id=1wRsgtl_jBL20Cm317Yp1879UHKs7-d5b,False,False,In Person,,,,https://slack.com/app_redirect?channel=C063RTQ2BPY,p7-10-miller
